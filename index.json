[{"categories":["data"],"contents":"大数据单机版部署\n基础环境 目录安排\n规划 目录 备注 应用程序安装路径 /data/application/app/ 数据存储目录 /data/application/data/ 数据日志目录 /data/application/logs/ 数据备份目录（备份服务器） /data/application/backup/ 服务器上临时存放地 /data/application/tmp/ 服务器上工具存放地 /data/application/tools// 服务器上监控存放地 /data/application/prometheus/ mkdir /data/application/{app,data,logs,backup,tmp,tools,prometheus} -p 安装JDK tar xvf jdk-8u121-linux-x64.tar.gz -C /data/application/app/ vim /etc/profile export JAVA_HOME=/data/application/app/jdk1.8.0_121/ export JRE_HOME=/data/application/app/jdk1.8.0_121/jre export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH zookeeper部署 tar xvf apache-zookeeper-3.8.0-bin.tar.gz -C /data/application/app/ mkdir /data/application/data/zookeeper 配置启动文件 vim /usr/lib/systemd/system/zookeeper.service\n[Unit] Description=Zookeeper Server Service After=network.target [Service] Type=forking UMask=0027 User=root Group=root ExecStart=/data/application/app/apache-zookeeper-3.8.0-bin/bin/zkServer.sh start ExecStop=/data/application/app/apache-zookeeper-3.8.0-bin/bin/zkServer.sh stop Restart=on-failure RestartSec=10 [Install] WantedBy=multi-user.target vim /data/application/app/apache-zookeeper-3.8.0-bin/bin/zkEnv.sh\nJAVA_HOME=\u0026#34;/data/application/app/jdk1.8.0_121/\u0026#34; 启动 systemctl start zookeeper Kafka 部署 cd /data/application/tools/ tar xvf kafka_2.12-2.4.1\\ .tgz -C /data/application/app/ mkdir /data/application/data/kafka/kafka-log vim server.properties\nbroker.id=1 #每个ID 不一样 listeners=PLAINTEXT://IP:9092 # 本机的ip地址 num.network.threads=9 num.io.threads=16 socket.send.buffer.bytes=1024000 socket.receive.buffer.bytes=1024000 socket.request.max.bytes=104857600 log.dirs=/data/application/data/kafka/kafka-log num.partitions=30 num.recovery.threads.per.data.dir=1 log.retention.hours=24 log.segment.bytes=1073741824 log.retention.check.interval.ms=300000 zookeeper.connect=10.1.61.128:2181,10.1.61.129:2181,10.1.61.130:2181 zookeeper.connection.timeout.ms=6000 delete.topic.enable = true auto.create.topics.enable=true message.max.bytes=20000000 replica.fetch.max.bytes=20485760 acks=1 log.cleanup.policy=delete num.network.threads主要处理网络io，读写缓冲区数据，基本没有io等待，配置线程数量为cpu核数加1。 num.io.threads主要进行磁盘io操作，高峰期可能有些io等待，因此配置需要大些。配置线程数量为cpu核数2倍，最大不超过3倍。 kafka JVM 调试 (机器实际内存的50% )\nvim /data/application/app/kafka_2.12-2.4.1/bin/kafka-server-start.sh\nif [ \u0026#34;x$KAFKA_HEAP_OPTS\u0026#34; = \u0026#34;x\u0026#34; ]; then export KAFKA_HEAP_OPTS=\u0026#34;-Xmx8G -Xms8G\u0026#34; fi 启动服务 /data/application/app/kafka_2.12-2.4.1/bin/kafka-server-start.sh -daemon /data/application/app/kafka_2.12-2.4.1/config/server.properties \u0026amp; 查看端口 netstat -lntp | grep \u0026#39;9092\u0026#39; hbase 部署","permalink":"https://xingxing.io/posts/data/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%83%A8%E7%BD%B2/","tags":["data"],"title":"大数据部署"},{"categories":["data"],"contents":"hbase 安装\nzookeeper\nzoo.cfg 配置文件\ntickTime=2000 initLimit=10 syncLimit=5 dataDir=/data/zookeeper clientPort=2181 maxClientCnxns=60 autopurge.snapRetainCount=10 autopurge.purgeInterval=1 ","permalink":"https://xingxing.io/posts/data/hbase-%E5%AE%89%E8%A3%85/","tags":["data"],"title":"Hbase 安装"},{"categories":["openapi"],"contents":"openapi\n","permalink":"https://xingxing.io/posts/devops/openapi/","tags":["openapi"],"title":"Openapi"},{"categories":["kubernetes"],"contents":"CRD\n","permalink":"https://xingxing.io/posts/kubernetes/crd/","tags":["kubernetes"],"title":"CRD"},{"categories":["devops"],"contents":"canal mysql 到mysql 数据同步\n使用canal 的tcp 模式达到mysql 到mysql 的数据同步\ncanal-server 安装 apiVersion: v1 kind: ConfigMap metadata: name: canal-server namespace: iot data: admin_manager: \u0026#34;canal-admin:8089\u0026#34; admin_port: \u0026#34;11110\u0026#34; admin_user: \u0026#34;admin\u0026#34; admin_password: \u0026#34;6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9\u0026#34; # 123456加密 #admin_register_cluster: \u0026#34;local\u0026#34; ##集群名称 admin_register_auto: \u0026#34;true\u0026#34; admin_register_name: \u0026#34;iot-canal-server\u0026#34; ##canal-server 自动注册名称 --- kind: StatefulSet ##canal-server 采用StatefulSet 方式部署 apiVersion: apps/v1 metadata: name: canal-server namespace: iot labels: app.kubernetes.io/name: canal-server app: canal-server spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: canal-server app: canal-server template: metadata: name: canal-server labels: app.kubernetes.io/name: canal-server app: canal-server spec: imagePullSecrets: - name: secret containers: - name: canal-server image: \u0026#39;canal/canal-server:v1.1.6\u0026#39; imagePullPolicy: Always ports: - name: tcp containerPort: 11111 protocol: TCP env: - name: POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: SERVICE_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: \u0026#39;metadata.labels[\u0026#39;\u0026#39;app\u0026#39;\u0026#39;]\u0026#39; - name: STS_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace - name: canal.register.ip value: $(POD_NAME).$(SERVICE_NAME).$(STS_NAMESPACE) ## canal-server采用实例名，防止每次重启导致的IP不同，Instance实例选择时的变化 - name: canal.admin.manager valueFrom: configMapKeyRef: name: canal-server key: admin_manager - name: canal.admin.port valueFrom: configMapKeyRef: name: canal-server key: admin_port - name: canal.admin.user valueFrom: configMapKeyRef: name: canal-server key: admin_user - name: canal.admin.passwd valueFrom: configMapKeyRef: name: canal-server key: admin_password #- name: canal.admin.register.cluster # valueFrom: # configMapKeyRef: # name: canal-server # key: admin_register_cluster - name: canal.admin.register.auto valueFrom: configMapKeyRef: name: canal-server key: admin_register_auto - name: canal.admin.register.name valueFrom: configMapKeyRef: name: canal-server key: admin_register_name resources: requests: cpu: 250m memory: 1024Mi livenessProbe: tcpSocket: port: 11112 initialDelaySeconds: 10 timeoutSeconds: 5 periodSeconds: 30 readinessProbe: tcpSocket: port: 11112 initialDelaySeconds: 10 timeoutSeconds: 5 periodSeconds: 30 restartPolicy: Always nodeSelector: {} affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 podAffinityTerm: labelSelector: matchLabels: app.kubernetes.io/name: canal-server namespaces: - iot topologyKey: kubernetes.io/hostname serviceName: canal-server revisionHistoryLimit: 10 --- kind: Service apiVersion: v1 metadata: name: canal-server namespace: iot spec: ports: - name: http nodePort: 31608 port: 11110 protocol: TCP targetPort: 11110 - name: tcp11111 nodePort: 31749 port: 11111 protocol: TCP targetPort: 11111 - name: tcp11112 nodePort: 30822 port: 11112 protocol: TCP targetPort: 11112 type: NodePort selector: app.kubernetes.io/name: canal-server app: canal-server canal-admin apiVersion: v1 kind: ConfigMap metadata: name: canal-admin namespace: iot data: admin_user: \u0026#34;admin\u0026#34; admin_password: \u0026#34;123456\u0026#34; datasource_address: \u0026#34;mysql:3306\u0026#34; datasource_database: \u0026#34;canal_manager\u0026#34; datasource_username: \u0026#34;canal\u0026#34; datasource_password: \u0026#34;canal\u0026#34; --- kind: Deployment apiVersion: apps/v1 metadata: name: canal-admin namespace: iot labels: app.kubernetes.io/name: canal-admin spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: canal-admin template: metadata: name: canal-admin creationTimestamp: null labels: app.kubernetes.io/name: canal-admin spec: containers: - name: canal-admin image: \u0026#39;canal/canal-admin:v1.1.6\u0026#39; imagePullPolicy: IfNotPresent ports: - name: web containerPort: 8089 protocol: TCP env: - name: server.port value: \u0026#39;8089\u0026#39; - name: canal.adminUser valueFrom: configMapKeyRef: name: canal-admin key: admin_user - name: canal.adminPasswd valueFrom: configMapKeyRef: name: canal-admin key: admin_password - name: spring.datasource.address valueFrom: configMapKeyRef: name: canal-admin key: datasource_address - name: spring.datasource.database valueFrom: configMapKeyRef: name: canal-admin key: datasource_database - name: spring.datasource.username valueFrom: configMapKeyRef: name: canal-admin key: datasource_username - name: spring.datasource.password valueFrom: configMapKeyRef: name: canal-admin key: datasource_password resources: requests: cpu: 250m memory: 256Mi livenessProbe: httpGet: path: / port: 8089 initialDelaySeconds: 10 timeoutSeconds: 5 periodSeconds: 30 readinessProbe: httpGet: path: / port: 8089 initialDelaySeconds: 10 timeoutSeconds: 5 periodSeconds: 30 restartPolicy: Always nodeSelector: {} affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 podAffinityTerm: labelSelector: matchLabels: app.kubernetes.io/name: canal-admin namespaces: - iot topologyKey: kubernetes.io/hostname revisionHistoryLimit: 10 --- kind: Service apiVersion: v1 metadata: name: canal-admin namespace: iot spec: type: NodePort ports: - protocol: TCP port: 8089 targetPort: 8089 selector: app.kubernetes.io/name: canal-admin 表同步配置 创建Instance，这里需要注意Instance 的名字要和在配置文件的名字一致，否则会同步不了数据\n创建一个名为dc-canal_dic_dt_data 的Instance\n##需要修改部分 # position info canal.instance.master.address=172.19.0.31:32060 canal.instance.master.journal.name= canal.instance.master.position= canal.instance.master.timestamp= canal.instance.master.gtid= # username/password canal.instance.dbUsername=canal canal.instance.dbPassword=canal canal.instance.connectionCharset = UTF-8 # enable druid Decrypt database password canal.instance.enableDruid=false canal.instance.filter.regex=dc-cana.* #可以搜索同步库下的所有表，只需要做映射即可 备注\n# 如果是同步一个表的话，就是db.table;如果是多个表同步可以显示写出来或者用正则匹配 # db.table1,db.table2,db.table3 #mysql 数据解析关注的表，Perl正则表达式. #多个正则之间以逗号(,)分隔，转义符需要双斜杠(\\\\) #常见例子： #1. 所有表：.* or .*\\\\..* #2. canal schema下所有表： canal\\\\..* #3. canal下的以canal打头的表：canal\\\\.canal.* #4. canal schema下的一张表：canal\\\\.test1 #5. 多个规则组合使用：canal\\\\..*,mysql.test1,mysql.test2 (逗号分隔) canal.adapter Client-adapter 分为适配器与启动器两部分，\ncanal.adapter-1.1.6.tar.gz ","permalink":"https://xingxing.io/posts/devops/canal%E9%85%8D%E7%BD%AEmysql-mysql%E5%90%8C%E6%AD%A5/","tags":["devops"],"title":"Canal Dts"},{"categories":["kubernetes"],"contents":"污点与容忍度\n节点亲和性是Pod 的一种属性，它使Pod 被吸引到一类特定的节点\n","permalink":"https://xingxing.io/posts/kubernetes/%E6%B1%A1%E7%82%B9%E4%B8%8E%E5%AE%B9%E5%BF%8D/","tags":["kubernetes"],"title":"污点与容忍"},{"categories":["kubernetes"],"contents":"kubectl 命令补齐工具\ncentos 下安装\nyum install -y bash-completion source /usr/share/bash-completion/bash_completion source \u0026lt;(kubectl completion bash) echo \u0026#34;source \u0026lt;(kubectl completion bash)\u0026#34; \u0026gt;\u0026gt; ~/.bashrc mac\n在 oh-my-zsh/plugins 插件目录下看下有没有kubectl 插件\nvim ~/.zshrc plugins=( git zsh-autosuggestions kubectl docker golang ) ","permalink":"https://xingxing.io/posts/kubernetes/kubectl-%E5%91%BD%E4%BB%A4%E8%A1%A5%E9%BD%90%E5%B7%A5%E5%85%B7/","tags":["kubernetes"],"title":"Kubectl 命令补齐工具"},{"categories":["devops"],"contents":"迁移工具\n","permalink":"https://xingxing.io/posts/devops/%E8%BF%81%E7%A7%BB%E5%B7%A5%E5%85%B7/","tags":["devops"],"title":"迁移工具"},{"categories":["devops"],"contents":"Canal 主要用途是基于MySQL 数据库增量日志解析，提供增量数据订阅和消费\n部署canal 单机版 canal 的工作原理\nMySQL 安装 mysql 主要用来存储canal 的一些信息\nmysql 创建pvc 存储\n--- apiVersion: v1 kind: PersistentVolume metadata: name: mysql-pv namespace: iot spec: capacity: storage: 50Gi volumeMode: Filesystem accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Recycle storageClassName: slow nfs: path: /data/mysql-data server: ###IP 地址 --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-pvc namespace: iot spec: accessModes: - ReadWriteMany volumeMode: Filesystem resources: requests: storage: 50Gi storageClassName: slow apiVersion: v1 kind: Service metadata: name: mysql namespace: iot spec: type: NodePort ports: - port: 3306 selector: app: mysql --- apiVersion: apps/v1 kind: Deployment metadata: name: mysql namespace: iot spec: selector: matchLabels: app: mysql strategy: type: Recreate template: metadata: labels: app: mysql spec: containers: - image: mysql:8.0.19 name: mysql env: # 在实际中使用 secret - name: MYSQL_ROOT_PASSWORD value: bdtp@2022 ports: - containerPort: 3306 name: mysql volumeMounts: - name: mysql-data mountPath: /var/lib/mysql volumes: - name: mysql-data persistentVolumeClaim: claimName: mysql-pvc 创建canal 数据库 create database canal_manager default character set utf8mb4 collate utf8mb4_unicode_ci; CREATE USER \u0026#39;canal\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED WITH mysql_native_password BY \u0026#39;canal\u0026#39;; grant all on canal_manager.* to \u0026#39;canal\u0026#39;@\u0026#39;%\u0026#39; ; flush privileges; #其中8.0远程连接要修改为mysql_native_password 认证连接，否则连接失败，如果不是mysql_native_password 则使用以下修改语句 ## ALTER USER \u0026#39;canal\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED WITH mysql_native_password BY \u0026#39;canal\u0026#39;; 导入sql https://github.com/alibaba/canal/blob/canal-1.1.6/admin/admin-web/src/main/resources/canal_manager.sql canal-admin 创建 apiVersion: v1 kind: ConfigMap metadata: name: canal-admin namespace: iot data: admin_user: \u0026#34;admin\u0026#34; admin_password: \u0026#34;123456\u0026#34; datasource_address: \u0026#34;mysql:3306\u0026#34; datasource_database: \u0026#34;canal_manager\u0026#34; datasource_username: \u0026#34;canal\u0026#34; datasource_password: \u0026#34;canal\u0026#34; --- kind: Deployment apiVersion: apps/v1 metadata: name: canal-admin namespace: iot labels: app.kubernetes.io/name: canal-admin spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: canal-admin template: metadata: name: canal-admin creationTimestamp: null labels: app.kubernetes.io/name: canal-admin spec: containers: - name: canal-admin image: \u0026#39;canal/canal-admin:v1.1.6\u0026#39; imagePullPolicy: IfNotPresent ports: - name: web containerPort: 8089 protocol: TCP env: - name: server.port value: \u0026#39;8089\u0026#39; - name: canal.adminUser valueFrom: configMapKeyRef: name: canal-admin key: admin_user - name: canal.adminPasswd valueFrom: configMapKeyRef: name: canal-admin key: admin_password - name: spring.datasource.address valueFrom: configMapKeyRef: name: canal-admin key: datasource_address - name: spring.datasource.database valueFrom: configMapKeyRef: name: canal-admin key: datasource_database - name: spring.datasource.username valueFrom: configMapKeyRef: name: canal-admin key: datasource_username - name: spring.datasource.password valueFrom: configMapKeyRef: name: canal-admin key: datasource_password resources: requests: cpu: 250m memory: 256Mi livenessProbe: httpGet: path: / port: 8089 initialDelaySeconds: 10 timeoutSeconds: 5 periodSeconds: 30 readinessProbe: httpGet: path: / port: 8089 initialDelaySeconds: 10 timeoutSeconds: 5 periodSeconds: 30 restartPolicy: Always nodeSelector: {} affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 podAffinityTerm: labelSelector: matchLabels: app.kubernetes.io/name: canal-admin namespaces: - iot topologyKey: kubernetes.io/hostname revisionHistoryLimit: 10 --- kind: Service apiVersion: v1 metadata: name: canal-admin namespace: iot spec: type: NodePort ports: - protocol: TCP port: 8089 targetPort: 8089 selector: app.kubernetes.io/name: canal-admin canal-server 部署\napiVersion: v1 kind: ConfigMap metadata: name: canal-server namespace: iot data: admin_manager: \u0026#34;canal-admin:8089\u0026#34; admin_port: \u0026#34;11110\u0026#34; admin_user: \u0026#34;admin\u0026#34; admin_password: \u0026#34;6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9\u0026#34; # 123456加密 #admin_register_cluster: \u0026#34;local\u0026#34; ##集群名称 admin_register_auto: \u0026#34;true\u0026#34; admin_register_name: \u0026#34;iot-canal-server\u0026#34; ##canal-server 自动注册名称 --- kind: StatefulSet ##canal-server 采用StatefulSet 方式部署 apiVersion: apps/v1 metadata: name: canal-server namespace: iot labels: app.kubernetes.io/name: canal-server app: canal-server spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: canal-server app: canal-server template: metadata: name: canal-server labels: app.kubernetes.io/name: canal-server app: canal-server spec: imagePullSecrets: - name: secret containers: - name: canal-server image: \u0026#39;canal/canal-server:v1.1.6\u0026#39; imagePullPolicy: Always ports: - name: tcp containerPort: 11111 protocol: TCP env: - name: POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: SERVICE_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: \u0026#39;metadata.labels[\u0026#39;\u0026#39;app\u0026#39;\u0026#39;]\u0026#39; - name: STS_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace - name: canal.register.ip value: $(POD_NAME).$(SERVICE_NAME).$(STS_NAMESPACE) ## canal-server采用实例名，防止每次重启导致的IP不同，Instance实例选择时的变化 - name: canal.admin.manager valueFrom: configMapKeyRef: name: canal-server key: admin_manager - name: canal.admin.port valueFrom: configMapKeyRef: name: canal-server key: admin_port - name: canal.admin.user valueFrom: configMapKeyRef: name: canal-server key: admin_user - name: canal.admin.passwd valueFrom: configMapKeyRef: name: canal-server key: admin_password #- name: canal.admin.register.cluster # valueFrom: # configMapKeyRef: # name: canal-server # key: admin_register_cluster - name: canal.admin.register.auto valueFrom: configMapKeyRef: name: canal-server key: admin_register_auto - name: canal.admin.register.name valueFrom: configMapKeyRef: name: canal-server key: admin_register_name resources: requests: cpu: 250m memory: 256Mi livenessProbe: tcpSocket: port: 11112 initialDelaySeconds: 10 timeoutSeconds: 5 periodSeconds: 30 readinessProbe: tcpSocket: port: 11112 initialDelaySeconds: 10 timeoutSeconds: 5 periodSeconds: 30 restartPolicy: Always nodeSelector: {} affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 podAffinityTerm: labelSelector: matchLabels: app.kubernetes.io/name: canal-server namespaces: - iot topologyKey: kubernetes.io/hostname serviceName: canal-server revisionHistoryLimit: 10 --- kind: Service apiVersion: v1 metadata: name: canal-server namespace: iot spec: ports: - protocol: TCP port: 11110 targetPort: 11110 type: ClusterIP selector: app.kubernetes.io/name: canal-server app: canal-server canlal-admin 配置修改 kafka 同步 canal-admin 运行正常之后，就可以通过 http://127.0.0.1:8089/ 访问，默认密码：admin/123456\n可以看到默认已经注册了\n点击操作—修改主配置-点击保存即可\n# tcp, kafka, rocketMQ, rabbitMQ canal.serverMode = kafka # 默认是tcp模式，修改为kafka ######### Kafka ############# ################################################## kafka.bootstrap.servers = 172.19.0.64:9092 kafka.acks = all kafka.compression.type = none kafka.batch.size = 16384 kafka.linger.ms = 1 kafka.max.request.size = 1048576 kafka.buffer.memory = 33554432 kafka.max.in.flight.requests.per.connection = 1 kafka.retries = 0 kafka.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\u0026#39;用户名\u0026#39; password=\u0026#39;密码\u0026#39;; kafka.sasl.mechanism=PLAIN kafka.security.protocol=PLAINTEXT kafka.kerberos.enable = false kafka.kerberos.krb5.file = \u0026#34;../conf/kerberos/krb5.conf\u0026#34; kafka.kerberos.jaas.file = \u0026#34;../conf/kerberos/jaas.conf\u0026#34; canal 创建instance 配置kafka 同步\n## 需要修改的部分，源库的ip数据库地址 canal.instance.master.address=127.0.0.1:3306 # 源库的用户名与密码 # username/password canal.instance.dbUsername=canal canal.instance.dbPassword=canal # 匹配同步的数据库规则 # table regex canal.instance.filter.regex=.*\\\\..* # table black regex canal.instance.filter.black.regex= # mq config # kafka topic 的名字，可以在kafka创建，与kafka topic对应，如果不填写，会自动创建 canal.mq.topic=example #备注： 需要在源库中创建用户名与密码并且同时具有同步的权限 GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO \u0026#39;canal\u0026#39;@\u0026#39;%\u0026#39;; FLUSH PRIVILEGES; kafka 查看消息 也可以查看instance 的日志\nrabbitmq 同步 rabbitmq 的k8s 部署\n--- apiVersion: apps/v1 kind: Deployment metadata: name: rabbitmq spec: replicas: 1 selector: matchLabels: app: rabbitmq template: metadata: labels: app: rabbitmq spec: containers: - name: rabbitmq image: rabbitmq:3.7.13-management imagePullPolicy: IfNotPresent ports: - containerPort: 5672 name: server-port - containerPort: 15672 name: http-port env: - name: RABBITMQ_DEFAULT_VHOST value: /iot - name: RABBITMQ_DEFAULT_USER value: admin - name: RABBITMQ_DEFAULT_PASS value: bdtp@2023 volumeMounts: - name: persisitent-storage-rabbitmq mountPath: /var/lib/rabbitmq - name: localtime readOnly: true mountPath: /etc/localtime volumes: - name: persisitent-storage-rabbitmq hostPath: path: /data/rabbitmq - name: localtime hostPath: type: File path: /etc/localtime --- apiVersion: v1 kind: Service metadata: name: rabbitmq-service labels: app: rabbitmq spec: selector: app: rabbitmq type: NodePort ports: - name: server-port port: 5672 targetPort: 5672 - name: http-port port: 15672 targetPort: 15672 nodePort: 30351 在rabbitmq 上创建exchange\n创建queues\nqueues 绑定exchanges\n自定义Routig key 名字\n修改主配置文件canal.properties\ncanal.serverMode = rabbitmq ################################################## ######### RabbitMQ\t############# ################################################## rabbitmq.host = rabbitmq-service.iot rabbitmq.virtual.host = /iot rabbitmq.exchange = canal-exchanges rabbitmq.routingKey = canal-routing rabbitmq.username = admin rabbitmq.password = bdtp@2023 rabbitmq.deliveryMode = direct rabbitmq.queue = canal-queues 修改配置文件instance.propertios\ncanal.instance.master.address=192.168.101.32:32060 # username/password canal.instance.dbUsername=canal canal.instance.dbPassword=canal canal.instance.connectionCharset = UTF-8 # enable druid Decrypt database password canal.instance.enableDruid=false # table regex canal.instance.filter.regex=test01.test_ddl #同步的表 # table black regex canal.instance.filter.black.regex= # mq config canal.mq.topic=canal-routing # 这里填写的是rabbitmq.routingKey # dynamic topic route by schema or table regex #canal.mq.dynamicTopic=mytest1.user,mytest2\\\\..*,.*\\\\..* canal.mq.partition=0 # hash partition config #canal.mq.partitionsNum=3 #canal.mq.partitionHash=test.table:id^name,.*\\\\..* ################################################# 验证rabbitmq 查看消息\n","permalink":"https://xingxing.io/posts/devops/canal/","tags":["devops"],"title":"Canal"},{"categories":["devops"],"contents":"seata 部署\n","permalink":"https://xingxing.io/posts/devops/seata-%E9%83%A8%E7%BD%B2/","tags":["devops"],"title":"Seata 部署"},{"categories":["devops"],"contents":"elasticsearch 故障\n故障1 解决 索引的偏移量默认是100000，超过了\n修改最大索引迁移量\ncurl -XPUT \u0026#34;http://IP:9200/_settings\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; { \u0026#34;index\u0026#34; : { \u0026#34;highlight.max_analyzed_offset\u0026#34; : 100000000 } }’ ","permalink":"https://xingxing.io/posts/devops/elasticsearch-%E6%95%85%E9%9A%9C/","tags":["devops"],"title":"Elasticsearch 故障"},{"categories":["nfs 静态"],"contents":"nfs 静态\n","permalink":"https://xingxing.io/posts/kubernetes/nfs-%E9%9D%99%E6%80%81/","tags":["nfs 静态"],"title":"Nfs 静态"},{"categories":["kubernetes"],"contents":"nfs 配置\nNFS-client-provisioner 是一个开源的NFS 外部Provisioner，利用NFS Server 为kubernetes 集群提供持久话存储。\n","permalink":"https://xingxing.io/posts/kubernetes/nfs-%E5%8A%A8%E6%80%81/","tags":["kubernetes"],"title":"Nfs 动态"},{"categories":["devops"],"contents":"prometheus 实战\nPrometheus 配置文件 global: #全局配置 scrape_interval: 15s # 采集数据时间间隔 scrape_timeout: 15s # 采集数据超时时间 rule_file: 告警规则 scrape_configs: 配置被监控端，称为target，每个target 用job_name 分组管理，又分为静态配置和服务发现 alerting: 告警配置 remote_write/remote_read: 从远程数据库读写 配置被监控端 目标(targets): 被监控端 实例(instances): 每个被监控端称为实例 作业(Job): 具有相同目标的实例集合称为作业 数据模型: Prometheus将所有数据存储为时间序列 具有相同度量名称以及标签属于同一个指标 每个时间序列都由标准名称和一组键值对(称为标签)唯一标识 通过标签查询指定指标 指标格式： \u0026lt;metric name\u0026gt;{\u0026lt;label name\u0026gt;=\u0026lt;label value\u0026gt;...} ","permalink":"https://xingxing.io/posts/devops/prometheus-%E5%AE%9E%E6%88%98/","tags":["devops"],"title":"Prometheus 实战"},{"categories":["data"],"contents":"hbase 基础\nHbase 介绍\nHbase 是一个面向列式存储的分布式数据库，底层存储基于HDFS实现，集群管理基于ZooKeeper 实现。Hbase 良好的分布式架构设计为海量数据快速存储、随机访问提供了可能，基于数据副本机制和分区机制可以轻松实现在线扩容、缩容和数据容灾 Hbase 知识框架\nHbase 定义\nApache Hbase 是以hdfs 为数据存储的，一种分布式、可扩展的NoSQL 数据库\nHbase 数据模型\n​ Hbase 的设计理念依据Google 的Bigtable 论文，论文中对于数据模型的首句介绍。Bigtable 是一个稀疏的、分布式的、持久的多维排序map。\n​\n","permalink":"https://xingxing.io/posts/data/hbase-%E5%9F%BA%E7%A1%80/","tags":["data"],"title":"Hbase 基础"},{"categories":["kubernetes"],"contents":"在k8s 上单独安装rancher\n安装了原生k8s，需要使用rancher 来管理 参考文档 helm 添加racnher helm repo add rancher-stable https://releases.rancher.com/server-charts/stable # 国内镜像源 helm repo add rancher-stable http://rancher-mirror.oss-cn-beijing.aliyuncs.com/server-charts/stable 为rancher 创建命名空间 kubectl create namespace cattle-system 选择自签证书\n参考证书生成脚本 安装rancher helm install rancher rancher-stable/rancher --version=2.6.8 --namespace cattle-system --set hostname=rancher.my.org --set bootstrapPassword=admin --set ingress.tls.source=secret --set privateCA=true --set systemDefaultRegistry=registry.cn-hangzhou.aliyuncs.com --set rancherImage=registry.cn-hangzhou.aliyuncs.com/rancher/rancher 添加TLS kubectl -n cattle-system create secret tls tls-rancher-ingress \\ --cert=tls.crt \\ --key=tls.key #使用私有CA证书执行 kubectl -n cattle-system create secret generic tls-ca \\ --from-file=cacerts.pem=./cacerts.pem 修改副本数 kubectl -n rancher get deploy rancher NAME READY UP-TO-DATE AVAILABLE AGE rancher 0/3 3 0 16m ## 修改副本数 kubectl scale --replicas=1 deployment/rancher -n cattle-system 删除资源 kubectl patch crd users.management.cattle.io -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;finalizers\u0026#34;:null}}\u0026#39; --type=merge ","permalink":"https://xingxing.io/posts/kubernetes/rancher-install/","tags":["kubernetes"],"title":"Rancher Install"},{"categories":["kubenetes"],"contents":"namespace 资源难以删除问题\n安装的rancher 有问题，需要卸载，在删除ns 的时候发现无论怎么删除都无法删除，状态如下 kubectl get ns NAME STATUS AGE c-m-xxwr49vg Terminating 15h cattle-fleet-clusters-system Terminating 20h cattle-fleet-local-system Terminating 20h cattle-fleet-system Terminating 20h cattle-global-data Terminating 20h cattle-global-nt Terminating 20h cattle-impersonation-system Active 20h cattle-system Terminating 22h cert-manager Active 20h cluster-fleet-local-local-1a3d67d0a899 Terminating 20h default Active 62d fleet-default Terminating 20h fleet-local Terminating 20h iot Active 62d kube-flannel Active 62d kube-mon Active 49d kube-node-lease Active 62d kube-public Active 62d kube-system Active 62d local Terminating 20h monitoring Active 33d p-j9snx Terminating 15h p-m9r5k Terminating 15h p-z5f57 Terminating 20h user-b78gj Terminating 15h 开始删除 查看集群可以命名空间隔离的资源\nkubectl api-resources -o name --verbs=list --namespaced | xargs -n 1 kubectl get --show-kind --ignore-not-found -n p-j9snx 使用原生接口删除\nkubectl get ns p-j9snx -o json \u0026gt; p-j9snx.json 编辑json 文件,删除以下spec部分\n\u0026#34;spec\u0026#34;: { \u0026#34;finalizers\u0026#34;: [ \u0026#34;kubernetes\u0026#34; ] }, 在终端中执行\nkubectl proxy 在另外一个终端中执行\ncurl -k -H \u0026#34;Content-Type: application/json\u0026#34; -X PUT --data-binary @p-j9snx.json http://127.0.0.1:8001/api/v1/namespaces/p-j9snx/finalize 查看命名空间是否删除\nkubectl get ns #查看已删除了 还有一种是删除方式是spec里面是空的\n\u0026#34;spec\u0026#34;: {}, 直接去删除yaml 文件\nkubectl edit ns p-z5f57 -o yaml 删除之后就立即删除了ns\n","permalink":"https://xingxing.io/posts/kubernetes/namespace-%E8%B5%84%E6%BA%90%E9%9A%BE%E4%BB%A5%E5%88%A0%E9%99%A4%E9%97%AE%E9%A2%98/","tags":["kubenetes"],"title":"Namespace 资源难以删除问题"},{"categories":["devops"],"contents":"prometheus\nprometheus 简介 prometheus 是一个开源监控系统和时序数据库，适合监控云原生环境，具有多维数据模型和强大的查询语句，并在一个生态系统中集成了检测、指标收集、服务发现和报警等功能。\n架构 模块介绍 Retrieval: 是负责定时去暴露的目标页面上去抓去采用指标数据 Storage: 是负责将采用数据写入指定的时序数据库存储 ProMQL: 是prometheus 提供的查询语言模块。可以和一些webui 或者grfana 集成 Jobs/Exporters: Prometheus可以从Jobs 或Exporters 中拉取监控数据。Exporter 以web API 的形式对外暴露数据采集接口。 Prometheus Server: Prometheus 还可以从其他的Prometheus Server 中拉取 Pushgateway: 对于一些以临时性Job 运行的组件，Prometheus 可能还没有来记得从中pull 监控数据的情况下，这些Job已经结束了，Job运行时可以在运行时将监控数据推送到Pushgateway中，Prometheus 从Pushgateway 中拉取数据，防止监控数据丢失 Service discovery: 是指Prometheus可以动态的发现一些服务，拉取数据进行监控 AlertManager:是一个独立于Prometheus 的外部组件，用于监控系统的告警，通过配置文件可以配置一些告警规则，Prometheus 会把告警推送到AlertManager 数据模型 Prometheus 采集的监控数据都是以指标(metric)的形式存储在内置的TSDB数据库中，这些数据都是时间序列；一个带时间戳的数据，这些数据具有一个标识符和一组样本值。除了存储的时间序列，Prometheus 还可以根据查询请求产生临时的、衍生的时间序列作为返回结果。\n时间序列 Prometheus 会将所有采集到的样本数据以时间序列的形式保存在内存数据库中，并定时刷新到硬盘上，时间序列是按照时间戳和值的序列方式存放的，我们可以称之为向量，每一条时间序列都由一个指标名称和一组标签(键值对)来唯一标识。\n部署 参考 https://prometheus.io/download/ kubernetes上的部署\nrbac.yaml\napiVersion: v1 kind: ServiceAccount metadata: name: prometheus namespace: kube-mon --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: prometheus rules: - apiGroups: - \u0026#34;\u0026#34; resources: - nodes - services - endpoints - pods - nodes/proxy verbs: - get - list - watch - apiGroups: - \u0026#34;extensions\u0026#34; resources: - ingresses verbs: - get - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - configmaps - nodes/metrics verbs: - get - nonResourceURLs: - /metrics verbs: - get --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: prometheus roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheus subjects: - kind: ServiceAccount name: prometheus namespace: kube-mon prometheus.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: prometheus namespace: kube-mon labels: app: prometheus spec: selector: matchLabels: app: prometheus template: metadata: labels: app: prometheus spec: serviceAccountName: prometheus #自动获取的 containers: - image: prom/prometheus:v2.38.0 name: prometheus args: - \u0026#34;--config.file=/etc/prometheus/prometheus.yml\u0026#34; - \u0026#34;--storage.tsdb.path=/prometheus\u0026#34; # 指定tsdb数据路径 - \u0026#34;--storage.tsdb.retention.time=24h\u0026#34; - \u0026#34;--web.enable-admin-api\u0026#34; # 控制对admin HTTP API的访问，其中包括删除时间序列等功能 - \u0026#34;--web.enable-lifecycle\u0026#34; # 支持热更新，直接执行localhost:9090/-/reload立即生效 ports: - containerPort: 9090 name: http volumeMounts: - mountPath: \u0026#34;/etc/prometheus\u0026#34; name: config-volume - mountPath: \u0026#34;prometheus\u0026#34; name: data resources: requests: memory: \u0026#34;2Gi\u0026#34; limits: memory: \u0026#34;4Gi\u0026#34; securityContext: # 如果不加,运行的时候会提示权限不足 runAsUser: 0 # volumes: - name: data persistentVolumeClaim: claimName: prometheus-data - configMap: name: prometheus-config name: config-volume restartPolicy: Always prometheus-svc.yaml\napiVersion: v1 kind: Service metadata: name: prometheus namespace: kube-mon labels: app: prometheus spec: selector: app: prometheus type: NodePort ports: - name: web port: 9090 targetPort: http prometheus-core-configmap.yml\napiVersion: v1 data: prometheus.yml: | global: scrape_interval: 10s scrape_timeout: 10s evaluation_interval: 10s alerting: alertmanagers: - static_configs: - targets: [\u0026#39;alertmanager:9093\u0026#39;] rule_files: - \u0026#34;/etc/prometheus-rules/*.rules\u0026#34; scrape_configs: - job_name: \u0026#39;telegraf\u0026#39; static_configs: - targets: [\u0026#39;10.96.18.19:9273\u0026#39;] #换成 telegraf 地址 - job_name: \u0026#39;kube-state-metrics\u0026#39; static_configs: - targets: [\u0026#39;kube-state-metrics.kube-mon.svc.cluster.local:8080\u0026#39;] - job_name: \u0026#39;kubernetes-apiservers\u0026#39; kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https # https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml#L37 - job_name: \u0026#39;kubernetes-nodes\u0026#39; kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics - job_name: \u0026#39;kubernetes-cadvisor\u0026#39; scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor # https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml#L79 - job_name: \u0026#39;kubernetes-endpoints\u0026#39; kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name # https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml#L119 - job_name: \u0026#39;kubernetes-services\u0026#39; metrics_path: /probe params: module: [http_2xx] kubernetes_sd_configs: - role: service relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.example.com:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name # https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml#L156 - job_name: \u0026#39;kubernetes-pods\u0026#39; kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name - job_name: \u0026#39;etcd\u0026#39; scheme: https #insecure_skip_verify: true static_configs: - targets: [\u0026#39;172.19.0.31:2379\u0026#39;] #替换成etcd地址 kind: ConfigMap metadata: name: prometheus-config namespace: kube-mon 部署kube-state-metrics\nKube-state-metrics 有版本对应，根据k8s 的版本选择对应的kube-state-metrics 版本\nkube-state-metrics-deploy.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: kube-state-metrics namespace: kube-mon spec: replicas: 1 selector: matchLabels: app: kube-state-metrics template: metadata: labels: app: kube-state-metrics spec: serviceAccountName: kube-state-metrics containers: - name: kube-state-metrics image: registry.cn-hangzhou.aliyuncs.com/kainstall/kube-state-metrics:v2.7.0 ports: - containerPort: 8080 kube-state-metrics-rbac.yaml\n--- apiVersion: v1 kind: ServiceAccount metadata: name: kube-state-metrics namespace: kube-mon --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: kube-state-metrics rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;nodes\u0026#34;, \u0026#34;pods\u0026#34;, \u0026#34;services\u0026#34;, \u0026#34;resourcequotas\u0026#34;, \u0026#34;replicationcontrollers\u0026#34;, \u0026#34;limitranges\u0026#34;, \u0026#34;persistentvolumeclaims\u0026#34;, \u0026#34;persistentvolumes\u0026#34;, \u0026#34;namespaces\u0026#34;, \u0026#34;endpoints\u0026#34;] verbs: [\u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;extensions\u0026#34;] resources: [\u0026#34;daemonsets\u0026#34;, \u0026#34;deployments\u0026#34;, \u0026#34;replicasets\u0026#34;] verbs: [\u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;apps\u0026#34;] resources: [\u0026#34;statefulsets\u0026#34;] verbs: [\u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;batch\u0026#34;] resources: [\u0026#34;cronjobs\u0026#34;, \u0026#34;jobs\u0026#34;] verbs: [\u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;autoscaling\u0026#34;] resources: [\u0026#34;horizontalpodautoscalers\u0026#34;] verbs: [\u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: kube-state-metrics roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kube-state-metrics subjects: - kind: ServiceAccount name: kube-state-metrics namespace: kube-mon kube-state-metrics-svc.yaml\napiVersion: v1 kind: Service metadata: labels: app: kube-state-metrics name: kube-state-metrics namespace: kube-mon spec: clusterIP: None clusterIPs: - None internalTrafficPolicy: Cluster ipFamilies: - IPv4 ipFamilyPolicy: SingleStack ports: - name: http-metrics port: 8080 protocol: TCP targetPort: http-metrics - name: telemetry port: 8081 protocol: TCP targetPort: telemetry selector: app: kube-state-metrics ","permalink":"https://xingxing.io/posts/devops/prometheus-%E5%AE%89%E8%A3%85/","tags":["devops"],"title":"Prometheus"},{"categories":["iot 组件容器部署"],"contents":"iot 组件容器部署\nInfluxDB部署 导出配置文件 docker pull influxdb:1.8.9-alpine #拉取最新版本镜像 docker run --rm influxdb:1.8.9-alpine influxd config \u0026gt; influxdb.conf #导出默认配置文件 K8s 部署 #创建ConfigMap kubectl create configmap influxdb-config --from-file influxdb.conf -n iot kubectl get cm influxdb-config -n # 查看内容 kubectl get cm influxdb-config -n iot -o yaml 存储卷 yum install nfs-utils -y #配置共享目录 vim /etc/exports /mnt/data/k8s-influxdb *(rw,no_root_squash) systemctl start nfs systemctl enable nfs #测试 mkdir /bac mount -t nfs IP:/mnt/data/k8s-influxdb /abc #创建PV pv.yaml apiVersion: v1 kind: PersistentVolume metadata: name: pv01 spec: capacity: storage: 20Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: slow nfs: path: /mnt/data/k8s-influxdb server: ip #ReadWriteMany # 创建pvc telegraf部署 docker run --rm telegraf:1.20 telegraf config \u0026gt; telegraf.conf kubectl create configmap telegraf-config --from-file telegraf.conf -n iot 部署 apiVersion: apps/v1 kind: Deployment metadata: name: telegraf namespace: iot labels: spec: replicas: 1 selector: matchLabels: app: telegraf template: metadata: labels: app: telegraf spec: containers: - name: telegraf image: telegraf:1.20 imagePullPolicy: IfNotPresent volumeMounts: - name: telegraf-config mountPath: /etc/telegraf volumes: - name: telegraf-config configMap: name: telegraf-config ","permalink":"https://xingxing.io/posts/devops/iot-%E7%BB%84%E4%BB%B6%E5%AE%B9%E5%99%A8%E9%83%A8%E7%BD%B2/","tags":["iot 组件容器部署"],"title":"Iot 组件容器部署"},{"categories":["kubernetes"],"contents":"k8s 单机版\n系统 配置 硬盘 centos 7.8 16C32GB 200GB 部署 1. 关闭swap分区 swapoff -a \u0026amp;\u0026amp; sysctl -w vm.swappiness=0 sed -ri \u0026#39;/^[^#]*swap/s@^@#@\u0026#39; /etc/fstab 2. 设置limit # vim /etc/security/limits.conf * soft nofile 655360 * hard nofile 131072 * soft nproc 655350 * hard nproc 655350 * soft memlock unlimited * hard memlock unlimited 3. 内核配置 cat \u0026gt; /etc/sysctl.d/kubernetes.conf \u0026lt;\u0026lt;EOF #将桥接的IPv4流量传递到iptables 的链 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system # 生效 4. containerd 安装 cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter # 设置所需的 sysctl 参数，参数在重新启动后保持不变 cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF # 应用 sysctl 参数而不重新启动 sudo sysctl --system #安装containerd yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine yum install -y yum-utils yum-config-manager \\ \u0026gt; --add-repo \\ \u0026gt; https://download.docker.com/linux/centos/docker-ce.repo yum install -y containerd.io containerd config default \u0026gt; /etc/containerd/config.toml systemctl restart containerd #vim /etc/containerd/config.toml 修改镜像 sandbox_image = \u0026#34;registry.aliyuncs.com/google_containers/pause:3.6\u0026#34; systemctl daemon-reload 5. 安装crictl https://github.com/kubernetes-sigs/cri-tools/releases\t6.kubectl https://kubernetes.io/zh-cn/docs/tasks/tools/install-kubectl-linux/#install-kubectl-binary-with-curl-on-linux #安装 v1.24.0 版本的kubectl curl -LO https://dl.k8s.io/release/v1.24.0/bin/linux/amd64/kubectl mv kubectl /usr/sbin/kubectl 7. kubeadm # 官方源，需要科学上网 cat \u0026lt;\u0026lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\\$basearch enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg exclude=kubelet kubeadm kubectl EOF # 将 SELinux 设置为 permissive 模式（相当于将其禁用） sudo setenforce 0 sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes sudo systemctl enable --now kubelet #改为阿里的源方便 cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF 初始化集群\n参考官网https://kubernetes.io/zh-cn/docs/reference/setup-tools/kubeadm/\nkubeadm init \\ --kubernetes-version=v1.24.0 \\ --pod-network-cidr=10.244.0.0/16 \\ --apiserver-advertise-address=172.19.0.31 \\ --image-repository registry.aliyuncs.com/google_containers 输出信息如下\nYour Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 172.19.0.31:6443 --token ojukna.dqkl862f0ajj7z9p \\ --discovery-token-ca-cert-hash sha256:8e02fa1e48ec71c683411b4cb24a9ef50b6f156fdfde055be98293e6adee1487 9. 去除污点 在mster 节点上部署Pod的话，需要删除node 的污点\nkubectl taint nodes --all node-role.kubernetes.io/master- kubectl taint nodes --all node-role.kubernetes.io/control-plane- node-role.kubernetes.io/master- kubectl describe node 节点名字 |grep Taints 10. 安装flannel 参考地址 https://github.com/flannel-io/flannel/blob/master/Documentation/kubernetes.md kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml 11. 故障解决 crictl images ls WARN[0000] image connect using default endpoints: [unix:///var/run/dockershim.sock unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead. ERRO[0000] unable to determine image API version: rpc error: code = Unavailable desc = connection error: desc = \u0026#34;transport: Error while dialing dial unix /var/run/dockershim.sock: connect: no such file or directory\u0026#34; #解决 vim /etc/crictl.yaml runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 10 debug: false #systemctl restart containerd coredns HTTP probe failed with statuscode: 503 是因为防火墙原因导致 systemctl stop firewalld 12 证书有效期 # 查看证书有效期 kubeadm certs check-expiration 更改证书到100年 下载源码修改改100年\nvim cmd/kubeadm/app/constants/constants.go vim staging/src/k8s.io/client-go/util/cert/cert.go #编译 kubeadm # 最好是在linux 环境中编译 KUBE_BUILD_PLATFORMS=linux/amd64 make all WHAT=cmd/kubeadm GOFLAGS=-v #编译 kubelet KUBE_BUILD_PLATFORMS=linux/amd64 make all WHAT=cmd/kubelet GOFLAGS=-v #编译 kubectl KUBE_BUILD_PLATFORMS=linux/amd64 make all WHAT=cmd/kubectl GOFLAGS=-v #所有组件都编译 KUBE_BUILD_PLATFORMS=linux/amd64 make all GOFLAGS=-v GOGCFLAGS=\u0026#34;-N -l\u0026#34; 查看证书到期时间 # 证书到期时间查看 kubeadm certs check-expiration #重置所有证书 kubeadm certs renew all #更新kubeconfig 文件 kubeadm init phase kubeconfig all mv $HOME/.kube/config $HOME/.kube/config.old cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config ","permalink":"https://xingxing.io/posts/kubernetes/k8s-%E5%8D%95%E6%9C%BA%E7%89%88/","tags":["kubernetes"],"title":"K8s 单机版"},{"categories":["Jenkins"],"contents":"Jenkins\n","permalink":"https://xingxing.io/posts/devops/jenkins/","tags":["Jenkins"],"title":"Jenkins"},{"categories":["kubernetes"],"contents":"velero 备份集群\nvelero 介绍 查看支持各个厂家的插件\nhttps://velero.io/docs/v1.10/supported-providers/ 安装 先按照velero 软件\nvelero install --provider aws --plugins velero/velero-plugin-for-aws:v1.2.1 --bucket ys-dev-k8s-bak-1307155645 \\ --secret-file ./credentials-velero \\ --use-restic \\ --default-volumes-to-restic \\ --backup-location-config \\ region=ap-beijing,s3ForcePathStyle=\u0026#34;true\u0026#34;,s3Url=https://cos.ap-beijing.myqcloud.com 或者 velero install --provider aws --plugins velero/velero-plugin-for-aws:v1.8.1 --use-volume-snapshots=false --bucket ys-dev-k8s-bak-1307155645 --secret-file ./credentials-velero --backup-location-config region=ap-beijing,s3ForcePathStyle=\u0026#34;true\u0026#34;,s3Url=https://cos.ap-beijing.myqcloud.com --use-node-agent #配置文件说明 vim credentials-velero ###是对象存储的认证文件 [default] aws_access_key_id = XXXX aws_secret_access_key = XXXX 安装完成查看 指定命名空间备份 velero backup create dmp-dev-backup --include-namespaces dmp-dev velero backup create dmp-dev-backup --include-namespaces dmp-dev Backup request \u0026#34;dmp-dev-backup\u0026#34; submitted successfully. Run `velero backup describe dmp-dev-backup` or `velero backup logs dmp-dev-backup` for more details. # 查看备份的进度 velero backup describe dmp-dev-backup --details 查看备份 备份完毕 查看备份是否有错误 velero backup logs dmp-test-backup | grep error 指定应用的备份 恢复 将备份存储位置临时更新为只读模式(可以防止在还原过程中，velero在备份存储位置中创建或删除备份对象)\nkubectl patch backupstoragelocation default --namespace velero \\ --type merge \\ --patch \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;accessMode\u0026#34;:\u0026#34;ReadOnly\u0026#34;}}\u0026#39; \\ backupstoragelocation.velero.io/default patched 执行恢复命令 velero restore create --from-backup dmp-dev-backup ##会显示如下内容\tRestore request \u0026#34;dmp-dev-backup-20220630094836\u0026#34; submitted successfully. Run `velero restore describe dmp-dev-backup-20220630094836` or `velero restore logs dmp-dev-backup-20220630094836` for more details. 查看恢复 velero restore get #查看恢复进度 velero restore describe dmp-dev-backup-20220630094836 --details 卸载velero kubectl delete namespace/velero clusterrolebinding/velero kubectl delete crds -l component=velero ","permalink":"https://xingxing.io/posts/kubernetes/velero-%E5%A4%87%E4%BB%BD%E9%9B%86%E7%BE%A4/","tags":["kubernetes"],"title":"Velero 备份集群"},{"categories":["data"],"contents":"pigsty centos 测试\n安装 1. 环境介绍 # 系统版本 cat /etc/redhat-release CentOS Linux release 7.9.2009 (Core) # 内核版本 Linux localhost.localdomain 3.10.0-1160.59.1.el7.x86_64 #1 SMP Wed Feb 23 16:47:03 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux 2. 下载 在管理节点上下载pigsty.tgz bash -c \u0026#34;$(curl -fsSL http://download.pigsty.cc/get)\u0026#34; ![image-20220406221131746](/Users/xingxing/Library/Application Support/typora-user-images/image-20220406221131746.png)\n下载完毕已经解压到了/root/pigsty 目录 Centos7.2 CentOS Linux release 7.2.1511 (Core) Linux localhost.localdomain 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux 安装 bash -c \u0026#34;$(curl -fsSL http://download.pigsty.cc/get)\u0026#34; ![image-20220412170516828](/Users/xingxing/Library/Application Support/typora-user-images/image-20220412170516828.png)\n故障处理\nrpm-sign-4.11.3-17.el7.x86_64 yum remove rpm-sign-4.11.3-17.el7.x86_64 集群安装 1. 集群安装 销毁数据库 -e 传入变量 -l 列出 -t 查看tak 强制覆盖式安装 2. 单机安装 3.声明与规划数据库 ","permalink":"https://xingxing.io/posts/data/pigsty-centos/","tags":["data"],"title":"Pigsty-centos7.9"},{"categories":["devops"],"contents":"mosquitto 制作镜像\n制作镜像 FROM centos:7 RUN yum install wget cmake make gcc* gcc-c++ net-tools openssl openssl-devel gcc-c++ -y \u0026amp;\u0026amp; wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo ADD mosquitto-2.0.14.tar.gz /data/ ADD v1.7.15.tar.gz /data/ WORKDIR /data/cJSON-1.7.15/ RUN make \u0026amp;\u0026amp; make install WORKDIR /data/mosquitto-2.0.14/ RUN make \u0026amp;\u0026amp; make install \u0026amp;\u0026amp; ldconfig \u0026amp;\u0026amp; yum -y update RUN rm -rf /data/v1.7.15.tar.gz \u0026amp;\u0026amp; rm -rf /data/mosquitto-2.0.14.tar.gz RUN echo /usr/local/lib \u0026gt; /etc/ld.so.conf.d/local.conf \u0026amp;\u0026amp; ldconfig COPY mosquitto.conf /etc/mosquitto/ EXPOSE 1883 CMD [\u0026#34;/usr/local/sbin/mosquitto\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;/etc/mosquitto/mosquitto.conf\u0026#34;] 配置单向认证 openssl genrsa -des3 -out ca.key 2048 # 证书密码 persagy 58.251.128.54,127.0.0.1,100.76.143.131,58.251.128.55 openssl req -new -x509 -days 1826 -key ca.key -out ca.crt openssl genrsa -out server.key 2048 openssl req -new -out server.csr -key server.key You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter \u0026#39;.\u0026#39;, the field will be left blank. ----- Country Name (2 letter code) [XX]:CN State or Province Name (full name) []:BJ Locality Name (eg, city) [Default City]:bj Organization Name (eg, company) [Default Company Ltd]:YS Organizational Unit Name (eg, section) []:ys Common Name (eg, your name or your server\u0026#39;s hostname) []: # 多个ip 地址 Email Address []: Please enter the following \u0026#39;extra\u0026#39; attributes to be sent with your certificate request A challenge password []: An optional company name []: openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 36000 openssl genrsa -out client.key 2048 openssl req -new -out client.csr -key client.key openssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out client.crt -days 36000 错误 error while loading shared libraries: libmosquitto.so.1: cannot open shared object file: No such file or directory error 18 at 0 depth lookup:self signed certificate #写在Dockerfile echo /usr/local/lib \u0026gt; /etc/ld.so.conf.d/local.conf \u0026amp;\u0026amp; ldconfig 验证 openssl verify -CAfile ca.crt server.crt 修改配置文件 bind_address 0.0.0.0 cafile /energy-sit/ca.crt certfile /energy-sit/server.crt keyfile /energy-sit/server.key 启动 mosquitto -c /etc/mosquitto/mosquitto.conf -d 订阅 mosquitto_sub -h xingxing.io -p 9883 -t \u0026#34;test\u0026#34; -u persagy -P persagy --cafile /energy-sit/server.crt --cert /energy-sit/client.crt --key /energy-sit/client.key --insecure 发布 mosquitto_pub -h xingxing.io -p 9883 -t \u0026#34;test\u0026#34; -m \u0026#39;nihaoa\u0026#39; -u persagy -P persagy --cafile /energy-sit/server.crt --cert /energy-sit/client.crt --key /energy-sit/client.key --insecure 改为wss+tls","permalink":"https://xingxing.io/posts/devops/mosquitto/","tags":["devops"],"title":"Mosquitto"},{"categories":["devops"],"contents":"腾讯云RDS数据库恢复\n环境准备 安装过程遇到报错\n去下载这个包 perl-DBD-MySQL-4.023-6.el7.x86_64 安装上即可, 如果有冲突的就需要 rpm -qa |grep mysql 找到关于mysql 的包全部卸载了 数据恢复 将腾讯云rds 的备份与mysqlbinlog 下载到本地 解压备份文件 使用xbstream 将数据文件解压到/data/mysql 目录\nxbstream -x --parallel=2 -C /data/mysql \u0026lt; /data/*****.xb 解压备份软件 下载安装qpress\nwget -d --user-agent=\u0026#34;Mozilla/5.0 (Windows NT x.y; rv:10.0) Gecko/20100101 Firefox/10.0\u0026#34; http://www.quicklz.com/qpress-11-linux-x64.tar tar -xf qpress-11-linux-x64.tar -C /usr/local/bin source /etc/profile 使用命令将目标目录下所有以.qb 结尾的文件解压出来\nxtrabackup --decompress --target-dir=/data/mysql Prepare 备份文件 备份解压出来之后，执行命令进行apply log 操作\nxtrabackup --prepare --target-dir=/data/mysql 修改配置文件 vi /data/mysql/backup-my.cnf - innodb_checksum_algorithm - innodb_log_checksum_algorithm - innodb_fast_checksum - innodb_page_size - innodb_log_block_size - redo_log_version # mysql 8.0 不添加此参数启动会报错，根据情况，初始配置有的是0 有些是1 ，根据报错修改 lower_case_table_names=1 修改文件属性 chown -R mysql:mysql /data/mysql 启动 mysqld 进程并登录验证 mysqld_safe --defaults-file=/data/mysql/backup-my.cnf --user=mysql --datadir=/data/mysql \u0026amp; # 备注，mysql 8.0 之后rpm 安装的是没有mysqld_safe 的，需要安装二进制的 如果有报错，可以看输出的日志进行解决\ncat /data/mysql/i-h01fyi82.err 查看日志是否有错误 登录mysql ./mysql -uroot -p # 输入之前的用户密码即可登录 mysql 忘记root 密码 因为root 用户不能在本地登录，登录其他账号提示\n#修改 vi /data/mysql/backup-my.cnf skip-grant-tables ##忽略mysql权限问题，直接登录 # 重新启动 mysqld_safe --defaults-file=/data/mysql/backup-my.cnf --user=mysql --datadir=/data/mysql \u0026amp; # 直接登录 mysql -utester -p 回车即可 use mysql ALTER USER \u0026#39;root\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;123456\u0026#39;; # 修改用户名与密码 flush privileges; #如果是tcp6 连接的导致客户端连接不上 # vi /data/mysql/backup-my.cnf bind-address=0.0.0.0 # 添加 #重启 mysqlbinlog 恢复 查看mysqlbinlog ./mysqlbinlog /opt/binlog_mysqlbin.000054 | less # 查看mysqlbinlog 选择要恢复的时间节点或者at 恢复数据 ./mysqlbinlog --stop-position=132281014 /opt/binlog_mysqlbin.000054 | /opt/mysql-8.0.27-el7-x86_64/bin/mysql -uroot -p # 提示 ERROR 1227 (42000) at line 3: Access denied; you need (at least one of) the SUPER, SYSTEM_VARIABLES_ADMIN, SESSION_VARIABLES_ADMIN or REPLICATION_APPLIER privilege(s) for this operation 解决方法 use mysql update user set Super_priv=\u0026#39;Y\u0026#39; where User=\u0026#39;root\u0026#39;; flush privileges; 命令行需要断开重新连接一下 # 再次执行恢复 ERROR 1781 (HY000) at line 24: @@SESSION.GTID_NEXT cannot be set to UUID:NUMBER when @@GLOBAL.GTID_MODE = OFF. 解决 show global variables like \u0026#39;gtid_mode\u0026#39;; set @@GLOBAL.GTID_MODE = OFF_PERMISSIVE; show global variables like \u0026#39;gtid_mode\u0026#39;; ##执行恢复 1. 首先备份的数据是3.22 00点备份的， mysqlbinlog 有2个备份， 第一个， mysqlbinlog 的恢复要从3月22日0:00 到第一个mysqlbinlog 备份到3月22日时间节点的结束， 第二个，mysqlbinlog 恢复从到开始时间到3月22日12点结束 #mysqlbinlog 也可以根据时间节点来恢复 --start-datetime \u0026amp; --stop-datetime 解析某一个时间段内的 binlog --start-position \u0026amp; --stop-position 解析在两个 position 之间的 binlog mysqlbinlog 恢复 #第一个binlog 的恢复基于at 的恢复 ./mysqlbinlog --start-position=143071637 --stop-position=301237997 /opt/binlog_mysqlbin.000053 |/opt/mysql-8.0.27-el7-x86_64/bin/mysql -u root -p # 第二个binlog 的恢复 恢复到at 132284243 ./mysqlbinlog --stop-position=132284243 /opt/binlog_mysqlbin.000054 | /opt/mysql-8.0.27-el7-x86_64/bin/mysql -uroot -p 验证数据完整性","permalink":"https://xingxing.io/posts/devops/rds-mysql-recover/","tags":["devops"],"title":"Rds Mysql Recover"},{"categories":["npm nexus"],"contents":"npm 私服使用中存在的问题\n","permalink":"https://xingxing.io/posts/devops/npm-nexus/","tags":["npm nexus"],"title":"Npm Nexus"},{"categories":["golang"],"contents":"映射 map 映射是存储一系列无序的key/value 对，通过key来对value 进行操作(增、删、改、查)。 映射的key 只能为可使用== 运算符的值类型(字符串、数字、布尔、数组)，value可以为任意类型\n1）声明 map 声明需要指定组成元素key 和value 的类型，在声明后，会被初始化为nil，表示暂不存在的映射\nvar names map[string]string 2）初始化 a) 使用字面量初始化:map[key]vtype{k1:v1,k2,v2....} b) 使用字面量初始化空映射: map[ktype]vtype{} c) 使用make 函数初始化 第一种方式 字面量初始化 var names map[string]string = map[string]string{\u0026#34;Go3037\u0026#34;: \u0026#34;陕西\u0026#34;} //字面量 第二种方式 make 初始化 var scores = make(map[string]int) //make 3) 增删改查 var names map[string]string = map[string]string{\u0026#34;Go3037\u0026#34;: \u0026#34;陕西\u0026#34;} //初始化，字面量 fmt.Printf(\u0026#34;%T %v\\n\u0026#34;, names, names) //kye 添加或者修改元素 names[\u0026#34;Go2004\u0026#34;] = \u0026#34;陕西西安\u0026#34; fmt.Println(names) names[\u0026#34;Go3037\u0026#34;] = \u0026#34;北京\u0026#34; fmt.Println(names) 判断是否存在 names[\u0026#34;Go001\u0026#34;] = \u0026#34;\u0026#34; fmt.Println(names[\u0026#34;Go001\u0026#34;], names[\u0026#34;Go002\u0026#34;]) v, ok := names[\u0026#34;Go001\u0026#34;] fmt.Println(v, ok) v, ok = names[\u0026#34;Go002\u0026#34;] fmt.Println(v, ok） 遍历 for k := range names { fmt.Println(k) } for k, v := range names { fmt.Println(k, v) } 删除 delete(names, \u0026#34;Go2004\u0026#34;) fmt.Println(names) ","permalink":"https://xingxing.io/posts/golang/%E6%98%A0%E5%B0%84-map/","tags":["golang"],"title":"映射 Map"},{"categories":["linux"],"contents":"vscode 乱码解决\n安装了vscode 之后，从vscode 里面打开终端是乱码，如何解决呢\n首次打开是乱码，此时打开mac 的终端看是否是乱码\nmac 的终端不是乱码 看一下mac 终端的字体\n此时需要终端的字体设置在vscode 上就可以了\n设置vscode 字体 选择设置打开setting.json 文件\n{ \u0026#34;terminal.external.osxExec\u0026#34;: \u0026#34;iTerm.app\u0026#34;, \u0026#34;terminal.integrated.shell.osx\u0026#34;: \u0026#34;zsh\u0026#34;, \u0026#34;terminal.integrated.fontFamily\u0026#34;: \u0026#34;MesloLGS NF\u0026#34;, \u0026#34;terminal.integrated.fontSize\u0026#34;: 12 } 乱码解决 ","permalink":"https://xingxing.io/posts/linux/vscode-%E4%B9%B1%E7%A0%81/","tags":["linux"],"title":"Vscode 乱码"},{"categories":["devops"],"contents":"gitlab update 升级测试 升级路线 参考官网路线 10.7-\u0026gt;10.8.7 -\u0026gt; 11.11.8 -\u0026gt; 12.0.12 -\u0026gt; 12.1.17 -\u0026gt; 12.10.14 -\u0026gt; 13.0.14 -\u0026gt; 13.1.11 -\u0026gt; 13.8.8 -\u0026gt; 13.12.15 -\u0026gt; latest 14.0.Z -\u0026gt; latest 14.1.Z -\u0026gt; latest 14.Y.Z https://docs.gitlab.com/ee/update/#upgrade-paths 官网的升级路线有问题纠正如下 10.7-\u0026gt;10.8.7 -\u0026gt; 11.11.8 -\u0026gt; 12.0.12 -\u0026gt; 12.1.17 -\u0026gt; 12.10.14 -\u0026gt; 13.0.14 -\u0026gt; 13.1.11 -\u0026gt; 13.8.8 -\u0026gt; 13.9.2-\u0026gt; 13.12.12 -\u0026gt; 14.0.11 -\u0026gt; 14.1.6-\u0026gt;14.1.7-\u0026gt;14.2.1-\u0026gt;14.2.5-\u0026gt;14.2.6-\u0026gt;14.2.7-\u0026gt; 14.3.4-\u0026gt;14.4.3-\u0026gt;14.5.2 恢复备份集并修改了数据库链接 vim gitlab.rb gitlab_rails[\u0026#39;db_host\u0026#39;] = \u0026#34;\u0026#34; 重启gitlab 遇到的错误\nWARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 解决 vim /etc/sysctl.conf net.core.somaxconn = 2048 2021-12-08_09:51:01.78141 612:M 08 Dec 09:51:01.781 # Fatal error loading the DB: Permission denied. Exiting. ==\u0026gt; /var/log/gitlab/gitlab-workhorse/current \u0026lt;== 2021-12-08_09:51:02.20495 time=\u0026#34;2021-12-08T09:51:02Z\u0026#34; level=info msg=\u0026#34;redis: dialing\u0026#34; address=/var/opt/gitlab/redis/redis.socket network=unix 2021-12-08_09:51:02.20498 time=\u0026#34;2021-12-08T09:51:02Z\u0026#34; level=error msg=\u0026#34;unknown error\u0026#34; error=\u0026#34;keywatcher: dial unix /var/opt/gitlab/redis/redis.socket: connect: connection refused\u0026#34; 解决 docker exec -ti gitlab bash chown gitlab-redis /var/opt/gitlab/redis/* 升级 拉取10.8.7 的镜像 https://hub.docker.com/r/gitlab/gitlab-ce/ 需要修改git.sh 启动脚本的镜像版本号 docker run -d \\ -p 80:80 \\ -p 443:443 \\ -p 22:22 \\ --name gitlab \\ --restart unless-stopped \\ -v /data/gitlab/config:/etc/gitlab \\ -v /data/gitlab/logs:/var/log/gitlab \\ -v /data/gitlab/data:/var/opt/gitlab \\ gitlab-ce:11.11.8-ce.0 #另外需要注意，每一个版本升级都需要替换gitlab.rb 配置文件 从10.8.7 升级到11.11.8 报错 .................. PG::UndefinedObject: ERROR: operator class \u0026#34;gin_trgm_ops\u0026#34; does not exist for access method \u0026#34;gin\u0026#34; : CREATE INDEX CONCURRENTLY \u0026#34;index_tags_on_name_trigram\u0026#34; ON \u0026#34;tags\u0026#34; USING gin (\u0026#34;name\u0026#34; gin_trgm_ops) ........................ 解决 # 需要pg 的扩展 yum install postgresql12-contrib.x86_64 psql -U root -h ip -d gitlabhq_production CREATE EXTENSION pg_trgm; 故障2 There was an error running gitlab-ctl reconfigure: service[gitlab-workhorse] (dynamically defined) had an error: Mixlib::ShellOut::ShellCommandFailed: Expected process to exit with [0], but received \u0026#39;1\u0026#39; ---- Begin output of /opt/gitlab/embedded/bin/chpst -u root:root /opt/gitlab/embedded/bin/sv restart /opt/gitlab/service/gitlab-workhorse ---- STDOUT: fail: /opt/gitlab/service/gitlab-workhorse: runsv not running STDERR: ---- End output of /opt/gitlab/embedded/bin/chpst -u root:root /opt/gitlab/embedded/bin/sv restart /opt/gitlab/service/gitlab-workhorse ---- Ran /opt/gitlab/embedded/bin/chpst -u root:root /opt/gitlab/embedded/bin/sv restart /opt/gitlab/service/gitlab-workhorse returned 1 解决 gitlab-ctl prometheus-upgrade 从12.0.12 升级到12.1.17 故障 故障1 /opt/gitlab/embedded/bin/runsvdir-start: line 24: ulimit: pending signals: cannot modify limit: Operation not permitted /opt/gitlab/embedded/bin/runsvdir-start: line 37: /proc/sys/fs/file-max: Read-only file system 解决 docker exec -it gitlab update-permissions docker restart gitlab 故障2 Error executing action `restart` on resource \u0026#39;service[gitlab-workhorse]\u0026#39; 解决 gitlab-rake gitlab:check SANITIZE=true gitlab-ctl reconfigure docker exec -it gitlab gitlab-ctl reconfigure 13.8.8.8 1. 这个版本需要升级PG 数据库到12.9 2. 迁移存储库参考官网https://docs.gitlab.com/ee/administration/raketasks/storage.html#migrate-to-hashed-storage pg 数据库迁移 # 备份数据 /usr/pgsql-12/bin/pg_dump -h 10.39.15.2 -U root -C gitlabhq_production \u0026gt; gitlabhq_production.sql # 导入数据 sudo -u postgres /usr/pgsql-12/bin/psql -U postgres \u0026lt; gitlabhq_production.sql # 创建 CREATE USER dbuser WITH PASSWORD \u0026#39;*****\u0026#39;; GRANT ALL PRIVILEGES ON DATABASE gitlabhq_production TO gitlab; ALTER USER myuser WITH SUPERUSER; sudo -u postgres /usr/pgsql-12/bin/psql -U gitlab -d gitlabhq_production -h 10.39.15.3 CREATE EXTENSION pg_trgm; 存储库迁移步骤 https://gitlab.com/gitlab-org/gitlab/-/issues/289816 # 进入 Rails 控制台 gitlab-rails console # 查询 项目 read-only 打开的 projects = Project.where(repository_read_only: true) # 关闭 项目的 read-only projects.each do |p| p.update!(repository_read_only:nil) end # 存储库迁移 gitlab-rake gitlab:storage:migrate_to_hashed #　全部迁移成功，以下命令查看所列出的项目总数与页面的理应一致 gitlab-rake gitlab:storage:hashed_projects # 查看，全部迁移成功以下两条命令应该为 0 gitlab-rake gitlab:storage:legacy_projects gitlab-rake gitlab:storage:legacy_attachments # 全部迁移成功，以下两条命令应该没有输出 gitlab-rake gitlab:storage:list_legacy_projects gitlab-rake gitlab:storage:list_legacy_attachments 13.9.2 -\u0026gt; 13.12.12 -\u0026gt; 14.0.11故障 如下报错在14.1.+ 版本解决\nActionView::Template::Error (PG::UndefinedTable: ERROR: relation \u0026#34;services\u0026#34; does not exist 直接按照官网的升级路线，13.12.15 -\u0026gt; latest 14.0.Z -\u0026gt; latest 14.1.Z -\u0026gt; latest 14.Y.Z 会遇到如下\nFinalize it manualy by running sudo gitlab-rake gitlab:background_migrations:finalize[CopyColumnUsingBackgroundMigrationJob,ci_stages,id,\u0026#39;[[\u0026#34;id\u0026#34;]\\, [\u0026#34;id_convert_to_bigint\u0026#34;]]\u0026#39;] For more information, check the documentation https://docs.gitlab.com/ee/user/admin_area/monitoring/background_migrations.html#database-migrations-failing-because-of-batched-background-migration-not-finished https://forum.gitlab.com/t/upgrade-problems-debian-14-2-4/58822 #如果有这样的报错，就返回到上一个升级的完整版本， 然后docker exec -ti gitlab bash 执行官网提示的语句 gitlab-rake gitlab:background_migrations:finalize[CopyColumnUsingBackgroundMigrationJob,events,id,\u0026#39;[[\u0026#34;id\u0026#34;]\\, [\u0026#34;id_convert_to_bigint\u0026#34;]]\u0026#39;] gitlab-rake gitlab:background_migrations:finalize[CopyColumnUsingBackgroundMigrationJob,push_event_payloads,event_id,\u0026#39;[[\u0026#34;event_id\u0026#34;]\\, [\u0026#34;event_id_convert_to_bigint\u0026#34;]]\u0026#39;] 14.1.17 升级到14.2.1 报错 #报错 Finalize it manualy by running sudo gitlab-rake gitlab:background_migrations:finalize[CopyColumnUsingBackgroundMigrationJob,ci_stages,id,\u0026#39;[[\u0026#34;id\u0026#34;]\\, [\u0026#34;id_convert_to_bigint\u0026#34;]]\u0026#39;] docker restart gitlab 然后打开一个新的终端执行 docker exec -ti gitlab gitlab-rake gitlab:background_migrations:finalize[CopyColumnUsingBackgroundMigrationJob,ci_stages,id,\u0026#39;[[\u0026#34;id\u0026#34;]\\, [\u0026#34;id_convert_to_bigint\u0026#34;]]\u0026#39;] 有done 显示标识成功或者$? 返回为0表示正常 ","permalink":"https://xingxing.io/posts/devops/gitlab-update/","tags":["devops"],"title":"Gitlab Update"},{"categories":["go练习"],"contents":"go练习 GO 练习题 99 乘法口诀 package main import \u0026#34;fmt\u0026#34; func main(){ for i :=1; i\u0026lt;=9; i++ { for j := 1; j \u0026lt;= i; j++ { fmt.Printf(\u0026#34;%d * %d = %d\\t\u0026#34;, j,i,i*j) } fmt.Println() } } 猜数字 猜数字，生成随机数0-100从控制台比较。大， 提示太大了; 小 , 提示太小了 , 等于 成功程序结束,最多猜测5次，未猜对，说太笨了，程序终结\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;math/rand\u0026#34; ) func main (){ for i:=1; i\u0026lt;=5; i++{ rand.Seed(time.Now().Unix()) // 设置随机数种子 a := rand.Intn(100) var b int fmt.Print(\u0026#34;请输入数字\u0026#34;) fmt.Scan(\u0026amp;b) if b \u0026gt; a { fmt.Println(\u0026#34;太大了\u0026#34;) }else if b == a { fmt.Println(\u0026#34;等于a\u0026#34;) break }else if b \u0026lt; a { fmt.Println(\u0026#34;太小了\u0026#34;) } } fmt.Println(\u0026#34;太笨了\u0026#34;) } ","permalink":"https://xingxing.io/posts/golang/go%E7%BB%83%E4%B9%A0/","tags":["go练习"],"title":"Go练习"},{"categories":["linux"],"contents":"进程僵死的排查","permalink":"https://xingxing.io/posts/linux/%E8%BF%9B%E7%A8%8B%E5%83%B5%E6%AD%BB%E7%9A%84%E6%8E%92%E6%9F%A5/","tags":["linux"],"title":"进程僵死的排查"},{"categories":["linux"],"contents":"haproxy 学习笔记 高性能负载均衡软件Haproxy 1. 四层和七层负载均衡的区别 所谓的四层就是ISO 参考模型中的第四层。四层负载均衡称为四层交换机，它主要是通过分析 IP层及 TCP/UDP 层的流量实现的基于IP加端口的负载均衡，常见的基于四层的负载均衡有Lvs F5. 以常见的TCP 应用为例，，负载均衡器在接收到第一个来自客户端的SYN 请求时，会通过设定的负载均衡算法选择一个最佳的后端服务器，同时将报文中目标IP 地址修改为后端服务器IP，然后直接转发给该后端服务器。这样一个负载均衡的请求就完成了。 ​ ​ 七层负载均衡器也称为七层交换机，位于OSI最高层，即应用层，此时负载均衡器支持多种应用协议，常见的有HTTP FTP SMTP等，七层负载均衡不但可以根据\u0026quot;IP+端口\u0026quot;的方式进行负载分流，还可以根据网站的URL 访问域名 浏览类别 语言等决定负载均衡的策略。 ​\n2. Haproxy 与Lvs 的异同 lvs 基于四层IP 负载均衡技术，而haproxy 是基于四层和七层技术，可提供tcp 与http 应用的负载均衡综合解决方案. lvs 工作在iso 模型的第四层，因此其状态监测功能单一，而haproxy 在状态监测方面功能强大，可支持端口、URL 脚本等多种状态监测方式。 haproxy 虽然功能强大，但是整体处理性能低于四层模式的lvs 负载均衡，而lvs 拥有接近硬件设备的网络吞吐和连接负载能力 ​\n安装haproxy 下载 http://www.haproxy.org/ tar xvf haproxy-1.8.3.tar.gz make TARGET=linux31 如果内核为2.6的 那么就是make TARGET=linux26即可 make install PREFIX=/usr/local/haproxy 安装目录为 /usr/local/haproxy mkdir /usr/local/haproxy/conf 创建配置文件 cp examples/option-http_proxy.cfg /usr/local/haproxy/conf/haproxy.cfg 拷贝配置文件 haproxy 结构 haproxy 配置文件 Haproxy 配置文件根据功能与用途，主要有5个部分组成，但有些部分并不是必须的，可以根据需要选择相应的部分进行配置。 （1）global部分 用来设置全局配置参数，属于进程级的配置，通常和操作系统配置有关 （2）default部分 默认参数的配置部分，在此部分设置的参数值，默认会自动被引用下载的frontend、backend 和listen 部分中，因此在frontend、backend、和listen 部分中也配置了defaults 部分的一样参数，那么defaults部分参数对应的值自动覆盖 (3)frontend 部分 此部分用于设置接受用户请求的前端虚拟节点。frontend 是在haproxy 1.3 版本之后才引入的组件，同时也引入了backend 组件 ，frontend 可以根据acl 规则直接指定要使用的后端backend （4）backend 部分 此部分用于设置集群后端服务集群的配置，也就是用来添加一组真实服务器，以处理前端用户的请求，添加的真实服务器类似于lvs 中的real server 节点 （5）listen 部分 此部分是frontend 部分和backend部分的结合体 haproxy配置文件详解 (1) global 部分 ​ ​ maxconn 20000 ​ ulimit-n 16384 ​ log 127.0.0.1 local0 ​ uid 200 ​ gid 200 ​ chroot /var/empty ​ nbproc 4 ​ daemon ​\nmaxconn: 设定每个haproxy 进程可接受最大并发连接数，此选项等同于linux 命令行选项\u0026quot;ulimit -n\u0026quot; uid/gid 设置运行haproxy 进程的用户和组 daemon 设置haproxy 进程进入后台运行 nbproc haproxy 启动的时候创建的进程数，一般修改小于服务器CPU 核数，创建多个进程，能够减少每个进程的任务队列，但是过多的进程会导致崩溃 （2）frontend 部分\nfrontend test-proxy bind 192.168.200.10:8080 mode http log global option httplog option dontlognull option nolinger option http_proxy maxconn 8000 timeout client 30s # layer3: Valid users acl allow_host src 192.168.200.150/32 http-request deny if !allow_host # layer7: prevent private network relaying acl forbidden_dst url_ip 192.168.0.0/24 acl forbidden_dst url_ip 172.16.0.0/12 acl forbidden_dst url_ip 10.0.0.0/8 http-request deny if forbidden_dst default_backend test-proxy-srv ​ ​ 这部分通过frontend 关键字起一个名为“test-proxy” 的前端虚拟节点， ​ bind: 此选项只能在frontend 和listen 部分进行定义，用于定义或几个监听的套接字 ​ mode 设置haproxy 实例默认的运行模式，有tcp http 模式 ​\noption httplog: 在默认情况下，haproxy 日志是不记得HTTP请求的，这样很不方便haproxy 问题的排查与监控，通过此选项可以启用日志记录HTTP 请求 option forwardfor: 如果后端服务器需要获得客户端的真实IP，就需要配置此参数，由于haproxy 工作于反向代理模式，因此发往后端真实服务器的请求中客户端IP haproxy 主机的ip，而非真实访问客户端的地址，这就导致真实服务器无法记录客户端真正请求来源的ip，而“x-forwarded-for” 则可用于解决此问题。 tcp 模式: 在此模式下，客户端和服务器端之间将建立一个全双工的连接，不会对七层报文做任何类型的检查，经常用于SSL SSH SMTP 等应用 http模式: 在此模式下，客户端请求在转发至后端服务器之前将会被深度分析，所有不与RFC 格式兼容的请求都会被拒绝。 timeout client: 设置连接客户端发送数据时最长等待时间，默认单位为毫秒 log global:表示使用全局的日志配置，这里的\u0026quot;global\u0026quot; 表示引用在haproxy 配置文件global 部分中定义的log 选项配置格式。 default_backend: 指定默认的后端服务器池，也就是指定一组后端真实服务器，而这些真实服务器组将在backend 段进行定义。这里test-proxy-srv 就是后端服务器组 （3）backend 部分 backend test-proxy-srv mode http timeout connect 5s timeout server 5s retries 2 option nolinger option http_proxy # layer7: Only GET method is valid acl valid_method method GET http-request deny if !valid_method # layer7: protect bad reply http-response deny if { res.hdr(content-type) audio/mp3 } ​ ​ option abortonclose: 如果设置了此参数，可以在服务器负载很高的情况下，自动结束掉当前队列中处理时间比较长的链接\n​ ​ balance: 此关键字用来定义负载均衡算法.目前haproxy 支持多种负载均衡算法，常用的有如下几种: ​\nrundrobin: 基于权重进行轮询调度的算法，在服务器性能分布比较均匀的时候，也是一种最公平、最合理的算法.此算法经常使用 source: 用基于请求源IP 的算法，此算法先对请求的源IP 进行hash 运算，然后将结果与后端服务器的权重总数相除后转发至某个匹配的后端服务器。这种方式可以使同一个客户端的请求始终被转发到特定的后端服务器。 leastconn: 此算法会将新的连接请求转发到具有最少连接数目的后端服务器，在回话时间较长的场景中推荐此算法，例如数据库负载均衡，不太适合基于http 的应用 uri:此算法会对部分或整个URI 进行hash 运算，再经过与服务器的总权重相除，最后转发到某台匹配的后端服务器上。 uri_param: 此算法会根据URL 路径中参数进行转发 static-dir: 也是基于权重进行轮询的调度算法，不过此算法为静态方法，在运行时调整其服务器权重不会生效 cookie:表示运行向cookie 插入SERVERID每台服务器的SERVERID ( ","permalink":"https://xingxing.io/posts/linux/haproxy-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","tags":["linux"],"title":"Haproxy 学习笔记"},{"categories":["data"],"contents":"clickhouse 基础学习 clickhouse https://clickhouse.yandex/tutorial.html 快速搭建集群参考\nhttps://clickhouse.yandex/reference_en.html 官网文档\nhttps://habrahabr.ru/company/smi2/blog/317682/ 关于集群配置参考\nClickhouse 基本介绍 Clickhouse 是属于OLAP(联机分析处理)开源的列存储数据管理系统。 特点： 可以线性扩展 简单，方便 高度可靠的 容错(支持多主机异步复制，可以跨多个数据中心部署，单个节点或整个数据中心的停机时间不会影响系统的读写可用性) 功能及应用介绍 功能 应用 深度列存储 Web app 分析 向量化查询执行(vectorized query execution) 广告网络和实时出价 数据压缩 电信 并行和分布式查询 信息安全 实时数据注入 监测和遥测 缺点： 1.\t不支持事务 2.\t没有update 和delete 3.\t支持的操作系统有限(只支持ubuntu) Clickhouse 安装部署 1.\t环境准备 三台4c8G，分别为10.39.1.36，10.39.1.37，10.39.1.40 ubuntu 14.04.x86x64 2.\t安装 sudo apt-key adv --keyserver keyserver.ubuntu.com --recv E0C56BD4 sudo mkdir -p /etc/apt/sources.list.d echo \u0026quot;deb http://repo.yandex.ru/clickhouse/trusty stable main\u0026quot; | sudo tee /etc/apt/sources.list.d/clickhouse.list sudo apt-get update sudo apt-get install clickhouse-server-common clickhouse-client sudo service clickhouse-server start 单机版本就可以直接启动使用了。 集群配置 ClickHouse deployment to cluster ClickHouse cluster is a homogenous cluster. Steps to set up 1. Install ClickHouse server on all machines of the cluster 2. Set up cluster configs in configuration file 3. Create local tables on each instance 4. Create a Distributed table clickhouse-client 客户端软件，集群的安装的可以选择， clickhouse-server-common 服务端软件，必须安装 clickhouse-compressor 数据压缩软件，集群的话必须安装 配置文件 /etc/clickhouse-server/ config.xml 配置文件 users.xml 用户权限配置文件 分布式查询集群或者复制需要单独创建一个新的文件 touch /etc/metrika.xml (可以参考) https://clickhouse.yandex/reference_en.html#External dictionaries https://clickhouse.yandex/reference_en.html#Data%20replication 分布式集群配置 vim /etc/metrika.xml \u0026lt;yandex\u0026gt; \u0026lt;clickhouse_remote_servers\u0026gt; \u0026lt;enncloud\u0026gt; \u0026lt;shard\u0026gt; \u0026lt;weight\u0026gt;1\u0026lt;/weight\u0026gt; \u0026lt;internal_replication\u0026gt;false\u0026lt;/internal_replication\u0026gt; \u0026lt;replica\u0026gt; \u0026lt;host\u0026gt;clickhouse01\u0026lt;/host\u0026gt; \u0026lt;port\u0026gt;9000\u0026lt;/port\u0026gt; \u0026lt;/replica\u0026gt; \u0026lt;replica\u0026gt; \u0026lt;host\u0026gt;clickhouse02\u0026lt;/host\u0026gt; \u0026lt;port\u0026gt;9000\u0026lt;/port\u0026gt; \u0026lt;/replica\u0026gt; \u0026lt;replica\u0026gt; \u0026lt;host\u0026gt;clickhouse03\u0026lt;/host\u0026gt; \u0026lt;port\u0026gt;9000\u0026lt;/port\u0026gt; \u0026lt;/replica\u0026gt; \u0026lt;/shard\u0026gt; \u0026lt;shard\u0026gt; \u0026lt;weight\u0026gt;3\u0026lt;/weight\u0026gt; \u0026lt;internal_replication\u0026gt;false\u0026lt;/internal_replication\u0026gt; \u0026lt;replica\u0026gt; \u0026lt;host\u0026gt;clickhouse01\u0026lt;/host\u0026gt; \u0026lt;port\u0026gt;9000\u0026lt;/port\u0026gt; \u0026lt;/replica\u0026gt; \u0026lt;replica\u0026gt; \u0026lt;host\u0026gt;clickhouse02\u0026lt;/host\u0026gt; \u0026lt;port\u0026gt;9000\u0026lt;/port\u0026gt; \u0026lt;/replica\u0026gt; \u0026lt;replica\u0026gt; \u0026lt;host\u0026gt;clickhouse03\u0026lt;/host\u0026gt; \u0026lt;port\u0026gt;9000\u0026lt;/port\u0026gt; \u0026lt;/replica\u0026gt; \u0026lt;/shard\u0026gt; \u0026lt;/enncloud\u0026gt; \u0026lt;/clickhouse_remote_servers\u0026gt; \u0026lt;networks\u0026gt; \u0026lt;ip\u0026gt;::/0\u0026lt;/ip\u0026gt; \u0026lt;/networks\u0026gt; \u0026lt;clickhouse_compression\u0026gt; \u0026lt;min_part_size\u0026gt;10000000000\u0026lt;/min_part_size\u0026gt; \u0026lt;min_part_size_ratio\u0026gt;0.01\u0026lt;/min_part_size_ratio\u0026gt; \u0026lt;method\u0026gt;lz4\u0026lt;/method\u0026gt; \u0026lt;/clickhouse_compression\u0026gt; \u0026lt;/yandex\u0026gt; 关于\u0026lt;internal_replication\u0026gt;false\u0026lt;/internal_replication\u0026gt; false 和true 的设置的区别 如果设置为true，则写操作将选择第一个健康副本并将数据写入它。如果分布式表”查看”复制表，则使用此选项，换句话说，如果将写入数据的将复制它们本身。 如果设置为false(默认值) 则将数据写入所有副本。这意味着分布式表复制数据本身，这比使用复制更槽糕，因为没有检查副本的一致性，并且随着时间的推移它们会包含稍微不同的数据。 关闭一台机器，其他机器还是分布式查询。 测试： 在所有的节点上创建test 表 CREATE TABLE test (FlightDate Date,Year UInt16) ENGINE = MergeTree(FlightDate, (Year, FlightDate), 8192); 创建分布式表并且关联本地表 CREATE TABLE test_all AS test ENGINE = Distributed(enncloud, default, test, rand()) 往分布式表插入数据，然后在其他节点查询 insert into test_all (FlightDate,Year)values('2013-10-12',2013); 可以抓包查询是否是分布式查询 tcpdump -i any -s 0 -l -w - dst port 9000 4.\t复制的配置 \u0026lt;yandex\u0026gt; \u0026lt;clickhouse_remote_servers\u0026gt; \u0026lt;perftest_1shards_3replicas\u0026gt; \u0026lt;shard\u0026gt; \u0026lt;replica\u0026gt; \u0026lt;host\u0026gt;clickhouse01\u0026lt;/host\u0026gt; \u0026lt;port\u0026gt;9000\u0026lt;/port\u0026gt; \u0026lt;/replica\u0026gt; \u0026lt;replica\u0026gt; \u0026lt;host\u0026gt;clickhouse02\u0026lt;/host\u0026gt; \u0026lt;port\u0026gt;9000\u0026lt;/port\u0026gt; \u0026lt;/replica\u0026gt; \u0026lt;replica\u0026gt; \u0026lt;host\u0026gt;clickhouse03\u0026lt;/host\u0026gt; \u0026lt;port\u0026gt;9000\u0026lt;/port\u0026gt; \u0026lt;/replica\u0026gt; \u0026lt;/shard\u0026gt; \u0026lt;/perftest_1shards_3replicas\u0026gt; \u0026lt;/clickhouse_remote_servers\u0026gt; \u0026lt;macros\u0026gt; \u0026lt;shard\u0026gt;02\u0026lt;/shard\u0026gt; \u0026lt;replica\u0026gt;10.39.1.36\u0026lt;/replica\u0026gt; \u0026lt;/macros\u0026gt; \u0026lt;networks\u0026gt; \u0026lt;ip\u0026gt;::/0\u0026lt;/ip\u0026gt; \u0026lt;/networks\u0026gt; \u0026lt;clickhouse_compression\u0026gt; \u0026lt;min_part_size\u0026gt;10000000000\u0026lt;/min_part_size\u0026gt; \u0026lt;min_part_size_ratio\u0026gt;0.01\u0026lt;/min_part_size_ratio\u0026gt; \u0026lt;method\u0026gt;lz4\u0026lt;/method\u0026gt; \u0026lt;/clickhouse_compression\u0026gt; \u0026lt;zookeeper-servers\u0026gt; \u0026lt;node index=\u0026quot;1\u0026quot;\u0026gt; \u0026lt;host\u0026gt;10.39.1.36\u0026lt;/host\u0026gt; \u0026lt;port\u0026gt;2181\u0026lt;/port\u0026gt; \u0026lt;/node\u0026gt; \u0026lt;/zookeeper-servers\u0026gt; \u0026lt;/yandex\u0026gt; 配置详解 shard 碎片标识符， \u0026lt;replica\u0026gt;10.39.1.36\u0026lt;/replica\u0026gt; 10.39.1.36 是标识符，可以看做是副本标识符，是独一无二的，每个节点的标识符为本机的ip，这样的话，三个节点的话，每个配置的副本标识符将为它自身的ip地址。 创建复制表 CREATE TABLE ontime_replica (FlightDate Date,Year UInt16) ENGINE = ReplicatedMergeTree('/clickhouse_perftest/tables/ontime_replica','{replica}',FlightDate,(Year, FlightDate),8192); 插入数据，在其他节点解析查询 接口 启用远程接口访问编辑 config.xml \u0026lt;listen_host\u0026gt;::\u0026lt;/listen_host\u0026gt; 1.\tHTTP interface 默认情况下，clcikhouse 服务器在端口8123 监听HTTP。 URL 长度限制为16KB 如果成功，则返回响应码200 如果失败，则返回响应码500 使用http interface 接口创建表插入数据并删除表 root@clickhouse01:~# echo 'CREATE TABLE abc (a UInt8) ENGINE = Memory' | POST 'http://10.39.1.36:8123/' root@clickhouse01:~# echo 'INSERT INTO abc VALUES (1),(2),(3)' | POST 'http://10.39.1.36:8123/' root@clickhouse01:~# echo '(4),(5),(6)' | POST 'http://10.39.1.36:8123/?query=INSERT INTO abc VALUES' root@clickhouse01:~# echo '(7),(8),(9)' | POST 'http://10.39.1.36:8123/?query=INSERT INTO abc FORMAT Values' 格式转换TabSeparated root@clickhouse01:~# echo -ne '10\\n11\\n12\\n' | POST 'http://10.39.1.36:8123/?query=INSERT INTO abc FORMAT TabSeparated' 查询的结果输出如下 root@clickhouse01:~# GET 'http://10.39.1.36:8123/?query=SELECT a FROM abc' 1 2 3 10 11 12 7 8 9 4 5 6 删除表 echo 'DROP TABLE abc' | POST 'http://10.39.1.36:8123/' 2.\tJDBC driver https://github.com/yandex/clickhouse-jdbc 3.\tThird-party client libraries 4.\tCommand-line client 语法 系统中有两种类型的解析器：一个完整的SQL 解析器(一个递归下降解析器)和一个数据格式分析器（一个快速流分析器）。在所有情况下，除了插入查询，只有完整的SQL解析器使用。 创建数据库 CREATE DATABASE [IF NOT EXISTS] db_name 创建表 CREATE [TEMPORARY] TABLE [IF NOT EXISTS] [db.]name ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE = engine 创建视图 CREATE [MATERIALIZED] VIEW [IF NOT EXISTS] [db.]name [ENGINE = engine] [POPULATE] AS SELECT ... 表引擎 建议使用来自MergeTree家族的存储引擎(LSM树) 表的存储引擎如下： 1.\tTinyLog 最简单的表引擎，它将数据存储在磁盘上，每列存储在单独的压缩文件中。写入时，数据会追加到文件的末尾。 并发数据访问不受任何限制： 如果您正在从表中读取并以相同的查询写入，读操作将会出现错误。 如果您同时在多个查询中写入表，数据将被破坏。 使用该表的典型方式是一次写入：首先只是写入一次数据，然后根据需要读取数据，查询在单个流中执行，该引擎是用于相对较小的表，（最多可推荐1,000,000行） 适合需要小表，不支持索引。 2.\tLog Log 与Tinylog不同之处在于，一个小文件的“标记”驻留在列文件中。这些标记写在每个数据块上，并包含偏移量，在哪里开始读取文件以便跳过指定的行数。这使得可以在多个线程中读取表数据。对于并发数据访问，读取操作可以同时执行。 Log 引擎不支持索引。类似的，如果写入表失败，则表被损坏，从其读取返回错误，log引擎适用于临时数据，一次写入表以及测试或者演示目的。 3.\tMemory 内存引擎将数据以未压缩格式存储在RAM 中。数据以读取时接收的格式完全相同的形式存储。并发数据访问同步，锁定很短：读写操作不会阻塞。 不支持索引，阅读并行化。 4.\tMerge 合并引擎不会自己存储数据，而是允许同时从任意数量的其他表读取数据。 读数据自动并行化。不支持写入表。读取时，使用实例读取的表的索引(如果存在)。合并引擎接受参数：数据库名称和表的正则表达式。例： Merge(hits, '^WatchLog') 数据库将从“hits”数据库中的表读取，名称与正则表示‘^watchLlog’ 匹配。 而不是数据库名称，您可以使用返回字符串的常量表达式。例如，currentDatabaase(). 正则表达式是re2(类似于PCRE)，区分大小写。 选择要读取的表时，即使与正则表达式匹配，也不会选择合并表本身。为了避免循环。 虚拟列 虚拟列是由表引擎提供的列，而不考虑定义。（create table 中未指定，但他们对于select 是可以访问的） 虚拟列与普通列的不同之处在于： 它们没有在表定义中指定， 无法使用insert 将数据添加到它们。 当使用insert 而不指定列表时，虚拟列将被忽略。 合并表包含String类型的虚拟列_table。 （如果表已经有一个'_table'列，虚拟列名为'_table1'，如果它已经有'_table1'，它被命名为'_table2'，依此类推）。它包含表的名称从中读取数据。 如果WHERE或PREWHERE子句包含不依赖于其他表列（作为连接元素之一或整个表达式）的“_table”列的条件，则将这些条件用作索引。条件是对表名的数据集进行读取数据，读操作将仅从触发条件的那些表执行。 5.\t分布式引擎 分布式引擎不存储数据本身，但允许在多个服务器上进行分布式查询处理。读数自动并行化。在读取期间，使用远程服务器的表索引。 分布式引擎接受参数：服务器器配置文件中的集群名称，远程数据库的名称，远程表的名称 例子： Distributed(logs, default, hits[, sharding_key]) 数据将从“日志”集群中的所有服务器从集群中每个服务器上的”default。hits” 表中读取。数据不仅被读取，而且在远程服务器上进行部分处理(在可能的范围内) 例如，对于使用group by 的查询，数据将在远程服务器聚合，聚合函数的中间状态将发送到请求者服务器，那么数据将进一步汇总 集群将数据写入的两种方法： 1.\t首先，你可以定义哪些服务器写入哪些数据，并直接在每个分片上执行写入。（在分片表”查看“的表中执行insert）这是最优的解决方案。因为数据可以完全独立写入不同的分片 2.\t其次，你可以在分布式表中执行insert，在这种情况下，表将通过服务器本身分发插入的数据。 6.\tMergeTree Mergetree 是clickhouse 最近的表引擎，它支持主键索引和日期，并提供了可能性，实时更新数据。 Example without sampling support: MergeTree(EventDate, (CounterID, EventDate), 8192) Example with sampling support: MergeTree(EventDate, intHash32(UserID), (CounterID, EventDate, intHash32(UserID)), 8192) 一个mergetree 型表必须有一个单独的列包含日期，在这个例子中，它是eventdate 的日期列的类型必须是日期。 7.\tCollapsingMergeTree 该引擎与MergeTree 不同之处在于它允许自动删除，或者合并时“折叠”某些行对 例子： CollapsingMergeTree(EventDate, (CounterID, EventDate, intHash32(UniqID), VisitID), 8192, Sign) 在这里sign 是一个列，old 值为-1，new 值为1 合并时，每组连续相同的主键值(用于排序数据的列)减少为不超过一行，列值为(sign_column=-1), 不超过一行，列值“sign_column=1”.(更改日志中的条目自己折叠) 8.\tSummingMergeTree 该存储引擎与MergeTree 的不同之处在于它在合并时总计数据 SummingMergeTree(EventDate, (OrderID, EventDate, BannerID, ...), 8192) 9.\tAggregatingMergeTree 它与MergreTree 不同之处在于，合并将存储在表中的聚合函数的状态组合为具有相同主键值的行。 例子： CREATE MATERIALIZED VIEW test.basic ENGINE = AggregatingMergeTree(StartDate, (CounterID, StartDate), 8192) AS SELECT CounterID, StartDate, sumState(Sign) AS Visits, uniqState(UserID) AS Users FROM test.visits GROUP BY CounterID, StartDate; INSERT INTO test.visits ... SELECT StartDate, sumMerge(Visits) AS Visits, uniqMerge(Users) AS Users FROM test.basic GROUP BY StartDate ORDER BY StartDate; 10.\tReplacingMergeTree 它与MergeTree 不同之处在于，它可以在合并时通过主键重复数据删除数据 例子： ReplacingMergeTree(EventDate, (OrderID, EventDate, BannerID, ...), 8192, ver) 11.\tNULL 当写入null 表时，数据将被忽略，从null 表中读取，响应为空 但是，你可以在null表上创建实例视图，因为写入表的数据将在视图中显示 12.\tView 用于实现视图，它不存储数据，而只存储指定的select 查询。从表读取时，它会运行此查询(并从表中删除所有不必要的列) a)\tMaterializedView 用于实现物化视图。对于存储数据，它使用在创建视图时指定的不同引擎。它只是使用这个引擎。 13.\tSet 始终在RAM的数据集。你可以使用insert 在表中插入数据。新元素将被添加到数据集中，而重复的元素将被忽略。 数据总是位于RAM中，对于insert，插入的数据块也会写入到磁盘上的表目录。当启动服务器时，这些数据被加载到RAM中。重启后，数据保持原样。 对于服务器重新启动，磁盘上的数据块可能会丢失或损坏。在后一种情况，你可能需要手动删除带有损坏数据的文件。 14.\tJoin 一个总是位于RAM 中的JOIN 数据结构 Join(ANY|ALL, LEFT|INNER, k1[, k2, ...]) 15.\tBuffer 缓存数据写入RAM，定期将其刷新到另一个表。在读操作期间，数据从缓冲器和另一个表同时读取。 例子： CREATE TABLE merge.hits_buffer AS merge.hits ENGINE = Buffer(merge, hits, 16, 10, 100, 10000, 1000000, 10000000, 100000000) 最大1.6GB 的内存消耗 16.\tData replication ReplicatedMergeTree ReplicatedCollapsingMergeTree ReplicatedAggregatingMergeTree ReplicatedSummingMergeTree MergeTree 系列中的表仅支持复制。复制在单个表的级别上工作，而不是整个服务器。服务器可以同时存储复制表和非复制表。 系统表 system.databases system.tables system.processes system.clusters cluster String - Cluster name. shard_num UInt32 - Number of a shard in the cluster, starting from 1. shard_weight UInt32 - Relative weight of a shard when writing data. replica_num UInt32 - Number of a replica in the shard, starting from 1. host_name String - Host name as specified in the config. host_address String - Host's IP address obtained from DNS. port UInt16 - The port used to access the server. user String - The username to use for connecting to the server. system.merges system.settings system.zookeeper 表函数 1.\tmerge 2.\tremote 数据格式 TabSeparated TabSeparatedWithNames TabSeparatedWithNamesAndTypes TabSeparatedRaw BlockTabSeparated CSV CSVWithNames RowBinary Pretty PrettyCompact PrettyCompactMonoBlock PrettySpace PrettyNoEscapes PrettyCompactNoEscapes PrettySpaceNoEscapes Vertical Values JSON JSONCompact JSONEachRow TSKV XML Null ​\n数据类型 1.\t整数 UInt8, UInt16, UInt32, UInt64, Int8, Int16, Int32, Int64 2.\t浮点数 Float32, Float64 3.\t字符串 任意长度的字符串，长度不限于此，该值可以包含任意字节集，包括空字节。字符串类型替换其他DBMS中类型varchar，blob，clob 和其他类型。 Cliclhouse 没有编码的概念，建议使用UTF-8编码， 4.\tFixedString(N) N字节固定长度字符串，N必须是严格正数的自然数。 5.\tDate 日期存储没有时区，最小值输出为0000-00-00 6.\tDateTime 允许将值与日期类型保存在相同的范围，最小输出为 0000-00-00 00:00:00 7.\t枚举 Enum8 or Enum16 8.\tArray(T) 不建议使用多维数组，不被很好的支持 9.\t元组 Tuple(T1, T2, ...) 10.\t嵌套的数据结构 Nested(Name1 Type1, Name2 Type2, ...) 11.\t聚合函数 AggregateFunction(name, types_of_arguments...) https://clickhouse.yandex/reference_en.html#AggregatingMergeTree 12.\t特殊的数据类型 特殊的数据类型值无法保存到结果中的表输出，而是用作运行查询的中间结果。 13.\tset 14.\tBoolean values 使用uint8 类型限制的值0 和值 1 分布式数据查询 10.1 分布式集群测试 查询集群状态 select * from system.clusters 使用官网的测试数据 https://raw.githubusercontent.com/yandex/ClickHouse/master/doc/example_datasets/1_ontime.txt CREATE TABLE a_test ( Year UInt16, Quarter UInt8, Month UInt8, DayofMonth UInt8, DayOfWeek UInt8, FlightDate Date, UniqueCarrier FixedString(7), AirlineID Int32, Carrier FixedString(2), TailNum String, FlightNum String, OriginAirportID Int32, OriginAirportSeqID Int32, OriginCityMarketID Int32, Origin FixedString(5), OriginCityName String, OriginState FixedString(2), OriginStateFips String, OriginStateName String, OriginWac Int32, DestAirportID Int32, DestAirportSeqID Int32, DestCityMarketID Int32, Dest FixedString(5), DestCityName String, DestState FixedString(2), DestStateFips String, DestStateName String, DestWac Int32, CRSDepTime Int32, DepTime Int32, DepDelay Int32, DepDelayMinutes Int32, DepDel15 Int32, DepartureDelayGroups String, DepTimeBlk String, TaxiOut Int32, WheelsOff Int32, WheelsOn Int32, TaxiIn Int32, CRSArrTime Int32, ArrTime Int32, ArrDelay Int32, ArrDelayMinutes Int32, ArrDel15 Int32, ArrivalDelayGroups Int32, ArrTimeBlk String, Cancelled UInt8, CancellationCode FixedString(1), Diverted UInt8, CRSElapsedTime Int32, ActualElapsedTime Int32, AirTime Int32, Flights Int32, Distance Int32, DistanceGroup UInt8, CarrierDelay Int32, WeatherDelay Int32, NASDelay Int32, SecurityDelay Int32, LateAircraftDelay Int32, FirstDepTime String, TotalAddGTime String, LongestAddGTime String, DivAirportLandings String, DivReachedDest String, DivActualElapsedTime String, DivArrDelay String, DivDistance String, Div1Airport String, Div1AirportID Int32, Div1AirportSeqID Int32, Div1WheelsOn String, Div1TotalGTime String, Div1LongestGTime String, Div1WheelsOff String, Div1TailNum String, Div2Airport String, Div2AirportID Int32, Div2AirportSeqID Int32, Div2WheelsOn String, Div2TotalGTime String, Div2LongestGTime String, Div2WheelsOff String, Div2TailNum String, Div3Airport String, Div3AirportID Int32, Div3AirportSeqID Int32, Div3WheelsOn String, Div3TotalGTime String, Div3LongestGTime String, Div3WheelsOff String, Div3TailNum String, Div4Airport String, Div4AirportID Int32, Div4AirportSeqID Int32, Div4WheelsOn String, Div4TotalGTime String, Div4LongestGTime String, Div4WheelsOff String, Div4TailNum String, Div5Airport String, Div5AirportID Int32, Div5AirportSeqID Int32, Div5WheelsOn String, Div5TotalGTime String, Div5LongestGTime String, Div5WheelsOff String, Div5TailNum String ) ENGINE = MergeTree(FlightDate, (Year, FlightDate), 8192) 2. 创建分布式关联表 CREATE TABLE a_all AS a_test ENGINE = Distributed(perftest_1shards_3replicas, default, a_test, rand()); 3.创建表和分布式关联表都需要在每个节点执行。 4. 插入数据到分布式表，在其他节点查询 下载的数据导入到a_all 表中 for i in *.zip; do echo $i; unzip -cq $i '*.csv' | sed 's/\\.00//g' | clickhouse-client --host=clickhouse01 --query=\u0026quot;INSERT INTO a_all FORMAT CSVWithNames\u0026quot;; done 在其他节点上查询并且监测是不是分布式 抓包 tcpdump -i any -s 0 -l -w - dst port 9000 1. 查询总的数据 select count(*) from a_all; 2. 查询1987-1991年每天的航班统计 SELECT DayOfWeek, count(*) AS c FROM a_all WHERE Year \u0026gt;= 1987 AND Year \u0026lt;= 1991 GROUP BY DayOfWeek ORDER BY c DESC; 3. 查询延误在一个小时以上出发的城市 SELECT OriginCityName, count() AS c, avg(DepDelay \u0026gt; 60) AS delays FROM a_all GROUP BY OriginCityName HAVING c \u0026gt; 100000 ORDER BY delays DESC LIMIT 20 4. 查询1991 年最后欢迎的目的地 SELECT OriginCityName, DestCityName, count(*) AS flights, bar(flights, 0, 20000, 40) FROM ontime_all WHERE Year = 1991 GROUP BY OriginCityName, DestCityName ORDER BY flights DESC LIMIT 20 SELECT OriginCityName \u0026lt; DestCityName ? OriginCityName : DestCityName AS a, OriginCityName \u0026lt; DestCityName ? DestCityName : OriginCityName AS b, count(*) AS flights, bar(flights, 0, 40000, 40) FROM a_all WHERE Year = 1991 GROUP BY a, b ORDER BY flights DESC LIMIT 20 5. 最受欢迎的出发城市 SELECT OriginCityName, count(*) AS flights FROM a_all GROUP BY OriginCityName ORDER BY flights DESC LIMIT 20 6. 创建array join 允许使用数组或嵌套数据结构执行join。 CREATE TABLE arrays_test (s String, arr Array(UInt8)) ENGINE = Memory INSERT INTO arrays_test VALUES ('Hello', [1,2]), ('World', [3,4,5]), ('Goodbye', []) SELECT * FROM arrays_test SELECT s, arr FROM arrays_test ARRAY JOIN arr SELECT s, arr, a FROM arrays_test ARRAY JOIN arr AS a SELECT s, arr, a, num, mapped FROM arrays_test ARRAY JOIN arr AS a, arrayEnumerate(arr) AS num, arrayMap(x -\u0026gt; x + 1, arr) AS mapped 数组联接也与嵌套的数据结构一起工作，例子 CREATE TABLE nested_test (s String, nest Nested(x UInt8, y UInt32)) ENGINE = Memory INSERT INTO nested_test VALUES ('Hello', [1,2], [10,20]), ('World', [3,4,5], [30,40,50]), ('Goodbye', [], []) SELECT * FROM nested_test SELECT s, nest.x, nest.y FROM nested_test ARRAY JOIN nest SELECT s, nest.x, nest.y FROM nested_test ARRAY JOIN nest.x, nest.y SELECT s, nest.x, nest.y FROM nested_test ARRAY JOIN nest.x 使用arrayEnumerate 函数 SELECT s, n.x, n.y, nest.x, nest.y, num FROM nested_test ARRAY JOIN nest AS n, arrayEnumerate(nest.x) AS num 集群复制 创建复制表： CREATE TABLE ontime_replica (FlightDate Date,Year UInt16) ENGINE = ReplicatedMergeTree('/clickhouse_perftest/tables/ontime_replica','{replica}',FlightDate,(Year, FlightDate),8192); insert into ontime_replica (FlightDate,Year)values('2017-04-18',2017); 在其他节点查询 select * from ontime_replica 十一、 创建用户 直接编辑user.xml 文件 在\u0026lt;user\u0026gt; 这个范围中添加 \u0026lt;/user\u0026gt; 如果使用明文密码，直接写入到下面的明文密码处 \u0026lt;password\u0026gt; 明文密码 \u0026lt;/password\u0026gt;. 如果使用SHA256加密的密码，请使用如下配置 Example: \u0026lt;password_sha256_hex\u0026gt;65e84be33532fb784c48129675f9eff3a682b27168c0ea744b2cf58ee02337c5\u0026lt;/password_sha256_hex\u0026gt; 使用下面的命令生成密码： PASSWORD=$(base64 \u0026lt; /dev/urandom | head -c8); echo \u0026quot;$PASSWORD\u0026quot;; echo -n \u0026quot;$PASSWORD\u0026quot; | sha256sum | tr -d '-' 第一行地密码是，第二行是加密过的密码 SHA256. 例如如下配置 \u0026lt;dba\u0026gt; \u0026lt;password_sha256_hex\u0026gt;ef375dec86573b84efd45187966705c4a7b00d74c6eb8e4b8372e4ea9ebbe796\u0026lt;/password_sha256_hex\u0026gt; \u0026lt;networks incl=\u0026quot;networks\u0026quot; /\u0026gt; \u0026lt;profile\u0026gt;default\u0026lt;/profile\u0026gt; \u0026lt;quota\u0026gt;default\u0026lt;/quota\u0026gt; \u0026lt;allow_databases\u0026gt; \u0026lt;database\u0026gt;default\u0026lt;/database\u0026gt; \u0026lt;/allow_databases\u0026gt; \u0026lt;/dba\u0026gt; 权限配置: readonly 只读权限 default 有些权限 github [https://github.com/xingxing9688/blong/blob/master/clickhouse%20.md](\n","permalink":"https://xingxing.io/posts/data/clickhouse-%E5%9F%BA%E7%A1%80/","tags":["data"],"title":"Clickhouse 基础学习"},{"categories":["data"],"contents":"hbase 集群部署 Hhase 集群部署 使用的软件 hadoop-2.7.4 hbase-1.2.6 jdk-8u144 zookeeper-3.4.10 Hbase 自带的有zookeeper，在这里使用自己部署的zookeeper zookeeper 集群部署 安装jdk 下载zookeeper 程序 修改zoo.cfg tickTime=2000 initLimit=10 syncLimit=5 dataLogDir=/zookeeper/logs dataDir=/zookeeper/data clientPort=2181 server.1= 10.39.6.178:2888:3888 server.2= 10.39.6.179:2888:3888 server.3= 10.39.6.180:2888:3888 添加myid，这里的myid 对应的server.n 一一对应。 这里的server.1 所以node 1节点myid=1 echo \u0026quot;1\u0026quot; /zookeeper/data/myid 创建所需要的目录 添加环境变量 vi /etc/profile export ZOOKEEPER_HOME=/application/zookeeper-3.4.10 export PATH=$PATH:$ZOOKEEPER_HOME/bin 启动\n将node 1 的配置全部打包拷贝到其他节点上，启动zookeeper 就行了 启动有错误可以使用zkServer.sh start-foreground 来追踪错误 角色\nzkServer.sh status 会显示zookeeper 状态 Mode: leader 这里的Mode: leader 和follower 一个集群中只有leader leader 领导者，用于负责进行投票的发起决议，更新系统状态 follower 跟随者 用于接受客户端请求并想客户端返回结果，在选主过程中参与投票 配置参数详解\ntickTime 这个时间是作为zookeeper服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是说每个tickTime 时间就会发送一个心跳。 initLimit 这个配置项是用来配置zookeeper接受客户端初始化连接时最长能忍受多少个心跳时间间隔数。 当已经超过10个心跳的时间(tickTime) 长度后zookeeper 服务器还没有收到客户端的返回信息，那么表明这个客户端连接失败，总的时间长度就是10*2000=20秒 syncLimit 这个配置项标识leader 与follower 之间发送消息，请求和应答时间长度，最长不能超过多少个tickTime 的长度，总的时间长度是5*2000=10秒 dataDir 保存数据目录 clientPort 端口，这个端口是客户端连接zookeeper服务器端口，zookeeper 会监听这个端口接受客户端访问请求 server.n=B:C:D 的n是一个数字，表示这个是第几号服务器，B是这个服务器的IP地址，C第一个端口用来集群成员的信息交换，表示这个服务器与集群中的leader 服务器交换信息的端口，D是leader 挂掉时专门用来进行选举leader 所用的端口 连接zookeeper集群 zkCli.sh -server 10.39.6.178:2181\nHadoop 安装 下载地址 http://apache.fayea.com/hadoop/common/stable/hadoop-2.7.4.tar.gz hbase01 到hbase02 hbase03 需要使用ssh无密钥登录。 hadoop 配置文件 配置文件 配置对象 主要内容 core-site.xml 集群全局参数 用户定义系统级别的参数,如HDFS URL Hadoop临时目录 hdfs-site.xml HDFS 参数 如名称节点和数据节点存放位置，文件副本的个数，文件读取权限 mapred-site.xml Mapreduce参数 包括JobHistry Server 和应用程序参数两部分，如reduce 任务的默认个数，任务所能够使用内存的默认上下限 yarn-site.xml 集群资源管理系统参数 包括ResourceManager，NodeManager 的通信端口，web 监控端口等 集群配置 vi /application/hadoop-2.7.4/etc/hadoop/hadoop-env.sh export JAVA_HOME=\u0026quot;/usr/java/jdk1.8.0_144\u0026quot; （rpm 安装的jdk 存储位置） vi /application/hadoop-2.7.4/etc/hadoop/core-site.xml \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.defaultFS\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://hbase01:9000\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;The name of the default file system\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.tmp.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/zookeeper/hadoopdata/tmp\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;A base for other temporary directories\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.native.lib\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;Should native hadoop libraries, if present, be used.\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; vi /application/hadoop-2.7.4/etc/hadoop/hdfs-site.xml \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.replication\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;3\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.name.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/zookeeper/hadoopdata/dfs/name\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.datanode.data.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/zookeeper/hadoopdata/dfs/data\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; vi /application/hadoop-2.7.4/etc/hadoop/mapred-site.xml \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.framework.name\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;yarn\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; vi /application/hadoop-2.7.4/etc/hadoop/yarn-site.xml \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.hostname\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hbase01\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.aux-services\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;mapreduce_shuffle\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; vi /application/hadoop-2.7.4/etc/hadoop/slaves hbase02 hbase03 将所有的配置COPY 到hbase02 hbase03 格式化HDFS存储 1. 在namenode 上执行 进入到hadoop 目录 ./bin/hadoop namenode -format 2. 在datanode ./bin/hadoop datanode -format 启动Hadoop 1. 启动HDFS ./sbin/start-dfs.sh ./sbin/stop-dfs.sh 2. 启动Yarn ./sbin/start-yarn.sh ./sbin/stop-yarn.sh 3.启动MapReduce JobHistory Server ./sbin/mr-jobhistory-daemon.sh start historyserver jps 查看进程 jps 12016 ResourceManager 11616 NameNode 11828 SecondaryNameNode 12317 JobHistoryServer 31453 Jps web 访问端口 NameNode 50070 ResourceManager 8088 MapReduce JobHistory Server 19888 Hbase 安装 hbase 配置文件修改 vi conf/hbase-env.sh export JAVA_HOME=/usr/java/jdk1.8.0_144 export HBASE_MANAGES_ZK=false vi conf/hbase-site.xml \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.cluster.distributed\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.rootdir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://hbase01:9000/hbase\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.zookeeper.quorum\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hbase01,hbase02,hbase03\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.zookeeper.property.dataDir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/zookeeper/data\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; vi conf/regionservers hbase02 hbase03 将上述配置同步到其他节点 hbase 启动 ./bin/start-hbase.sh 查看Hbase 的状态 jps 12016 ResourceManager 11616 NameNode 12546 HMaster 10403 QuorumPeerMain 11828 SecondaryNameNode 21225 Jps 12317 JobHistoryServer 进入hbase shell，使用命令查看hbase 状态 ./bin/hbase shell SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/application/hbase-1.2.6/lib/slf4j-l HBase Shell; enter 'help\u0026lt;RETURN\u0026gt;' for list of supported commands. Type \u0026quot;exit\u0026lt;RETURN\u0026gt;\u0026quot; to leave the HBase Shell Version 1.2.6, rUnknown, Mon May 29 02:25:32 CDT 2017 hbase(main):001:0\u0026gt; status 1 active master, 0 backup masters, 2 servers, 0 dead, 1.0000 average load hbase(main):002:0\u0026gt; Hbase web ui 端口为16010 ​\n​\n","permalink":"https://xingxing.io/posts/data/hbase-%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/","tags":["data"],"title":"Hbase 集群部署"},{"categories":["harbor ha"],"contents":"harbor 高可用部署 数据库使用外接的mysql redis 解决session 问题\n1.cephfs 部署 1.1 机器列表 10.0.0.1 harbor-ceph-node1 10.0.0.2 harbor-ceph-node2 10.0.0.3 harbor-ceph-node3 10.0.0.4 harbor-ceph-node4 10.0.0.5 harbor-ceph-node5 1.2 安装ceph-deploy 1.2.1 添加ceph 源 我在跳板机上(10.0.0.10)安装ceph-deploy http://docs.ceph.org.cn/install/get-packages/ 参考资料\nvim /etc/yum.repos.d/ceph.repo\n[ceph] name=Ceph packages for $basearch baseurl=http://download.ceph.com/rpm-luminous/el7/$basearch enabled=1 priority=2 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc [ceph-noarch] name=Ceph noarch packages baseurl=http://download.ceph.com/rpm-luminous/el7/noarch enabled=1 priority=2 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc [ceph-source] name=Ceph source packages baseurl=http://download.ceph.com/rpm-luminous/el7/SRPMS enabled=0 priority=2 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc 使用的版本是rpm-luminous,系统为el7\n1.2.2 安装release.asc 密钥 rpm --import 'https://download.ceph.com/keys/release.asc' 1.2.3 安装ceph-deploy(10.0.0.10) yum install ceph-deploy -y 1.3 安装NTP SSH 在所有的ceph 节点安装NTP 和SSH yum install ntp ntpdate ntp-doc -y yum install openssh-server -y 1.4 创建ceph 用户 1.4.1 在各ceph 节点创建新用户 useradd -d /home/ceph -m ceph passwd ceph 1.4.2 确保各ceph节点新创建的用户都有sudo 权限 echo \u0026quot;ceph ALL = (root) NOPASSWD:ALL\u0026quot; | sudo tee /etc/sudoers.d/ceph chmod 0440 /etc/sudoers.d/ceph 1.4.3 生成ssh密钥对 su - ceph ssh-keygen 直接回车就行 1.4.4 把公钥拷贝到ceph 节点 ssh-copy-id -i .ssh/id_rsa.pub ceph@10.0.0.1 ...... 1.4.5 使用ssh 配置文件 su - ceph vi .ssh/config Host node1 Hostname harbor-ceph-node1 User ceph Host node2 Hostname harbor-ceph-node2 User ceph Host node3 Hostname harbor-ceph-node3 User ceph Host node4 Hostname harbor-ceph-node4 User ceph Host node5 Hostname harbor-ceph-node5 User ceph chmod 644 .ssh/config 需要在ceph 节点上做一下hosts 解析 1.5 创建分区 为ceph 单独挂载一个独立磁盘 fdisk /dev/vdc\tmkfs.xfs /dev/vdc1 mkdir /ceph-harbor blikd 写如到 /etc/fstab mount /dev/vdc1 /ceph-harbor 1.6 安装存储集群 创建ceph 存储集群，它有3个monitor 和5个osd 守护进程。 在10.0.0.10上面 su - ceph mkdir my-cluster cd my-cluster 使用sudo 注意事项 禁用 requiretty 在某些发行版（如 CentOS ）上，执行 ceph-deploy 命令时，如果你的 Ceph 节点默认设置了 requiretty 那就会遇到报错。可以这样禁用此功能：执行 sudo visudo ，找到 Defaults requiretty 选项，把它改为 Defaults:ceph !requiretty ，这样 ceph-deploy 就能用 ceph 用户登录并使用 sudo 了。 1.6.1 创建集群 ceph-deploy new {initial-monitor-node(s)} ceph-deploy new node1 node2 node3 #会在界面进行输出，网络状态好的情况下应该没有问题，不行的话可以替换一下hosts，在github上找一个可以翻墙的hosts 文件 ceph.conf 配置文件默认的副本数3，可以在[global] 段： osd pool default size = 2 如果有多个网卡，可以public network 写入ceph配置文件的[global] public network = {ip-address}/{netmask} 网络配置参考http://docs.ceph.org.cn/rados/configuration/network-config-ref/ 1.6.2 安装ceph ceph-deploy install {ceph-node} [{ceph-node} ...] ceph-deploy install node1 node2 node3 node4 node5 ceph-deploy 将在各节点安装ceph， 安装失败可以清除重新安装 ceph-deploy purge {ceph-node} [{ceph-node}] ​ ​ 配置初始 monitor(s)、并收集所有密钥： ​ ceph-deploy mon create-initial\n1.6.3 配置初始化monitor(s) 并收集所有密钥 ceph-deploy mon create-initial 完成上述操作后，当前目录里应该会出现这些密钥环： {cluster-name}.client.admin.keyring {cluster-name}.bootstrap-osd.keyring {cluster-name}.bootstrap-mds.keyring {cluster-name}.bootstrap-rgw.keyring 1.6.4 部署OSD ceph-deploy osd prepare {ceph-node}:/path/to/directory ceph-deploy osd prepare harbor-ceph-node1:/ceph-harbor harbor-ceph-node2:/ceph-harbor harbor-ceph-node3:/ceph-harbor harbor-ceph-node4:/ceph-harbor harbor-ceph-node5:/ceph-harbor 1.6.4 激活OSD ceph-deploy osd activate {ceph-node}:/path/to/directory ceph-deploy osd activate harbor-ceph-node1:/ceph-harbor harbor-ceph-node2:/ceph-harbor harbor-ceph-node3:/ceph-harbor harbor-ceph-node4:/ceph-harbor harbor-ceph-node5:/ceph-harbor 1.6.5 检查集群状态 ceph health 1.7 创建cephfs 1.7.1 创建元数据库服务器 至少需要一个元数据服务器才能使用 CephFS ，执行下列命令创建元数据服务器： ceph-deploy mds create {ceph-node} 创建mds ceph-deploy mds create harbor-ceph-node1 # 因为我有3个副本，创建一个mds 就可以了 1.7.2 查看元数据的状态 ceph mds stat e5: 1/1/1 up {0=harbor-ceph-node1=up:active} 1.7.3 创建文件系统 存储池、归置组配置参考 http://docs.ceph.org.cn/rados/configuration/pool-pg-config-ref/ http://docs.ceph.org.cn/rados/operations/placement-groups/ 1.7.4 创建存储池 要用默认设置为文件系统创建两个存储池 ceph osd pool create cephfs_data \u0026lt;pg_num\u0026gt; ceph osd pool create cephfs_metadata \u0026lt;pg_num\u0026gt; 创建存储池 ceph osd pool create cephfs_data 625 ceph osd pool create cephfs_metadata 625 1.7.6 创建文件系统 创建好存储池后，你可以用fs new 命令创建文件系统： ceph fs new \u0026lt;fs_name\u0026gt; \u0026lt;metadata\u0026gt; \u0026lt;data\u0026gt; ceph fs new cephfs cephfs_metadata cephfs_data ceph fs ls name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ] 1.7.7 查看状态 文件系统创建完毕后，MDS 服务器就能active, 比如在一个单MDS 系统中： ceph mds stat e5: 1/1/1 up {0=harbor-ceph-node1=up:active} 1.8 客户端mount cephfs 参考资料 http://docs.ceph.org.cn/cephfs/kernel/ http://docs.ceph.org.cn/start/quick-cephfs/#id3 客户端： 第一步 创建挂载目录 mkdir /harbor 第二步 查看cephfs 秘钥 name 和secret 在ceph 的osd 下面目录去获取 cat /etc/ceph/ceph.client.admin.keyring 第三步 mount 文件系统 如果有多个监视器： mount.ceph monhost1,monhost2,monhost3:/ /mnt/foo mount -t ceph 10.0.0.1:6789,10.0.0.2:6789,10.0.0.3:6789:/ /harbor -o name=admin,secret=AQCprj5ZAilVHRAA73bX47zr2fGKpIwgk/DGsA== 第四步 写入fstab 参考资料 http://docs.ceph.com/docs/master/cephfs/fstab/ vim /etc/fstab 10.0.0.1:6789,10.0.0.2:6789,10.0.0.3:6789:/ /harbor ceph name=admin,secret=AQCprj5ZAilVHRAA73bX47zr2fGKpIwgk/DGsA==,noatime,_netdev 0 0 2 harbor 安装 机器列表 10.0.0.12 harbor02 10.0.0.13 harbor03 2.1 安装docker https://docs.docker.com/engine/installation/linux/centos/#install-from-a-package yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo yum-config-manager --enable docker-ce-edge yum install docker-ce -y ​\n2.2 安装docker-compose curl -L https://github.com/docker/compose/releases/download/1.13.0/docker-compose-`uname -s`-`uname -m` \u0026gt; /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose docker-compose --version docker-compose version 1.13.0, build 1719ceb 3.安装harbor 3.1 下载harbor harbor 程序包下载 https://github.com/vmware/harbor/releases 安装目录/opt/harbor ./prepare ./install.sh 默认安装导出数据库 3.2 harbor 数据库导出 docker exec -ti 2f2df11a189f bash #数据库的默认密码在cat common/config/adminserver/env 中查看 mysqldump -u root -p --databases registry \u0026gt; registry.dump 退出container,将备份的数据copy 出来 docker cp 2f2df11a189f:/tmp/registry.dump . 3.3 导入数据库文件 mysql -uharbor -h 10.0.0.250 -p source ./registry.dump; ​\n3.4 配置外接数据库 cp docker-compose.yml docker-compose.yml.bak 修改docker-compose.yml 删除的部分，并将以前的80端口修改为8090 ```yaml mysql: image: vmware/harbor-db:v1.1.1 container_name: harbor-db restart: always volumes: - /data/database:/var/lib/mysql:z networks: - harbor env_file: - ./common/config/db/env depends_on: - log logging: driver: \u0026quot;syslog\u0026quot; options: syslog-address: \u0026quot;tcp://127.0.0.1:1514\u0026quot; tag: \u0026quot;mysql\u0026quot; - mysql ``` 3.5 修改harbor 引入的环境变量 vi common/templates/adminserver/env MYSQL_HOST=mysql MYSQL_PORT=3306 MYSQL_USR=root MYSQL_PWD=$db_password # 这里不需要修改密码，直接在harbor.cfg 中修改db_password 密码，然后传入变量进来 RESET=true 3.6 添加redis 这里需要注意如果使用 vim common/templates/ui/env 在最后一行添加 _REDIS_URL=reids_ip:port,100,redis_password,0 _REDIS_URL=10.0.0.10:6379,100,Ar9as445p4vZ,0 3.7 修改配置harbor.cfg 配置LDAP，新版本的1.1.1 在配置的时候需要不增加用户的情况下配置LDAP，如果重启的话也需要去数据库里面删除用户，然后才可以配置测试LDAP 编辑harbor.cfg 主配置文件 vim harbor.cfg hostname = 域名 ui_url_protocol = http db_password = 数据库密码 ssl_cert = /harbor/data/cert/server.crt # 放在共享存储上 ssl_cert_key = /harbor/data/cert/server.key # 放在共享存储上 secretkey_path = /harbor/data # 放在共享存储上 harbor_admin_password = Harbor12345 # harbor 登录密码设置 #auth_mode = db_auth auth_mode = ldap_auth 这里需要注意的是，第一次启动的可以选择db 认证，在web 界面修改为LDAP之后，也同时也需要在配置文件中修改LDAP ./prepare ./install.sh harbor 默认只支持一种认证，所以配置LDAP 之后，需要在配置文件也修改为LDAP认证。 备注：在web界面修改的密码以及配置的邮件同时也需要添加到harbor.cfg 中去 3.8 节点加入LB 配置LB 使用keeplived 或者云平台的LB选择ip hash 解析，否则docker login 的时候会报认证的错误 ​ ​ 如果harbor各个节点上harbor.cfg 中的hostname= 配置为ip加端口，那么harbor 页面会显示镜像的名称为ip:port ，但是前面push pull 仍然可以使用域名可以正常使用。\n3.9 增加节点 1. 在harbor 上创建一个仓库，可以设置为私有或者公开，把harbor 所需要的镜像上传到harbor中，然后修改docker-compose.yml 中的镜像地址为镜像仓库的地址。 2. 直接把其他节点的/opt/harbor 这个目录打包为harbor.tar.gz 直接copy 到其他节点上，然后解压直接运行 ./prepare ./install.sh 运行之后使用ip 访问，确认没有问题直接直接加入到LB中 3.10 修改密码 因为共用一个数据库，如果需要修改admin 的密码那么在每一个节点的harbor.cfg 中也需要添加新的密码， ","permalink":"https://xingxing.io/posts/kubernetes/harbor-ha/","tags":["harbor ha"],"title":"Harbor Ha"},{"categories":["devops"],"contents":"ansible playbook 的学习 ansible playbook playbooks 是ansible 的配置 部署 编排语言，他们可以被描述为一个需要希望远程主机执行命令的方案，或者一组IT程序运行的命令 是以YAML 格式的来操作的，所以遵循如下格式 规则一 缩进 yaml 使用一个固定的缩进风格表示数据层结构关系，需要每个缩进级别由两个空格组成，一定不能使用tab 键 规则二 冒号 每个冒号后面一定要有一个空格(以冒号结尾不需要空格，表示文件路径的模板可以不需要空格) 规则三 短横线 想要表示列表项，使用一个短横杠加一个空格，多个项使用同样的缩进级别作为同一个列表的一部分 ​\n配置 有以下核心组件 tasks: 任务，由模块定义的操作的列表 variables: 变量 templates: 模板，即使用了模板语法的文本文件 handlers: 由特定条件出发的tasks roles: 角色 ​\n基础组件 Hosts: 运行指定任务的目标主机 remote_user: 在远程主机以哪个用户身份运行 sudo_user: 非管理员用户由哪一些组成 tasks: 任务列表 由模块与木块参数组成 ansible-playbook --syntax-check /path/to/playbook.yaml 使用--syntan-check 做语法检测 测试运行 -C --list-host 影响的主机 --list-tasks 列出任务列表 --list-tags 列出所有标签 运行ansible-playbook ​\n例子： --- - hosts: all remote_user: root tasks: - name: install nginx package yum: name=nginx ignore_errors: yes - name: start nginx service service: name=nginx state=started 忽略错误ignore_errors: yes playbook的语法具有如下特性 1. 需要以\u0026quot;---\u0026quot;(3个减号)开始，且需顶行首写 2. 次行开始正常些ploybook的内容 3. 使用#号注释代码 4. 缩进必须是统一的，不能将空格和tab混用 5. 缩进的级别必须一致，同样的缩进代表同样的级别，程序判别配置的级别是通过缩进结合换行来实现的 6. YAML文件内容和linux系统大小写判断方式保持一致，是区别大小写的，k/v 值均需大小写敏感 7. k/v 值可同行写也可换行写.同行使用“:” 分割，换行写需要以“-” 分割， 8. 一个完整的代码块功能需要最少元素，需包括name:task 9. 一个name只能包括一个task 变量 ansible-playbook -e pkgname=httpd op.yaml --- - hosts: all remote_user: root tasks: -name: install {{pkgname}} yum: name={{ pkgname }} tags: install {{pkgname}} -name: start {{pkgname}} service: name={{ pkgname }} state=started enabled=true 也可以指定变量 - hosts: all remote_user: root vars: - pkgname: nginx tasks: -name: install {{pkgname}} yum: name={{ pkgname }} tags: install {{pkgname}} -name: start {{pkgname}} service: name={{ pkgname }} state=started enabled=true ansible-playbook 其他选项技巧 --inventory=PATH(-i PATH): 指定inventory 文件，默认文件在/etc/ansible/hosts --verbose(-v): 显示详细输出 --forks=num(-f num): 指定并发执行的任务数，默认为5 --check (-c) playbook 检测， 实战 shell 脚本与playbook的转换 - hosts: all tasks: - name: \u0026quot;安装apche\u0026quot; command: yum install httpd - name: 复制配置文件 command: cp /tmp/httpd.conf /etc/httpd/conf/httpd.conf ansible-playbook ansible-nginx.yaml --list-host --list-host playbook 拓展 ​\n","permalink":"https://xingxing.io/posts/devops/ansible-playbook/","tags":["devops"],"title":"Ansible Playbook"},{"categories":["kubernetes"],"contents":"kubernetes cka 证书涉及的知识点 1. 应用程序生命周期 #创建应用 kubectl create deployment web --image=nginx #使用service 将pod暴露出去 kubectl expose deployment web --port=80 --type=NodePort --target-port=80 --name=web #查看 kubectl get pods,svc #访问应用 http://NodeIP:Port #端口随机生成，通过get svc获取 2. YAML 文件创建资源对象 kubectl create 创建一个资源 kubectl apply 从一个文件创建或者更新资源 标签重要性: 通过标签关联不同的资源 #查看service 关联的pod ip kubectl get endpoints apiVersion API版本 kind 资源类型 metadata 资源元数据 spec 资源规格 replicas 副本(实例)数量 selector 标签选择器，与下面metadata.labels 保持一致 template pod模板 spec pod 的规格 containers 容器配置 3. Deployment 介绍 Deployment 是最常用的k8s 工作负载控制器，是k8s 的一个抽象概念，用于更高级层次对象，部署和管理pod。 主要功能: 管理Pod 与ReplicaSet 具有上线部署、副本设定、滚动升级、回滚等功能 提供声明式更新，例如只更新一个新的image 应用场景: 网站 API 微服务 deployment 应用生命周期管理流程 应用程序——部署——升级——回滚——下线 当执行apply 部署应用时，deployment 向RS pod 扩容副本数量 回滚 kubectl rollout history deployment/web 查看历史版本 kubectl rollout undo deployment/web 回滚上一个版本 kubectl rollout undo deployment/web --to-revision=2 回滚历史指定版本 #回滚是重新部署某一次部署时的状态，即当时版本所有配置 4. Pod对象 Pod 是kubernetes 创建和管理的最小单元，一个pod 由一个容器或多个容器组成，这些容器共享存储、网络 POD的特点 一个pod 可以理解为一个应用实例，提供服务 Pod中容器始终部署一个Node上 Pod 中容器共享网络，存储资源 kubernetes直接管理pod，而不是容器 pod的主要用法: 运行单个容器:最常见的用法，在这种情况下，可以将pod看做是单个容器的抽象封装 运行多个容器: 封装多个紧密耦合且需要共享资源的应用程序 管理命令 创建pod: kubectl apply -f pod.yaml 或者使用命令: kubectl run nginx --image=nginx 查看pod: kubectl get pods kubectl describe pod \u0026lt;pod名称\u0026gt; -n namesapce 查看日志: kubectl logs \u0026lt;pod名称\u0026gt; -n namesapces kubectl logs \u0026lt;pod名称\u0026gt; -n namespaces 进入终端 kubectl exec \u0026lt;pod名称\u0026gt; -n namespaces bash/sh 删除pod kubectl delete pod \u0026lt;pod名称\u0026gt; Pod 重启策略+健康检查 重启策略(restartPolicy) * Always: 当容器终止退出后，总是重启容器，默认策略 * OnFailure: 当容器异常退出(退出状态码非0)时，才重启机器 * Never: 当容器终止退出，从不重启容器 健康检查有以下两种类型: * livenessProbe(存活检查): 如果检查失败了，将杀死容器，根据pod的 restartPolicy 来操作 * readinessprobe(就绪检查): 如果检查失败，kubernetes会把pod 从service endpoints 剔除 支持以下三种检查方法: httpGet: 发送HTTP请求，返回200-400范围状态码为成功 exec: 执行shell 命令返回状态码是0为成功 tcpSocket: 发起TCP Socket建立成功 环境变量 变量值得几种定义方式 * 自定义变量值 * 变量值从Pod 属性获取 * 变量值从Secret、ConfigMap 获取 kubernetes 调度 kubernetes 是基于list-watch机制控制的架构，实现组件间交互的解耦。 其他组件监控自己负责的资源，当这些资源发生变化时，kube-apiserver 会通知这些组件，这个过程类似发布与订阅 master: apiserver controller-manager scheduler node: kube-proxy kubelet docker 创建一个pod的流程 1. kubelet 向apiserver发送一个创建pod 的请求 2. apiserver 接收到并向etcd 写入存储，写入成功后返回一个提示 3. scheduler 向apiserver查询未分配的pod资源，通过自身调度算法选择一个合适node 进行绑定(给这个pod打一个标记，标记分配到node1) 4. kubelet 向apiserver 查询分配到自己节点的pod，调用docker api(/var/run/docker.sock) 创建容器 5. kubelet 获取docker 创建容器的状态，并汇报给apiserver，apiserver 更新状态到etcd 存储 6. kubectl get pods 就能查看到pod状态 资源限制对调度的影响 apiVersion: v1 kind: Pod metadata: labels: run: resources name: resources spec: containers: - image: nginx name: web resources: #最小资源 requests: cpu: 500m memory: 512Mi limits: cpu: 2000m memory: 1000Mi#pod会根据Request 的值去查找有足够资源的Node来调度此Pod limits 是最大可用资源，一般是requests的20% 左右，不太超出太多 limits 不能小于requests reqeusts 只是一个预留机制，不是pod配置写多少，宿主机就会占多少资源k8s 根据reqeusts来统计每个节点预分配资源，来判断下一个pod 能不能分配到这个节点 nodeSelector \u0026amp; nodeAffinity nodeSelector: 用于将pod调度到匹配Label的Node上，如果没有匹配的标签会调度失败。 作用: 约束pod到特定节点运行 完全匹配节点标签应用场景: 专用节点: 根据业务线将node分组管理 配置特殊硬件: 部分node 配有SSD硬盘 GPU 第一步 给节点打一个标签kubectl label node k8s-node1 disktype=ssd 第二步 pod的yaml修改 root@k8s-master-01:~# cat resources1.yaml apiVersion: v1 kind: Pod metadata: name: resources-sshspec: nodeSelector: disktype: \u0026#34;ssd\u0026#34; containers: - image: nginx name: web kubectl apply -f resources1.yaml #然后查看是否跑到特定的节点上kubectl get pod -o wide nodeAffinity 节点亲和性类似于nodeSelector,可以根据节点上的标签来约束pod可以调度哪些节点 相比nodeSelector: 匹配有更多的逻辑组合，不只是字符串的完全相等，支持的操作 符有: In、Notln、Exists、DoesNotExist、Gt、Lt 调度氛围软策略和硬策略，而不是硬性要求 硬(required): 必须满足 软(preferred): 尝试满足，但不保证# 硬性限制cat resources1.yaml apiVersion: v1kind: Podmetadata: name: nodeaffinity-podspec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: type ### key的值 operator: In values: - gpu ### values的值 containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent #软性限制 apiVersion: v1kind: Podmetadata: name: nginxspec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 # 权重值,1-100，权重越大，调度到匹配节点的概率越大 preference: matchExpressions: - key: disktype operator: In values: - ssd containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent Taint(污点)与Tolerations(污点容忍) Taints: 避免Pod调度到特定Node 上Tolerations: 允许pod调度到持有Taints的Node上应用场景: 专用节点: 根据业务线将Node分组管理，希望在默认情况下不调度该节点，只有配置了污点容忍才允许分配 配备特殊硬件: 部分Node 配有SSD硬盘、GPU，希望在默认情况下不调度该节点，只有配置看了污点容忍才允许配置 基于Taint的驱逐 Taint污点 格式kubectl taint node [node] key=value:[effect]# 例子kubectl taint node k8s-node1 gpu=yes:NoSchedule其中[effect]可取值:NoSchedule: 一定不能被调度PreferNoSchedule: 尽量不要调度，非必须配置容忍NoExecute: 不仅不会调整，还会驱逐Node上已有的Pod# 污点容忍是可以直接放宽条件写的，例如calico 不分key 与value，符合带有污点的是NoSchedule tolerations: - effect: NoSchedule opeartor: Exists nodeName nodeName 指定节点名称，不经过调度器，并且给定节点上运行的kubelet 进程尝试执行该Pod，因此，如果nodeName在PodSpec中指定了，则它优先于上面的节点选择方法。 例子:apiVersion: v1kind: Podmetadata: name: nginxspec: containers: - name: nginx image: nginx nodeName: kube-01 service 介绍 将运行在一组pod上的应用程序公开为网络服务的抽象方法。使用kubernetes，你无需修改应用程序即可使用不熟悉的服务发现机制，kubernetes 为pods提供自己的IP地址，并为一组Pod 提供相同的DNS 名，并且可以在它们之间进行负载均衡。 使用Pod IP就面临着ip 变化的情况，因为Pod 是非永久性资源，如果使用deployment 来运行你的应用程序，则它可以动态创建和销毁Pod，这样的话就会导致ip 随时变化。# server 资源kubernetes Service 是一种抽象的定义，Service 是不可变的，除非删除，这样的话可以有效的解决Pod IP变动的话问题，也可以有效的解耦这种关系。 即使有多个Pod，那么只需要连接一个可用的Service 即可解决负载问题。 定义Service apiVersion: v1kind: Servicemetadata: name: my-service #Service 的名字spec: selector: app: MyApp # 指定关联Pod的标签 ports: - protocol: TCP #协议 port: 80 # service 端口 targetPort: 9376 # 容器端口(应用程序监听端口) type: ClusterIP #服务类型 Service 三种常用类型 ##ClusterIP ClusterIP 集群内部使用，只能在k8s 集群内## NodePort NodePort 对外暴露应用,通过每个节点上的IP和静态端口(NodePort)暴露服务。NodePort 服务会路由到自动创建的ClusterIP 服务。通过请求\u0026lt;节点IP\u0026gt;:\u0026lt;节点端口\u0026gt;，可以从集群的外部访问一个NodePort服务，端口范围30000-32767 在apiserver 里面可以修改端口范围。 NodePort 会在每台Node 上监听端口接收用户流量， ###LoadBalancer LoadBalancer 对外暴露应用，适用公有云 Pod 对外暴露: 集群之内的其他应用 Service 代理模式 ipvs 在ipvs 模式下，kube-proxy 监视kubernetes 服务和端点，调用netlink 接口相应地创建IPVS规则，并定期将IPVS规则与kubernetes 服务和端点同步，该控制循环可确保IPVS状态与所需状态匹配。访问服务时，IPVS将流量定向到后端Pod之一。IPVS 代理模式基于类似于iptables模式的netfilter 挂钩函数，但是使用哈希表作为基础数据结构，并且在内核空间中工作。这意味着，与iptables 模式下的kube-proxy 相比，IPVS 模式下的kube-proxy 重定向通信延迟要短，并且在同步代理规则时具有更好的性能。与其他代理模式相比，IPVS模式还支持更高的网络流量吞吐量。 IPVS 提供了更多选项来平衡后端Pod 的流量，这些是:rr: 轮替lc: 最少链接dn: 目标地址哈希sh: 源地址哈希sed: 最短预期延迟nq: 从不排队 iptables kube-proxy 会监视kubernetes控制节点对Service 对象和Endpoints 对象的添加和移除。对每个Service，它会配置iptables 规则，从而捕获到达该Service 的clusterIP 和端口的请求，进而将请求重定向到Service的一组后端中的某个Pod上面。对于Endpoints对象，它也会配置iptables 规则，这个规则会选择一个后端组合。 DNS coresDNS: 是一个DNS 服务器，kubernetes 默认采用，以Pod 部署在集群中，CoreDNS 服务监视kubernetes API，为每一个Service 创建DNS 记录用于域名解析ClusterIP A 记录格式 \u0026lt;service-name\u0026gt;.\u0026lt;namespace-name\u0026gt;.svc.cluster.local Ingress ingress 公开了从集群外部到集群内服务的HTTP与HTTPS路由规则集合，而具体实现流量路由是有Ingress Controller 负责。 官网文档https://kubernetes.io/zh/docs/concepts/services-networking/ingress/ 需要先部署ingress-controlle 组件https://github.com/kubernetes/ingress-nginx 创建ingress-nginx 规则 cat ingress-web.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: test-ingressspec: rules: - host: web.xingxing.io http: paths: - path: / pathType: Prefix backend: service: name: web # 关联service 的名称 port: number: 80## ingress 配置https Ingress Controller 怎么工作的？ Ingress Controller 通过与kubernetes API 交互，动态的去感知集群中Ingress 规则变化，然后读取它，按照自定义的规则，规则就是写明了哪个域名对应哪个service，生成一段Nginx 配置，应用到管理的Nginx 服务，然后热加载生效。 以此来达到Nginx 负载均衡器配置与动态更新的问题 网络存储卷 NFS NFS卷: 提供对NFS挂载支持，可以自动将NFS共享路径挂载到Pod 中 # 安装nfs 插件 yum install nfs-utils -y ConfigMap ConfigMap 是一种API 对象，用来将非机密性的数据保存到键值对中。使用时，Pods 可以将其用作环境变量、命令行参数或者存储卷中的配置文件。 ConfigMap 将你的环境配置信息和容器镜像解耦，便于应用配置的修改 例子\napiVersion: v1kind: ConfigMapmetadata: name: game-demodata: # 类属性键；每一个键都映射到一个简单的值 player_initial_lives: \u0026#34;3\u0026#34; ui_properties_file_name: \u0026#34;user-interface.properties\u0026#34; # 类文件键 game.properties: | enemy.types=aliens,monsters player.maximum-lives=5 user-interface.properties: | color.good=purple color.bad=yellow allow.textmode=true #以环境变量方式使用的ConfigMap 数据不会被自动更新。更新这些数据需要重新启动Pod apiVersion: v1kind: Podmetadata: name: configmap-demo-podspec: containers: - name: demo image: alpine command: [\u0026#34;sleep\u0026#34;, \u0026#34;3600\u0026#34;] env: # 定义环境变量 - name: PLAYER_INITIAL_LIVES # 请注意这里和 ConfigMap 中的键名是不一样的 valueFrom: configMapKeyRef: name: game-demo # 这个值来自 ConfigMap key: player_initial_lives # 需要取值的键 - name: UI_PROPERTIES_FILE_NAME valueFrom: configMapKeyRef: name: game-demo key: ui_properties_file_name volumeMounts: - name: config mountPath: \u0026#34;/config\u0026#34; readOnly: true volumes: # 你可以在 Pod 级别设置卷，然后将其挂载到 Pod 内的容器中 - name: config configMap: # 提供你想要挂载的 ConfigMap 的名字 name: game-demo # 来自 ConfigMap 的一组键，将被创建为文件 items: - key: \u0026#34;game.properties\u0026#34; path: \u0026#34;game.properties\u0026#34; - key: \u0026#34;user-interface.properties\u0026#34; path: \u0026#34;user-interface.properties\u0026#34; Secret Secret 对象类型用来保存敏感信息，例如密码、OAuth 令牌和SSH秘钥。 kubernetes Secret 默认情况下存储为base64-编码的、非加密的字符串。默认情况下，能够访问API 的任何人，或者能够访问kubernetes 下层数据存储(etcd)的任何人都可以明文方式读取这些数据。为了能够安全地使用Secret，可以配合使用RBAC 规则来限制。 kubernetes 安全框架 k8s 安全控制框架主要由下面3个阶段进行控制，每一个阶段都支持插件方式，通过API Server 配置来启用插件。 1. Authentication(鉴权)2. Authorization(授权)3. Admission Control(准入控制)客户端要访问k8s集群API Server，一般需要证书、Token 或者用户名+密码；如果Pod 访问，需要ServiceAccount #k8s APIserver 提供三种客户端身份认证https 证书认证: 基于CA证书签名的数字证书认证(kubeconfig)http Token 认证： 通过一个Token 来识别用户(serviceaccount)http Base认证： 用户名+密码的方式认证(已弃用) RBAC(Role-Based Access Control,基于角色的访问控制：负责完成授权(Authorization))工作。 RBAC根据API 请求属性，决定允许还是拒绝。 比较常见的授权维度:user: 用户group: 用户分组资源,例如pod、deployment 资源操作方法: get list create update patch watch delete 命名空间API 组 准入控制(Admission Control ) Admission Control 实际上市一个准入控制器插件列表，发送到API Server 的请求都需要经过这个列表中的每个准入控制器插件的检查，检查不通过，则拒绝请求。 启用准入控制kube-apiserver --enable-admission-plugins=NamespaceLifecycle,LimitRanger ...关闭准入控制kube-apiserver --disable-admission-plugins=PodNodeSelector,AlwaysDeny ...查看插件哪些是默认启用的kube-apiserver -h | grep enable-admission-plugins 基于角色的权限访问控制: RBAC RBAC (Role-Based Access Control, 基于角色的访问控制)是k8s 默认授权策略，并且是动态配置策略(修改即时生效)。#主体(subject)User 用户Group 用户组SeriveAccount 服务账号#角色Role 授权特定命名空间的访问权限ClusterRole: 授权所有命名空间的访问权限# 角色绑定RoleBinding 将角色绑定到主体(subject) # 指定的命名空间中执行授权ClusterRoleBinding 将集群角色绑定到主体 #在集群范围执行授权 etcd 集群备份恢复 #单实例备份## 备份ETCDCTL_API=3 etcdctl snapshot save snap.db --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key ## 恢复# 1. 先暂停kube-apiserver和etcd容器 mv /etc/kubernetes/manifests/ /etc/kubernetes/manifests.bak mv /var/lib/etcd/ /var/lib/etcd.bak #2. 恢复 ETCDCTL_API=3 etcdctl snapshot restore snap.db --data-dir=/var/lib/etcd #3. 启动kube-apiserver 和etcd容器 mv /etc/kubernetes/manifests.bak /etc/kubernetes/manifests ####集群的备份，备份文件使用一个节点上的，--endpoints=IP地址是主机自己的IP地址#备份ETCDCTL_API=3 etcdctl snapshot save snap.db --endpoints=https://$IP:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key #集群恢复 1. 先暂停kube-apiserver 和etcd ###二进制systemctl stop kube-apiserver systemctl stop etcd #kubeadm 与单集群一样###在每个master 节点上进行恢复先将刚在master 节点上备份的数据拷贝到其他节点上。确保恢复数据的一致性--name k8s-master-01(etcd 配置文件写的名称，每个节点都不一样)--initial-advertise-peer-urls=https://10.39.60.175:2380(在那个节点恢复写哪个节点IP)--data-dir=/var/lib/etcd # etcd 数据读取的目录，需要和配置文件一致 ETCDCTL_API=3 etcdctl snapshot restore snap.db --name k8s-master-01 --initial-cluster=\u0026#34;k8s-master-01=https://10.39.60.175:2380,k8s-master-03=https://10.39.60.157:2380,k8s-master-02=https://10.39.60.154:2380\u0026#34; --initial-cluster-token=etcd-cluster --initial-advertise-peer-urls=https://10.39.60.175:2380 --data-dir=/var/lib/etcd ### 启动kube-apiserver 和etcd容器mv /etc/kubernetes/manifests.bak /etc/kubernetes/manifests ##kubeadm 方式### 二进制方式systemctl start etcd systemctl start kube-apiserver ","permalink":"https://xingxing.io/posts/kubernetes/kubernetes-cka/","tags":["kubernetes"],"title":"Kubernetes Cka"},{"categories":["kubernetes"],"contents":"kubernetes 1.20 使用containerd 安装集群 1. 节点规划 主机名 IP地址 角色 配置 k8s-master-03 10.39.60.157 Master 4C8G/20GB k8s-master-02 10.39.60.154 Master 4c8G/20GB k8s-master-01 10.39.60.175 master 4c8G/20GB 信息 备注 系统版本 debian Linux k8s-master-01 4.19.0-12-amd64 Docker版本 无 k8s 版本 1.20 Pod 网段 172.168.0.0/16 Service 网段 192.168.0.0/12 LB 用云端的alb 2.基本信息配置 2.1 所有节点配置hosts 10.39.60.157 k8s-master-03 10.39.60.154 k8s-master-02 10.39.60.175 k8s-master-01 2.2 安装软件包 2.3 关闭swap分区 swapoff -a \u0026amp;\u0026amp; sysctl -w vm.swappiness=0 sed -ri \u0026#39;/^[^#]*swap/s@^@#@\u0026#39; /etc/fstab 2.4 时钟同步 ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime echo \u0026#39;Asia/Shanghai\u0026#39; \u0026gt;/etc/timezone ntpdate time2.aliyun.com # 加入到crontab */5 * * * * ntpdate time2.aliyun.com 2.5 设置limit # /etc/security/limits.conf * soft nofile 655360 * hard nofile 131072 * soft nproc 655350 * hard nproc 655350 * soft memlock unlimited * hard memlock unlimited 2.6 master01 节点免秘钥登录其他节点 ssh-keygen -t rsa for i in k8s-master-02 k8s-master-03 k8s-master-01; do ssh-copy-id -i .ssh/id_rsa.pub $i;done 2.7 内核配置 内核参数 cat \u0026gt; /etc/sysctl.d/kubernetes.conf \u0026lt;\u0026lt;EOF #将桥接的IPv4流量传递到iptables 的链 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system # 生效 3. 安装 https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/#containerd 3.1 containerd 安装 安装和配置 cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/containerd.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter # 设置必需的 sysctl 参数，这些参数在重新启动后仍然存在。 cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF # 应用 sysctl 参数而无需重新启动 #安装containerd sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \\ \u0026#34;deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\ $(lsb_release -cs) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt-get install containerd.io #配置 sudo mkdir -p /etc/containerd containerd config default | sudo tee /etc/containerd/config.toml #重新启动containerd sudo systemctl restart containerd 使用 systemd cgroup 驱动程序 结合 runc 使用 systemd cgroup 驱动，在 /etc/containerd/config.toml 中设置 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc] ... [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] SystemdCgroup = true #重启 vim /etc/containerd/config.toml sudo systemctl restart containerd 3.2 kubeadm 安装 https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#configure-cgroup-driver-used-by-kubelet-on-control-plane-node sudo apt-get updatesudo apt-get install -y apt-transport-https ca-certificates curlapt-get update \u0026amp;\u0026amp; apt-get install -y apt-transport-httpscurl -s https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/apt/sources.list.d/kubernetes.listdeb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial mainEOF#指定版本安装apt-get install kubelet=1.20.0-00 kubeadm=1.20.0-00 kubectl=1.20.0-00 3.3 启动kubelet systemctl daemon-reloadsystemctl enable --now kubelet## 初始化成功之后kubelet 会自动启动 3.4 打印初始化文件 kubeadm config print init-defaults kubeadm config print init-defaults --component-configs KubeletConfiguration kubeadm config print init-defaults --component-configs KubeProxyConfiguration ### 根据打印的文件可以修改为自己的也可以使用 kubeadm init\\ --apiserver-advertise-address=10.39.60.175\\ --image-repository registry.aliyuncs.com/google_containers\\ --kubernetes-version v1.20.0\\ --service-cidr=10.96.0.0/12\\ --pod-network-cidr=10.244.0.0/16\\ --control-plane endpoint=10.39.60.221:6443\\ --ignore-preflight-errors=all #--control-plane-endpoint 负载均衡的ip，如果不是集群可不写， 集群的配置信息会生成在kubectl -n kube-system get cm kubeadm-config -oyaml 这里，方便查看,如果涉及到修改可能需要kubeadm reset 重置集群 3.4 输出信息 Your Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configAlternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.confYou should now deploy a pod network to the cluster.Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of control-plane nodes by copying certificate authoritiesand service account keys on each node and then running the following as root: kubeadm join 10.39.60.221:6443 --token 05iqjt.mk4i4jijsq3t89mu \\ --discovery-token-ca-cert-hash sha256:745f7ea4bad5e2f9a62157411a4c81e1eb3dccbcfca2a4e9547e8a8016f3824d \\ --control-plane Then you can join any number of worker nodes by running the following on each as root:kubeadm join 10.39.60.221:6443 --token 05iqjt.mk4i4jijsq3t89mu \\ --discovery-token-ca-cert-hash sha256:745f7ea4bad5e2f9a62157411a4c81e1eb3dccbcfca2a4e9547e8a8016f3824d #--control-plane 为加入Master 节点 #token具有实效性，如果失效，可以创建kubeadm token create –print-join-command 创建新的 join token#故障解决由于各种原因会导致的失败kubeadm reset 重新初始化 3.8 master1 上操作 mkdir -p $HOME/.kubecp -i /etc/kubernetes/admin.conf $HOME/.kube/configchown $(id -u):$(id -g) $HOME/.kube/configexport KUBECONFIG=/etc/kubernetes/admin.conf# kubectl get node 查看集群信息 3.9 安装calico https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises curl https://docs.projectcalico.org/manifests/calico-typha.yaml -o calico.yaml #需要修改calico.yaml 中CALICO_IPV4POOL_CIDR 与kubeadm init 指定的 --pod-network-cidr 一致 kubectl apply -f calico.yaml 3.10 加入其他Master 节点 ## 根据提示执行 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 3.11 遇到的故障 #故障一 root@k8s-master-02:~# kubeadm join 10.39.60.175:6443 --token yjqd42.mimqzh2bczx4ck9w --discovery-token-ca-cert-hash sha256:c7b8645c2263f4557a65eb53f9c076a6cf84dbdd4e94fa9a89ea07f671f9d53a --control-plane --certificate-key a4a4df512944b2505a0f10320d7ea2d0857e335860977b57e0a1de48331a5e8f [preflight] Running pre-flight checks [WARNING SystemVerification]: missing optional cgroups: hugetlb[preflight] Reading configuration from the cluster...[preflight] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39;error execution phase preflight: One or more conditions for hosting a new control plane instance is not satisfied.unable to add a new control plane instance a cluster that doesn\u0026#39;t have a stable controlPlaneEndpoint addressPlease ensure that:* The cluster has a stable controlPlaneEndpoint address.* The certificates that must be shared among control plane instances are provided.To see the stack trace of this error execute with --v=5 or higher# 解决# 在k8s-master01 上kubectl -n kube-system get cm kubeadm-config -o yaml 3.12 配置Master to node #这里主要是让master直接可以运行pods #执行命令kubectl taint node node-name node-role.kubernetes.io/master-#禁止master运行pod kubectl taint nodes node-name node-role.kubernetes.io/master=:NoSchedule#增加 ROLES 标签: kubectl label nodes localhost node-role.kubernetes.io/node=#删除 ROLES 标签kubectl label nodes localhost node-role.kubernetes.io/node-#cordns 可以根据节点数进行调整kubectl scale deploy/coredns --replicas=3 -n kube-system #增加副本数 3.15 Metrics Server部署 官网文档参考 #下载到本地wget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.5.0/components.yamlvim components.yaml......................containers: - args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --kubelet-use-node-status-port image: k8s.gcr.io/metrics-server:v0.5.0 #修改为阿里云或者自己本地拉取的 imagePullPolicy: IfNotPresent ### 添加下面几行 command: - /metrics-server - --kubelet-preferred-address-types=InternalIP,Hostname,Internaldns,ExternalDNS,ExternalIP - --kubelet-insecure-tls # 关闭kubelet认证 #开启认证 - --requestheader-client-ca-file #因为镜像需要科学上网拉取，可以先把镜像pull下来，然后修改镜像地址 [root@k8s-master-01 opt]# kubectl apply -f components.yamlserviceaccount/metrics-server createdclusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader createdclusterrole.rbac.authorization.k8s.io/system:metrics-server createdrolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader createdclusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator createdclusterrolebinding.rbac.authorization.k8s.io/system:metrics-server createdservice/metrics-server createddeployment.apps/metrics-server createdapiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created 验证 root@k8s-master-01:~# kubectl top nodeNAME CPU(cores) CPU% MEMORY(bytes) MEMORY% i-konvehyo 49m 1% 703Mi 12% i-mz9wlzx3 45m 1% 832Mi 14% k8s-master-01 243m 6% 1245Mi 21% k8s-master-02 109m 2% 1192Mi 20% k8s-master-03 103m 2% 1153Mi 19% #查看pod 的资源使用kubectl top pods -n kube-system 安装bash-completion 命令补全 apt-get install bash-completionsource \u0026lt;(kubectl completion bash)然后就可以自动补全了 部署kubernetes-dashboard\n# 修改recommended.yaml ## 注释掉Dashboard Secret，不然后面访问网页不安全，证书过期 #apiVersion: v1#kind: Secret#metadata:# labels:# k8s-app: kubernetes-dashboard# name: kubernetes-dashboard-certs# namespace: kubernetes-dashboard#type: Opaque#将type: targetPort 修改为 type: NodePort 38000root@k8s-master-01:~# kubectl apply -f recommended.yaml namespace/kubernetes-dashboard createdserviceaccount/kubernetes-dashboard createdservice/kubernetes-dashboard createdsecret/kubernetes-dashboard-certs createdsecret/kubernetes-dashboard-csrf createdsecret/kubernetes-dashboard-key-holder createdconfigmap/kubernetes-dashboard-settings createdrole.rbac.authorization.k8s.io/kubernetes-dashboard createdclusterrole.rbac.authorization.k8s.io/kubernetes-dashboard createdrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard createdclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard createddeployment.apps/kubernetes-dashboard createdservice/dashboard-metrics-scraper createddeployment.apps/dashboard-metrics-scraper created 创建service account 并绑定默认的cluster-admin 管理员集群角色\nkubectl create serviceaccount dashboard-admin -n kube-systemkubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-adminkubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | awk \u0026#39;/dashboard-admin/{print $1}\u0026#39;) ","permalink":"https://xingxing.io/posts/kubernetes/kubernetes-1.20-containerd/","tags":["kubernetes"],"title":"Kubernetes 1"},{"categories":["kubernetes"],"contents":"Docker 基础知识 1. Docker 是什么？ Docker 是一个用于开发，交付和运行应用程序的开放平台。Docker 使您能够将应用程序与基础架构分开，从而可以快速交付软件，减少编写代码和在生产环境中运行代码之间的延迟 2. Docker 安装 yum install -y yum-utils yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo yum install docker-ce docker-ce-cli containerd.io #yum list docker-ce --showduplicates | sort -r #yum install docker-ce-\u0026lt;VERSION_STRING\u0026gt; docker-ce-cli-\u0026lt;VERSION_STRING\u0026gt; containerd.io systemctl start docker 3. Docker 基本命令 docker restart/stop/start 重启，关闭，启动 docker run -d 后台运行 -p 端口 -h 指定主机名 --dns 指定dns 服务器 --name 容器的名字 --net=“bridge” 容器的网络选择 --privileged=false 容器特权 -v 挂载存储 日志 docker log rmi 删除镜像 rm 删除容器 4. Docker 网络 当docker 启动时，会自动在主机上创建一个docker0 虚拟网桥，实际上是Linux 的一个bridge，可以理解为软件交换机，它会在挂载到它的网口之间进行转发。 创建网络 docker network create -d bridge my-net 网络模式 bridge 为每一个容器分配 设置ip等，并将容器连接到docker0 虚拟网桥，默认为该模式 host 容器不会虚拟出自己的网卡，配置自己的ip等，而是使用宿主机的ip和端口 node 容器有独立的network namespace，但并没有对其进行任何网络设置，如分配veth 和网桥连接 ip 等 container 新创建的容器不会创建自己的网卡和配置自己的ip，而是和一个指定的容器共享ip、端口范围等 5. Dockerfile","permalink":"https://xingxing.io/posts/kubernetes/docker-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","tags":["kubernetes"],"title":"Docker 基础知识"},{"categories":["linux"],"contents":"面试基础知识 TCP 三次握手四次挥手 ******三次握手\n第一次握手: 客户端给服务端发一个SYN 报文，客户端处于SYN_SEND 状态 第二次握手: 服务器收到客户端的SYN的报文之后，会以自己的SYN报文作为应答，并且也是指定了自己的初始化序列号ISN。同时会把客户端的ISN+1 作为ACK的值，表示已经客户端的SYN，此时服务器处于SYN_RCAD状态 第三次握手: 客户端收到SYN报文之后，会发送一个ACK 报文，表示收到服务端的SYN报文，建立连接处于 ESTABLISHED 状态 ****四次挥手\n第一次挥手(FIN=1 seq=x) 假设客户端想要关闭连接，客户端发送一个FIN标志位置为1的包，表示自己已经没有数据可以发送了，但是仍然可以接受数据 客户端进入 FIN_WAIT_1 状态 第二次挥手(ACK=1,ACKnum=x+1) 服务端确认客户端的FIN包，发送一个确认包，表示自己接受到了客户端关闭连接的请求，但是还没有准备好关闭连接。 发送完毕后，服务器端进入CLOSE_WAIT 状态，客户端接收到这个确认包之后，进入FIN_WAIT_2状态，等待服务器端关闭连接 第三次挥手(FIN=1,seq=y) 服务器端准备好关闭连接时，向客户端发送结束连接请求，FIN设置为1。 发送完毕后，服务器端进入 LAST_ACK 状态，等待来自客户端的最后一个ACK 第四次挥手(ACK=1,ACKnum=y+1) 客户端接收到了来自服务器端的关闭请求，发送了一个确认包，并进入 TIME_WAIT 状态，等待可能出现要求重传的ACK包。 服务器端接受到了这个确认包之后，关闭连接，进入CLOSED 状态。 TCP 的状态 LISTEN: 侦听来自远方的TCP端口的连接请求 SYS-SENT: 再发送连接请求后等待匹配的连接请求 SYN-RECEIVED: 再收到和发送一个连接请求后等待对方对连接请求的确认 ESTABLISGED: 代表打开一个连接 FIN-WAIT-1: 等待远程TCP连接中断请求，或先前的连接中断请求的确认 FIN-WAIT-2: 从远程TCP等待连接中断请求 CLOSE-WAIT: 等待从本地用户发来的连接中断请求 LAST-ACK: 等待原来的发向远程TCP的连接中断请求的确认 TIME-WAIT: 等待足够的时间以确保远程TCP接收到连接中断请求的确认 CLOSED: 没有任何连接状态 ","permalink":"https://xingxing.io/posts/linux/%E9%9D%A2%E8%AF%95%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","tags":["linux"],"title":"面试基础知识"},{"categories":["devops"],"contents":"harbor 2.1 安装 1. 下载harbor 从这里下载发布的正式版本 下载了harbor-offline-installer-v2.1.1.tgz 包 解压之后就会看到这些文件 harbor.yaml 是用来设置系统参数 参考官网介绍 https://goharbor.io/docs/2.0.0/install-config/configure-yml-file/ 2.证书配置 hostname: reg.mydomain.com http: port: 80 https: port: 443 certificate: /opt/ssl/****.crt private_key: /opt/ssl/*****.key external_url: https://reg.domain.com harbor_admin_password: Harbor12345 clair: updaters_interval: 12 trivy: ignore_unfixed: false skip_update: false insecure: false jobservice: max_job_workers: 10 notification: webhook_job_max_retry: 10 chart: absolute_url: disabled log: level: info local: rotate_count: 50 rotate_size: 200M location: /var/log/harbor _version: 2.0.0 3.外接DB设置 psql -U root -h 10.39.61.242 -d postgres #创建harbor 用户，并创建harbor 所涉及的数据库 create user harbor with password \u0026#39;harbor123\u0026#39;; CREATE DATABASE harbor; create database harbor_clair; create database notary_signer; create database notary_server; #授权 GRANT ALL PRIVILEGES ON DATABASE harbor to harbor; GRANT ALL PRIVILEGES ON DATABASE harbor_clair to harbor; GRANT ALL PRIVILEGES ON DATABASE notary_signer to harbor; GRANT ALL PRIVILEGES ON DATABASE notary_server to harbor; #配置 external_database: harbor: host: ***** port: 5432 db_name: harbor username: harbor password: **** ssl_mode: disable max_idle_conns: 2 max_open_conns: 0 clair: host: **** port: 5432 db_name: clair username: harbor password: *** ssl_mode: disable notary_signer: host: ***** port: 5432 db_name: notary_signer username: **** password: **** ssl_mode: disable notary_server: host: ***** port: 5432 db_name: notary_server username: **** password: **** ssl_mode: disable 4 redis 配置 external_redis: host: ***:6379 password: enN12345 registry_db_index: 1 jobservice_db_index: 2 chartmuseum_db_index: 3 clair_db_index: 4 trivy_db_index: 5 idle_timeout_seconds: 30 5.存储配置(S3) registry 存储可以参考官网https://docs.docker.com/registry/configuration/\nstorage_service: s3: accesskey: **** secretkey: **** region: **** regionendpoint: **** bucket: harbor encrypt: false keyid: mykeyid secure: true v4auth: true chunksize: 5242880 multipartcopychunksize: 33554432 multipartcopymaxconcurrency: 100 multipartcopythresholdsize: 33554432 rootdirectory: /s3/harbor ","permalink":"https://xingxing.io/posts/devops/harbor-2.1.1-%E5%AE%89%E8%A3%85/","tags":["devops"],"title":"Harbor 2.1.1 安装"},{"categories":["linux"],"contents":"磁盘IO 存在瓶颈，如何找到元凶\n","permalink":"https://xingxing.io/posts/linux/%E7%A3%81%E7%9B%98io/","tags":["linux"],"title":"磁盘IO"},{"categories":["linux"],"contents":"常见故障总结 yum 安装nfs 失败原因\nTransaction check error: package nfs-utils-1:1.3.0-0.66.el7_8.x86_64 is already installed Error Summary 解决 rpm -e nfs-utils-1:1.3.0-0.66.el7_8.x86_64 再次安装即可，原因是版本冲突，卸载不需要的版本 ","permalink":"https://xingxing.io/posts/linux/%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C/","tags":["linux"],"title":"常见故障"},{"categories":["kubernetes"],"contents":"etcd 基础知识 1. etcd 功能模块组成 etcd 可分为Client层、API 层、Raft 算法层、逻辑层与存储层 Client 层:\nClient 层包括client v2 与v3 两个大版本API 客户端库，提供了简洁易用的API，同时支持负载均衡、节点故障自动转移，可极大降低业务使用etcd 复杂度，提升开发效率、服务可用性。 API 层\nAPI 网络层主要包括client 访问server 与server 节点之间的通信协议。 节点间通过Raft 算法实现数据赋值和Leader 选举等功能时使用的HTTP协议 Raft 算法层\nRaft 算法层实现了Leader 选举、日志复制、ReadIndex 等核心算法特性，用于保障etcd 多个节点间数据一致性、提升服务可用性等，是etcd 的基石和亮点 功能逻辑层\netcd 核心特性实现层，如典型的KVServer 模块、 MVCC模块、Auth 鉴权模块、Lease租约模块、 Compactor压缩模块等，其中MVCC模块主要由boltdb模块组成。 存储层\n存储层包预写日志(WAL)模块、快照(Snapshot)模块、boltdb模块。其中WAL 可保障etcd crash后数据不丢失，boltdb则保存了集群元数据和用户写入的数据。 2. etcd 读 串行读 当client 发起一个更新hello 为world 请求后，若leader 收到写请求，它会将此请求持久到WAL 日志，并广播给各个节点，若一半以上节点持久化成功，则该请求对应的日志条目被标识为已提交，etcdserver 模块异步从Raft 模块获取已提交的日志条目，应用到状态机(boltdb). 此时若client 发起一个读取hello 的请求，假设此请求从状态机中读取，如果连接到是C节点，若C 节点磁盘I/O 出现波动，可能导致它们已提交的日志 条目很慢，则会出现更新hello 为world的写命令，在client 读hello 的时候还未被提交到状态机，因为就可能读取到旧数据。 由状态机数据返回、无需通过Raft 协议与集群交互的模式，在etcd里叫做串行读，他具有低延迟、高吞吐量的特点，适合对数据一致性要求不高的场景 线性读 当收到一个线性请求时，它首先会从Leader获取集群最新的已提交的日志索引(committed index) Leader 收到ReadIndex 请求时，为防止脑裂等异常场景，会向Flower 节点发送心跳确认，一半以上节点确认Leader 身份证后才能将已提交的索引(committed index)返回给节点C C节点会等待，直到状态机应用索引(applied index)大于Leader 的已提交索引时(committed index)然后去通知读请求，数据已赶上Leader，你可以去状态机中访问数据 Leader选举 当etcd server 收到client 发起的put hello 写请求后，KV 模块会向Raft 模块提交一个put 提案，我们知道只有集群Leader 才能处理写提案，如果此集群中无Leader，整个请求就会超时 首先在Raft 协议中它定义了集群中如下节点状态，任何时刻，每个节点肯定处于其中一个状态： Follower， 跟随者，同步从Leader 收到的日志，etcd 启动的时候默认为此状态： Candidate，竞选者，可以发起Leader 选举 Leader， 集群领导者，唯一性，拥有同步日志的的特权，需定时广播心跳给Follower 节点，以维持领导者身份 当Follwer 节点接收到Leader 节点心跳信息超时后，它会转成Candidate 节点，并发起竞选Leader 投票，若获得集群多数节点的支持后，它就可转变成Leader 节点 ","permalink":"https://xingxing.io/posts/kubernetes/etcd-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","tags":["kubernetes"],"title":"etcd 基础知识4 "},{"categories":["kubernetes"],"contents":"kubernetes 编译 1. 默认mac 编译会报错 /usr/local/go/pkg/tool/linux_amd64/link: signal: killed 通过查看原因，发现是docker 默认在mac 版本上的资源限制问题，解决问题如下\n选择docker 的首选项-设置\n修改完毕，点击应用即可\n2.设置docker代理 根据代理软件设置所对应的代理\n3. 编译 git clone https://github.com/kubernetes/kubernetes cd kubernetes git checkout v1.19.4 ./build/run.sh make 开始执行编译(是在容器中编译) +++ [1207 10:12:19] Placing binaries Generated bindata file : test/e2e/generated/bindata.go has 8978 test/e2e/generated/bindata.go lines of lovely automated artifacts No changes in generated bindata file: staging/src/k8s.io/kubectl/pkg/generated/bindata.go Go version: go version go1.15.2 linux/amd64 +++ [1207 10:12:24] Building go targets for linux/amd64: cmd/kube-proxy cmd/kube-apiserver cmd/kube-controller-manager cmd/kubelet cmd/kubeadm cmd/kube-scheduler vendor/k8s.io/kube-aggregator vendor/k8s.io/apiextensions-apiserver cluster/gce/gci/mounter cmd/kubectl cmd/gendocs cmd/genkubedocs cmd/genman cmd/genyaml cmd/genswaggertypedocs cmd/linkcheck vendor/github.com/onsi/ginkgo/ginkgo test/e2e/e2e.test cluster/images/conformance/go-runner cmd/kubemark vendor/github.com/onsi/ginkgo/ginkgo test/e2e_node/e2e_node.test Env for linux/amd64: GOOS=linux GOARCH=amd64 GOROOT=/usr/local/go CGO_ENABLED= CC= Coverage is disabled. Coverage is disabled. +++ [1207 10:38:11] Placing binaries +++ [1207 10:38:55] Syncing out of container #编译完成的二进制在_output 目录中 ls -l _output/dockerized/bin/linux/amd64 4. 关于证书期限 **** 修改cert.go 文件\nvim staging/src/k8s.io/client-go/util/cert/cert.go ................. # 默认已经是10年，可以不修改，最高99年，不超过100 NotAfter: now.Add(duration365d * 10).UTC(), .......... **** 修改constants.go 文件\nvim cmd/kubeadm/app/constants/constants.go ............. #默认是一年，修改为10年 CertificateValidity = time.Hour * 24 * 365 ............. #修改 CertificateValidity = time.Hour * 24 * 365 * 10 重新编译kubeadm make all WHAT=cmd/kubeadm GOFLAGS=-v 5. 制作pause 镜像 cd build/pause make container staging-k8s.gcr.io/pause-amd64:3.2 #生成镜像 6. 编译完成 1. 生成镜像 staging-k8s.gcr.io/hyperkube-amd64:v1.18.9 # 包含kube-apiserver 等组件信息的镜像 staging-k8s.gcr.io/pause-amd64:3.1 # pause 镜像 ","permalink":"https://xingxing.io/posts/kubernetes/kubernetes-%E7%BC%96%E8%AF%91/","tags":["kubernetes"],"title":"Kubernetes 编译"},{"categories":["kubernetes"],"contents":"RBAC(Role-Based Access Control 基于角色的访问控制) ","permalink":"https://xingxing.io/posts/kubernetes/kubernetes-rbac%E8%AE%A4%E8%AF%81/","tags":["kubernetes"],"title":"Kubernetes RBAC认证"},{"categories":["kubernetes"],"contents":"kubernetes 安全机制 授权策略 API server 的授权策略(通过API server的启动参数 \u0026ldquo;\u0026ndash;authorization-node\u0026rdquo; 设置)\nAlwayDeny 表示拒绝所有请求，一般用于测试 AlwayAllow 允许接收所有请求，kubernetes 默认配置 ABAC(Attribute-Based Access Control): 基于属性的访问控制，定义了一种访问控制的范例，通过使用将属性组合在一起的策略，将访问权限授予用户。策略可以使用任何类型的属性(用户属性，资源属性，对象环境属性等) RBAC: Role-Based Access Control 基于角色的访问控制是一种企业内个人用户的角色来管理对计算机或网络资源的访问的方法。 Node: 一种专用模式，根据计划运行的pod 为kubelet 授予权限 Webhook: 是一个HTTP的回调，发生某些事情时调用的HTTP POST；通过HTTP POST 进行简单的事件通知。实现WebHook 的web 应用程序在发生某些事情时将消息发布到URL。 kubernetes 权限请求过程 kubernetes 使用API 服务器授权API 请求，它根据所有策略评估所有请求属性来决定允许或拒绝请求。一个API请求的所有部分必须被某些策略允许才能继续。默认情况下拒绝权限。 配置多个授权模式时，将按顺序检查每个模块。如果任何授权模块批准或拒绝请求，则立即返回该决定，并且不会与其他授权模块协商。如果所有模块对请求没有意见，则拒绝请求。一个拒绝响应返回HTTP状态代码403。 kubernetes的权限属性 user 身份验证期间提供的user 字符串 group 经过身份验证的用户所属的组名列表 extra 由身份验证层提供的任意字符串键到字符串值的映射 API 指示请求是否针对API 资源 Request path 各种非资源端点的路径， 如/api API request verb API动词 get list create update patch 等 HTTP request verb HTTP动词get post put 和delete 用于非资源请求 Resource 正在访问的资源ID或名称(仅限资源请求) 对于使用get update RBAC权限 RBAC(Role-Based Access Control,基于角色的访问控制)\nRBAC 优势\n对集群中的资源和非资源权限均有完整的覆盖\n整个RBAC 完全由几个API 对象完成，同其他API 对象一样，可以用kubectl 或API进行操作\n可以在运行时进行调整，无须重新启动API Server\n要使用RBAC 授权模式，需要在API Server 的启动参数中加上\u0026ndash;authorization-mode=RBAC\nRBAC 资源对象 User Account: 用户，这是有外部独立服务进行管理的，管理员进行私钥的分配，用户可以使用KeyStone或者Goolge账号，甚至一个用户名和密码的文件列表也可以，对于用户的管理集群内部没有一个关联的资源对象，所以用户不能通过集群内部API来进行管理 Group: 组，这是用来关联多个账号的，集群中有一些默认创建的组，比如cluster-admin Service Account: 服务账号，通过kubernetes API 来管理一些用户账号，和namesapces 进行关联的，适用于集群内部运行的应用程序，需要通过API来完成权限认证，所以在集群内部进行权限操作，我们都需要用到ServiceAccount 创建用户\n创建用户test #创建私钥 openssl genrsa -out test.key 2048 # openssl req -new -key test.key -out test.csr -subj \u0026#34;/CN=test/0=devops\u0026#34; # 生成证书文件 openssl x509 -req -in test.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out test.crt -days 500 #创建凭证 kubectl config set-credentials test --client-certificate=test.crt --client-key=test.key #为用户设置新的context kubectl config set-context test-context --cluster=kubernetes --namespace=kube-system --user=test 使用test 访问 [root@k8s-master-03 Role]# kubectl get pods --context=test-context Error from server (Forbidden): pods is forbidden: User \u0026#34;test\u0026#34; cannot list resource \u0026#34;pods\u0026#34; in API group \u0026#34;\u0026#34; in the namespace \u0026#34;kube-system\u0026#34; 就需要创建角色权限 接下来给test 创建角色\n1) Role(角色) 一个角色就是一组权限的集合，这里的权限都是许可形式的，不存在拒绝的规则。在一个命名空间中，可以用角色来定义一个角色，如果是集群级别的，就需要使用ClusterRole了，\n角色只能对命名空间内的资源进行授权，例如: 已经创建了test账号了，再创建test 的Role\ntest.yaml\nkind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: kube-system name: test rules: - apiGroups: [\u0026#34;\u0026#34;,\u0026#34;app\u0026#34;,\u0026#34;extensions\u0026#34;] #\u0026#34;\u0026#34; 空字符串，表示核心API 群 resources: [\u0026#34;pods\u0026#34;,\u0026#34;deployments\u0026#34;,\u0026#34;replicasets\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;create\u0026#34;,\u0026#34;update\u0026#34;,\u0026#34;delete\u0026#34;] Rules 中的参数说明如下:\napiGroup: 支持的API 组列表\nresources: 支持的资源对象列表，例如 pods、 deployment、jobs 等\nversb: 对资源对象的操作方法列表，例如 get、awtch、list、delete、replace、patch等\n创建role之后，需要test账号与 testRole 进行绑定，就需要创建Rolebinding\n2)Rolebinding(角色绑定) 角色绑定或集群绑定用来把一个角色绑定到一个目标上，绑定目标可以是User(用户)、Group(组)或者Service Account。使用RoleBinding为某个命名空间授权，使用ClusterRoleBinding 为集群范围内授权.\n例子: 下面的例子中的RoleBinding 会将在前面创建的test 用户进行绑定\nkind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: test-rolebinding # rolebinding 名称 namespace: kube-system subjects: - kind: User name: test # name 是不区分大小写的 apiGroup: \u0026#34;\u0026#34; roleRef: # \u0026#34;roleRef\u0026#34; 指定与某Role 或ClusterRole 的绑定关系 kind: Role # 此字段必须是Role 或者CLusterRole name: test # 此字段要必须与你要绑定的Role 或者ClusterRole 名称匹配 apiGroup: ”“ 测试访问，test-context 指定的namespace 是kube-system\n注意: Role与RoleBinding 对象都是namespace 对象，它们对权限的限制规则仅在它们的namespace 内有效，roleRef 也只能引用当前namespace 里的role 对象\n3)ClusterRole(集群角色) 集群角色除了具有和角色一致的命名空间内资源的管理能力，因其集群级别的范围，还可以用于以下特殊元素的授权。\n集群范围的资源，例如node\n非资源型路径，例如 ”/healthz“\n包含全部命名空间的资源，例如pods(kubectl get pods \u0026ndash;all-namespaces这样的操作授权)\n创建xing 用户 openssl genrsa -out xing.key 2048 openssl req -new -key xing.key -out xing.csr -subj \u0026#34;/CN=xing/0=xing\u0026#34; openssl x509 -req -in xing.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out xing.crt -days 500 kubectl config set-credentials xing --client-certificate=xing.crt --client-key=xing.key kubectl config set-context xing-context --cluster=kubernetes --namespace=kube-system --user=xing 创建Clusterrole cat cluster-xing.yaml kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: xing-clusterrole rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;nodes\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;configmaps\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;] 创建ClusterRoleBinding kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: xing-clusterrolebinding subjects: - kind: User name: xing apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: xing-clusterrole apiGroup: rbac.authorization.k8s.io 测试 Service Account 授权管理 默认的RBAC 策略为控制平台组件、节点和控制器授予有限范围的权限，但是出kube-system 外的Service Account 是没有任何权限。\n这就要求用户为Service Account赋予所需的权限。细粒度的角色分配能够提高安全性，但也会提高管理成本。粗放的授权方式可能会给Service Account 多余的权限，但更易于管理。\n首先，需要定义一个Service Account\n创建service account #serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: namespace: kube-system # serviceaccount 创建在了kube-system 下 name: abc # service account 名字 创建cluster-role cluster-role.yaml kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: abc rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;configmaps\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;] 创建ClusterRole-namesapce 编写cluster-server.yaml的yaml 文件，为ServiceAccount 分配权限:\nkind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cr-namespace-abc rules: - apiGroups: - \u0026#34;\u0026#34; resources: - namespaces/status - namespaces verbs: - get - list - watch kubectl apply -f serviceaccount.yaml kubectl apply -f cluster-role.yaml kubectl apply -f cluster-server.yaml 绑定角色 对sa和集群角色建立绑定关系\nkubectl create rolebinding abc --clusterrole=abc --serviceaccount=kube-system:abc --namespace=kube-system #--clusterrole=abc 名字与上面创建cluster-role 名字一致 #--serviceaccount=kube-system:abc 所在的namespace空间与serviceaccount名字 #--namespace=kube-system 授权的namespace 查看sa与 kubectl get sa -n kube-system abc -o yaml #获取secrets 名字 kubectl get secrets abc-token-fpkl2 -o yaml #获取ca与secrets的token token= \u0026lt;token内容\u0026gt; echo $token|base64 -d # 解码 客户端config配置文件 config 文件在家目录的.kube/config\napiVersion: v1 kind: Config clusters: - cluster: server: https://xxxx:6443 certificate-authority-data: #ca认证 name: abc users: - name: \u0026#34;abc\u0026#34; user: token: # 这里的token 需要base64 解码 contexts: - context: cluster: abc user: \u0026#34;abc\u0026#34; name: abc preferences: {} current-context: abc 用户授权 创建用户cluster-sa 的serviceaccount\n#cat sa.yaml apiVersion: v1 kind: ServiceAccount metadata: namespace: kube-system name: cluster-sa 创建role kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: kube-system name: cluster-role # 在这里定义role 的名字 rules: - apiGroups: [\u0026#34;\u0026#34;,\u0026#34;app\u0026#34;,\u0026#34;extensions\u0026#34;] resources: [\u0026#34;pods\u0026#34;,\u0026#34;deployments\u0026#34;,\u0026#34;replicasets\u0026#34;,\u0026#34;configmaps\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;create\u0026#34;,\u0026#34;update\u0026#34;,\u0026#34;delete\u0026#34;] 绑定roleBinding cat role-binding.yaml kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cluster-rolebinding namespace: kube-system subjects: - kind: ServiceAccount name: cluster-sa namespace: kube-system roleRef: kind: Role name: cluster-role apiGroup: rbac.authorization.k8s.io 客户端配置与验证 kubectl get sa -n kube-system cluster-sa -o yaml #获取secrets kubectl get secrets -n kube-system cluster-sa-token-wxlwn -o yaml token=***** echo $token|base64 -d config apiVersion: v1 kind: Config clusters: - cluster: server: https://xxxxx:6443 certificate-authority-data: # ca证书文件 name: cluster-sa users: - name: \u0026#34;cluster-sa\u0026#34; user: token: #解压后的token contexts: - context: cluster: cluster-sa user: \u0026#34;cluster-sa\u0026#34; name: cluster-sa preferences: {} current-context: cluster-sa ","permalink":"https://xingxing.io/posts/kubernetes/kubernetes-%E5%AE%89%E5%85%A8%E6%9C%BA%E5%88%B6/","tags":["kubernetes"],"title":"kubernetes 安全机制"},{"categories":["devops"],"contents":"maven npm 等jenkins 工具的集成 CI 工具构建\n安装maven https://maven.apache.org/ maven 解压之后 vim /etc/profile export M2_HOME=/opt/apache-maven-3.6.3 export PATH=${M2_HOME}/bin:$PATH source /etc/profile mvn -v jenkins—全局工具配置\n就可以使用mvn构建了\nCD 工具\n","permalink":"https://xingxing.io/posts/devops/jenkins-%E5%B7%A5%E5%85%B7%E9%9B%86%E6%88%90/","tags":["devops"],"title":"“ jenkins-工具集成 ”"},{"categories":["devops"],"contents":"jenkins 忘记密码 找到config.xml 文件并且备份\nfind / -name config.xml\n/var/lib/jenkins/config.xml cp /var/lib/jenkins/config.xml /var/lib/jenkins/config.xml.bak vim /var/lib/jenkins/config.xml\n删除以下安全模块\n\u0026lt;useSecurity\u0026gt;true\u0026lt;/useSecurity\u0026gt; \u0026lt;authorizationStrategy class=\u0026#34;hudson.security.FullControlOnceLoggedInAuthorizationStrategy\u0026#34;\u0026gt; \u0026lt;denyAnonymousReadAccess\u0026gt;true\u0026lt;/denyAnonymousReadAccess\u0026gt; \u0026lt;/authorizationStrategy\u0026gt; \u0026lt;securityRealm class=\u0026#34;hudson.security.HudsonPrivateSecurityRealm\u0026#34;\u0026gt; \u0026lt;disableSignup\u0026gt;true\u0026lt;/disableSignup\u0026gt; \u0026lt;enableCaptcha\u0026gt;false\u0026lt;/enableCaptcha\u0026gt; \u0026lt;/securityRealm\u0026gt; 重启systemctl restart jenkins\n登录jenins 选择系统管理，打开全局安全设置\n​ 选择jenkins 专有用户数据库，点击保存\n返回界面，选择管理用户，就会看到用户列表，在admin 右边点击进去 这样就会在页面看到登录，点击登录\n即可登录jenkins\n","permalink":"https://xingxing.io/posts/devops/jenkins-%E5%BF%98%E8%AE%B0%E5%AF%86%E7%A0%81/","tags":["devops"],"title":"Jenkins 忘记密码"},{"categories":["devops"],"contents":"nexus 解决node-saas 依赖慢的问题 node-sass 问题的解决 在nexus 上创建npm私库\n创建一个本地的repository\n允许上传\n创建proxy repository\n创建group\n当我们设置这些之后，还会发现node 程序在构建的，如果没有设置淘宝源的话还是会去淘宝或者github 上去下载文件\nDownloading binary from https://npm.taobao.org/mirrors/node-sass/v4.14.1/linux-x64-64_binding.node Download complete 每次下载从外网去下载会比较麻烦，也无法进行缓存，导致了构建缓慢，在nexus 如何解决呢\n在nexus 中创建一个repository 类型选为raw(proxy)\n注意，这里的proxy 的代理地址是淘宝源的node-saas 地址 https://npm.taobao.org/mirrors/node-sass/ 在构建的项目的时候填写如下地址就好\nnpm config set registry=http://nexus.XXX.com/repository/npm/ npm config set sass-binary-site http://nexus.XXX.cn/repository/node-saas/ 在去构建的时候，会发现nexus 会缓存这些包\nphantomjs 问题的解决","permalink":"https://xingxing.io/posts/devops/nexus-%E8%A7%A3%E5%86%B3node-saas%E6%85%A2%E7%AD%89%E9%97%AE%E9%A2%98/","tags":["devops"],"title":"Nexus 解决node Saas慢的问题"},{"categories":["devops"],"contents":"Jenkins 使用docker 安装 jenkins 容器安装\ndocker run --name jenkins -itd \\ -p 8081:8080 \\ -p 50000:50000 \\ -v ~/jenkins:/var/jenkins_home \\ jenkins/jenkins:lts docker logs -f jenkins #查看日志并获取初始化密码 ","permalink":"https://xingxing.io/posts/devops/jenkins-docker/","tags":["devops"],"title":"Jenkins Docker"},{"categories":["devops"],"contents":"Jenkinsfile Jenkinsfile 组成\n指定node节点/workspace 指定运行选项 指定运行选项 指定stages阶段 指定构建后操作 #指定node 节点/workspace\n#agent\npipeline { agent { node { label \u0026#34;master\u0026#34; // 指定运行节点的标签或者名称 customWorkspace \u0026#34;${workspace}\u0026#34; //指定运行工作目录(可选) } } options{ timestamps() // 日志会有时间 skipDefaultCheckout() //删除模式checkout scm 语句 disableConcurrentBuilds() // 禁止并行 timeout(time: 1,uint: \u0026#39;HOURS\u0026#39;) //流水线超时设置1h } #Pipeline 定义-stage\n指定stages 阶段(一个或多个) 模板添加了三个阶段 * GetCode * Build * CodeScan stage { //下载代码 stage (\u0026#34;GetCode\u0026#34;) { // 阶段名称 steps { //步骤 timeout(time:5,uint: \u0026#34;MINUTES\u0026#34;) { //步骤超时时间 script{ // 填写运行代码 println(‘获取代码’) } } } } } //构建 stage(\u0026#34;Build\u0026#34;) { steps{ timeout(time:20,uint:\u0026#34;MINUTES\u0026#34;){ script{ println(\u0026#39;应用打包\u0026#39;) } } } } //代码扫描 stage(\u0026#34;CodeScan\u0026#34;){ steps{ timeout(time:30, unit:\u0026#34;MINUTES\u0026#34;){ script{ print(\u0026#34;代码扫描\u0026#34;) } } } } #Pipeline 定义-post\n指定构建后操作\n解释:\nalways{}: 总是执行脚本片段 success{}: 成功后执行 failure{}: 失败后执行 aborted{}: 取消后执行 currendBuild 是一个全局变量 description 构建描述 //构建后操作 post{ always { script{ println(\u0026#34;always\u0026#34;) } } } success{ script{ currentBuild.description += \u0026#34;\\n 构建成功!\u0026#34; } } failure { script{ currentBuild.description += \u0026#34;\\n构建失败\u0026#34; } } aborted { script { currentBuild.description +=\u0026#34;\\n 构建取消\u0026#34; } } Pipeline 语法-agent\nagent (代理) agent 指定了流水线的执行节点 参数: any 在任何可用的节点上执行pipeline node 没有指定agent的时候默认 label 指定标签上的节点运行Pipeline node 允许额外的选项 这两种是一样的 agent {node {label \u0026#39;labelname\u0026#39;}} agent { label \u0026#39;labelname\u0026#39;} post 定义一个或多个steps,这些阶段根据流水线的完成情况而运行(取决于post部分的位置)，post支持以下post-condition块中其中之一；always,changed,failure,success,unstable和aborted. 这些条件块允许在post 部分的步骤执行取决于流水线或阶段的完成状态 always 无论流水线或者阶段的完成状态 changed 只有当流水线或者阶段完成状态与之前不同时 failure 只有当流水线或者阶段状态为\u0026#34;failure\u0026#34;运行 success 只有当流水线或者阶段状态为\u0026#34;success\u0026#34;运行 unstable 只有当流水线或者阶段状态为\u0026#34;unstable\u0026#34;运行，如测试失败 aborted 只有当流水线或者阶段状态为\u0026#34;aborted\u0026#34;运行， 手动取消 stages(阶段) 包含一系列一个或多个stage指令，建议stages至少包含一个stage 指定用于连续交付过程中的每个离散部分，比如构建 测试 和部署 pipeline { agent any stages { stage(\u0026#39;Hello\u0026#39;) { steps { echo \u0026#39;Hello World\u0026#39; } } } } steps (步骤) step 是每个阶段中要执行的每个步骤 pipeline { agent any stages { stage(\u0026#39;Example\u0026#39;) { steps { echo \u0026#39;Hello World\u0026#39; } } } } 参数\ntrigger 触发器\ntriggers { cron ( \u0026#39;H */4 * * 1-5 \u0026#39;)} tool\n获取通过自动安装或手动放置工具的环境变量。支持maven/jdk/gradle，工具的名称必须在系统设置-全局工具配置中定义\nwhen\nwhen 指令允许流水线根据给定的条件决定是否应该执行阶段。 when指令必须包含至少一个条件。如果when 指令包含多个条件，所有的条件必须返回true，阶段才能执行\nwhen {branch \u0026#39;master\u0026#39; } script\nscript 步骤需要[scripted-pipline ] 块并在声明式中执行。对于大多数用例来说，应该声明式流水线中的\u0026#34;脚本\u0026#34;步骤是不必要的，但是它可以提供一个有用的\u0026#34;逃生出口\u0026#34;，非平凡的规模和/或复杂性的script 块应该被转移到共享库 ","permalink":"https://xingxing.io/posts/devops/jenkins-%E8%AF%AD%E6%B3%95/","tags":["devops"],"title":"Jenkins 语法"},{"categories":["devops"],"contents":"Jenkins 是一个自包含的开源自动化服务器，可用于自动化与构建，测试以及交付或部署软件有关的各种任务。 安装 apt-get install openjdk-8-jre wget -q -O - https://pkg.jenkins.io/debian/jenkins.io.key | sudo apt-key add - sudo sh -c \u0026#39;echo deb https://pkg.jenkins.io/debian binary/ \u0026gt; \\ /etc/apt/sources.list.d/jenkins.list\u0026#39; sudo apt-get update sudo apt-get install jenkin 配置jenkins 选择自定义插件，先不选择插件进行安装 进入jenkins 修改源为中文社区的源 系统管理—插件管理—高级\nhttps://jenkins-zh.gitee.io/update-center-mirror/tsinghua/update-center.json ​ 修改完毕，点击提交即可\n安装插件 搜索插件pipline\n添加slave 节点 将会看到如下界面\n下载agent.jar 到对应的slave 节点，将mv agent.jar /opt/jenkins/ 目录下 创建/opt/jenkins 目录 # 注意 slave 节点需要安装jdk环境 使用脚本启动 cat start.sh #!/bin/bash nohup java -jar agent.jar -jnlpUrl http://10.39.12.9:8080/computer/node/slave-agent.jnlp -secret 58c3f623072f0cb0cf6022974a1c07d7bfec690479bbfdd9b38f5244cb0cbb62 -workDir \u0026#34;/opt/jenkins\u0026#34; \u0026amp; slave 节点添加完成 ​\n","permalink":"https://xingxing.io/posts/devops/jenkins-%E4%BA%91%E4%B8%BB%E6%9C%BA%E9%83%A8%E7%BD%B2/","tags":["devops"],"title":"Jenkins 部署"},{"categories":["kubernetes"],"contents":"rancher 的基本使用 创建项目\n看到选择，选择集群名称\n​\n​ ​\n查看创建的项目\ndemo-test 项目创建应用\n​ 创建好的应用如何访问\n除了使用nodePort 方式之后，可以使用nginx 来访问或者haproxy\n","permalink":"https://xingxing.io/posts/kubernetes/rancher-%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","tags":["kubernetes"],"title":"Rancher-基本使用"},{"categories":["kubernetes"],"contents":"rancher 配置LDAP 认证 rancher 配置LDAP 认证\n安全—认证—LDAP 配置相关信息即可\n登录的时候选择AD认证\n","permalink":"https://xingxing.io/posts/kubernetes/rancher-%E9%85%8D%E7%BD%AEldap%E8%AE%A4%E8%AF%81/","tags":["kubernetes"],"title":"Rancher 配置LDAP认证"},{"categories":["kubernetes"],"contents":"面试题 1. pod 与pod 之间是如何通信的 1.1 同一个node 上pod容器之间的通信 Pod1与Pod2都是通过veth连接到同一个docker0 网桥上，他们的IP地址IP1 与IP2 都是从docker0 的网段上获取的，他们和网桥本身的IP3 是同一个网段 1.2 docker 中的namespace 与cgroup namespaces\ndocker 通过namespace实现资源隔离，通过cgroup 实现资源限制，通过写时复制技术(copy-on-write)实现了文件操作 命名空间(namespace)是linux为我们提供的用于分离进程树、网络接口、挂载点以及进程间通信等资源的方法。 当我们运行(docker run 或者docker start)一个docker 容器时，docker 会设置一系列的namespaces，这些namespaces提供了一层隔离，容器的各个方面都在单独的namespaces中运行，并且对其的访问权限仅限该namespaces docker 在linux 上使用以下几个命名空间: pid namespaces 用于进程隔离 net namespaces 管理网络接口 ipc namespaces 管理对IPC资源的访问(IPC: 进程间通信(信号量、消息队列和共享内存)) mnt namespaces 管理文件系统挂载点 ust namespaces 隔离主机名和域名 user namespaces 隔离用户和用户组 CGgoup\nlinux Cgroup 可让你为系统中所运行任务(进程)的用户定义组 ","permalink":"https://xingxing.io/posts/kubernetes/%E9%9D%A2%E8%AF%95%E9%A2%98/","tags":["kubernetes"],"title":"面试题"},{"categories":["kubernetes"],"contents":"使用rke 部署rancher 集群 安装docker\ncurl https://releases.rancher.com/install-docker/18.09.sh | sh 安装kubernetes 命令行工具kubectl\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.18.0/bin/linux/amd64/kubectl chmod +x ./kubectl mv ./kubectl /usr/local/bin/kubectl kubectl version --client 安装RKE\nmv rke_linux-amd64 rke chmod +x rke rke --version 使用RKE 安装需要生成一个cluster.yml 文件可以使用rke 的命令来生成\nrke config 来生成这个文件\nrke config [+] Cluster Level SSH Private Key Path [~/.ssh/id_rsa]: [+] Number of Hosts [1]: 3 [+] SSH Address of host (1) [none]: 10.39.15.23 [+] SSH Port of host (1) [22]: [+] SSH Private Key Path of host (10.39.15.23) [none]: [-] You have entered empty SSH key path, trying fetch from SSH key parameter [+] SSH Private Key of host (10.39.15.23) [none]: [-] You have entered empty SSH key, defaulting to cluster level SSH key: ~/.ssh/id_rsa [+] SSH User of host (10.39.15.23) [ubuntu]: root [+] Is host (10.39.15.23) a Control Plane host (y/n)? [y]: y [+] Is host (10.39.15.23) a Worker host (y/n)? [n]: y [+] Is host (10.39.15.23) an etcd host (y/n)? [n]: y [+] Override Hostname of host (10.39.15.23) [none]: [+] Internal IP of host (10.39.15.23) [none]: [+] Docker socket path on host (10.39.15.23) [/var/run/docker.sock]: [+] SSH Address of host (2) [none]: 10.39.15.24 [+] SSH Port of host (2) [22]: [+] SSH Private Key Path of host (10.39.15.24) [none]: [-] You have entered empty SSH key path, trying fetch from SSH key parameter [+] SSH Private Key of host (10.39.15.24) [none]: [-] You have entered empty SSH key, defaulting to cluster level SSH key: ~/.ssh/id_rsa [+] SSH User of host (10.39.15.24) [ubuntu]: root [+] Is host (10.39.15.24) a Control Plane host (y/n)? [y]: y [+] Is host (10.39.15.24) a Worker host (y/n)? [n]: y [+] Is host (10.39.15.24) an etcd host (y/n)? [n]: y [+] Override Hostname of host (10.39.15.24) [none]: [+] Internal IP of host (10.39.15.24) [none]: [+] Docker socket path on host (10.39.15.24) [/var/run/docker.sock]: [+] SSH Address of host (3) [none]: 10.39.15.25 [+] SSH Port of host (3) [22]: [+] SSH Private Key Path of host (10.39.15.25) [none]: [-] You have entered empty SSH key path, trying fetch from SSH key parameter [+] SSH Private Key of host (10.39.15.25) [none]: [-] You have entered empty SSH key, defaulting to cluster level SSH key: ~/.ssh/id_rsa [+] SSH User of host (10.39.15.25) [ubuntu]: root [+] Is host (10.39.15.25) a Control Plane host (y/n)? [y]: y [+] Is host (10.39.15.25) a Worker host (y/n)? [n]: y [+] Is host (10.39.15.25) an etcd host (y/n)? [n]: y [+] Override Hostname of host (10.39.15.25) [none]: [+] Internal IP of host (10.39.15.25) [none]: [+] Docker socket path on host (10.39.15.25) [/var/run/docker.sock]: [+] Network Plugin Type (flannel, calico, weave, canal, aci) [canal]: calico [+] Authentication Strategy [x509]: [+] Authorization Mode (rbac, none) [rbac]: none [+] Kubernetes Docker image [rancher/hyperkube:v1.20.5-rancher1]: [+] Cluster domain [cluster.local]: [+] Service Cluster IP Range [10.43.0.0/16]: 192.168.0.0/16 [+] Enable PodSecurityPolicy [n]: [+] Cluster Network CIDR [10.42.0.0/16]: 172.16.0.0/16 [+] Cluster DNS Service IP [10.43.0.10]: [+] Add addon manifest URLs or YAML files [no]: cluster.yml 生成之后就可以执行\nrke up #来部署kubernetes 集群 export KUBECONFIG=$(pwd)/kube_config_cluster.yml #rke up 执行完毕使用这个命令 #将kube_config_cluster.yml 文件拷贝到$HOME/.kube/config [root@i-3lhxok9k ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION 10.39.12.2 Ready controlplane,etcd,worker 17h v1.18.6 10.39.12.3 Ready controlplane,etcd,worker 17h v1.18.6 10.39.12.7 Ready controlplane,etcd,worker 17h v1.18.6 安装rancher\nrancher 的安装需要helm\n#添加仓库 helm repo add rancher-stable https://releases.rancher.com/server-charts/stable 为rancher 创建Namespace\nkubectl create namespace cattle-system 选择已有证书安装rancher\n1. helm install rancher rancher-stable/rancher --namespace cattle-system --set hostname=rancher-test.enncloud.cn --set ingress.tls.source=secret 添加已有证书\nkubectl -n cattle-system create secret tls tls-rancher-ingress --cert=/opt/enneloud.cer --key=/opt/privateKey.key 第一次登录的时候需要设置一下密码\n部署过程会遇到一些错误，可以清楚之后，重新安装\nsudo df -h|grep kubelet |awk -F % \u0026#39;{print $2}\u0026#39; |xargs umount sudo rm /var/lib/kubelet/* -rf sudo rm /etc/kubernetes/* -rf sudo rm /etc/cni/* -rf sudo rm /var/lib/rancher/* -rf sudo rm /var/lib/etcd/* -rf sudo rm /var/lib/cni/* -rf sudo rm /opt/cni/* -rf sudo ip link del flannel.1 sudo ip link del cni0 sudo iptables -F \u0026amp;\u0026amp; iptables -t nat -F sudo docker ps -a|awk \u0026#39;{print $1}\u0026#39; |xargs docker rm -f sudo docker volume ls|awk \u0026#39;{print $2}\u0026#39; |xargs docker volume rm sudo systemctl restart docker ","permalink":"https://xingxing.io/posts/kubernetes/rancher_rke%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/","tags":["kubernetes"],"title":"Rancher_RKE集群部署"},{"categories":["linux"],"contents":"使用lvm 逻辑卷 yum install lvm2 创建pv\n[root@prometheus-tke ~]# pvcreate /dev/vdd WARNING: xfs signature detected on /dev/vdd at offset 0. Wipe it? [y/n]: y Wiping xfs signature on /dev/vdd. Physical volume \u0026#34;/dev/vdd\u0026#34; successfully created. [root@prometheus-tke ~]# pvcreate /dev/vde WARNING: xfs signature detected on /dev/vde at offset 0. Wipe it? [y/n]: y Wiping xfs signature on /dev/vde. Physical volume \u0026#34;/dev/vde\u0026#34; successfully created. [root@prometheus-tke ~]# vgcreate vgmoint /dev/vdd /dev/vde Volume group \u0026#34;vgmoint\u0026#34; successfully created You have mail in /var/spool/mail/root [root@prometheus-tke ~]# lvcreate -l 100%FREE vgmoint Logical volume \u0026#34;lvol0\u0026#34; created. You have mail in /var/spool/mail/root 挂载\nmount /dev/vgmoint/lvol0 /var/lib/monitoring 查看UUID blkid 将UUID 写入到/etc/fstab 中即可 ","permalink":"https://xingxing.io/posts/linux/lvm%E9%80%BB%E8%BE%91%E5%8D%B7/","tags":["linux"],"title":"Lvm逻辑卷"},{"categories":["kubernetes"],"contents":"Pod 健康检查与服务可用性检查 kubernetes 对Pod的健康状态可以通过两类探针来检查: LivenessProbe 和ReadinessProbe，kubelet定期执行者两类探针来诊断容器的监控状况\n(1) livenessProbe 探针: 用于判断容器是否存活(Running 状态)，如果LivenessProbe 探针探测到容器不健康，则kubelet 将杀掉该容器，并根据容器的重启策略做响应的处理。如果一个容器不包含livenessProbe探针，那么kubelet 认为该容器的LivenessProbe 探针返回值永远是Success\n(2) ReadinessProbe 探针: 用于判断容器服务是否可用(Ready状态)，达到Ready 状态的Pod才可以接收请求。对于被Service 管理的Pod，Service 与Pod Endpoint 的关联关系也将基于Pod是否Ready 进行设置。如果在运行过程中Ready 状态变为False，则系统自动将其从Service的后端Endpoint列表中隔离出去，后续再把恢复到Ready状态的Pod加回到后端的Endpoint列表。这样就能保证客户端在访问Service时不会被转发服务不可用的Pod 实例上。\n","permalink":"https://xingxing.io/posts/kubernetes/%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5%E4%B8%8E%E6%9C%8D%E5%8A%A1%E5%8F%AF%E7%94%A8%E6%80%A7%E6%A3%80%E6%9F%A5/","tags":["kubernetes"],"title":"Pod健康检查与服务可用性检查"},{"categories":["kubernetes"],"contents":"Pod 的重启策略 Pod 在整个生命周期中被系统定义为各种状态，熟悉Pod的各种状态对于理解如何设置Pod调度策略。重启策略是有很有必要的\n状态值 描述 Pending API server已经创建该Pod，但是Pod内还有一个或多个容器的镜像没有创建，包括正在下载镜像的过程 Running Pod 内所有容器均已创建，且至少有一个容器处于运行状态、正在启动状态或正在重启状态 Succeeded Pod 内所有容器均成功执行后退出，且不会再重启 Failed Pod 内所有容器均已退出，但至少有一个容器退出为失败状态 Unknown 由于某种原因无法获取该Pod的状态，可能由于网络通信不畅 Pod 的重启策略\nAlways: 当容器失效时，由kubelet自动重启该容器\nOnfailure: 当容器终止运行且退出码不为0时，由kubelet 自动重启该容器\nNever: 不论容器运行状态如何，kubelet 都不会重启该容器\nkubelet 重启失效容器的时间间隔已sync-frequency 乘以2n 来计算，例如 1、2、4、8倍等，最长延时5分钟，并且在成功重启后10min后重置该时间\nPod的重启策略与控制方式息息相关，当前可用于管理Pod 的控制器包括ReplicationController、Job DaemonSet 及直接通过kubelet 管理(静态Pod)，每种控制对Pod 的重启策略要求如下:\nRC与DaemonSet ：必须设置为Always\nJob: OnFailure 或Never，确保容器执行完成后不再重启\nkubelet: 在Pod 失效时自动重启，不论将RestartPolicy 设置什么值，也不会对Pod 进行健康检查\n","permalink":"https://xingxing.io/posts/kubernetes/pod%E9%87%8D%E5%90%AF%E7%AD%96%E7%95%A5/","tags":["kubernetes"],"title":"Pod重启策略"},{"categories":["golang"],"contents":"go 数组与切片 1. 数组 数组就是指一系列同一类型的数据集合.数组中包含的每个数据被称为数组元素，一个数组包含的元素被称为数组的长度\n数组长度必须是常量，且是类型的组成部分。\n定义数组的格式: var [n] , n\u0026gt;=0\n数组元素使用操作符[]来索引，索引从0开始。因此一个数组的首元素是array[0],其最后一个元素是array[len(array)-1]\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { // 定义一个数组,[10]int 和[5]int是不同类型 //[数字]这个数字作为数组元素的个数 var a [10]int var b [5]int fmt.Printf(\u0026#34;len(a)=%d,len(b)=%d\\n\u0026#34;, len(a), len(b)) } package main import ( \u0026#34;fmt\u0026#34; ) func main() { // 定义一个数组,[10]int 和[5]int是不同类型 //[数字]这个数字作为数组元素的个数 var a [10]int var b [5]int fmt.Printf(\u0026#34;len(a)=%d,len(b)=%d\\n\u0026#34;, len(a), len(b)) //赋值 for i := 0; i \u0026lt; len(a); i++ { a[i] = i + 1 } //打印 //第一个返回下标，第二个返回元素 for i, data := range a { fmt.Printf(\u0026#34;a[%d]=%d\\n\u0026#34;, i, data) } } 1.1 数组初始化 1. 声明定义同时赋值,叫初始化 1. 全局初始化 var a[5]int =[5] int {1,2,3,4,5} 2. 简洁写法 b := [5]int{1,2,3,4,5} 3. 部分初始化，没有初始化的元素，int自动复制为0 c :=[5]int{1,2,3} 4. 指定某个元素初始化 d :=[5]int{2:10,4:20} 实例\npackage main import \u0026#34;fmt\u0026#34; func main() { var a [5]int = [5]int{1, 2, 3, 4, 5} fmt.Println(\u0026#34;a=\u0026#34;, a) //1. 全局省略写法 b := [5]int{1, 2, 3, 4, 5} fmt.Println(\u0026#34;b=\u0026#34;, b) c := [5]int{1, 2, 3} fmt.Println(\u0026#34;c=\u0026#34;, c) } 二维数组\n比较\npackage main import \u0026#34;fmt\u0026#34; func main() { // 支持比较，比较2个数组类型要一致，只支持==和 != a := [5]int{1, 2, 3, 4, 5} b := [5]int{1, 2, 3, 4, 5} c := [5]int{1, 2, 3} fmt.Println(\u0026#34;a==b\u0026#34;, a == b) fmt.Println(\u0026#34;a==c\u0026#34;, a == c) //同类型的数组可以赋值 var d [5]int d = a fmt.Println(\u0026#34;d=\u0026#34;, d) } 2. 切片 切片(slice)概述\n数组的长度在定义之后无法修改，数组是值类型，每次传递都将产生一份副本，显然这种数据结构无法满足开发者的真实需求。Go语言提供了数组切片(Slice)来弥补数组的不足。\n2.1 切片的创建语法 make([]type,length,capacity) make([]type,length) []type{} []type{value1,value2...,valueN} 语法 含义/结果 s[n] 切片s中索引位置为n 的项 s[n:m] 从切片s的索引位置n到m-1处所获得的切片 s[n:] 从切片s的索引位置n到len(s)-1处所获得的切片 s[:m] 从切片s到索引位置0到m-1处获得的切片 s[:] 从切片s 的索引位置0到len(s)-1 处所获得的切片 cap(s) 从切片s的容量，总是len\u0026gt;=len(s) Len(s) 切皮s中所包含项的个数，总是\u0026lt;= cap(s) s[:cap(s)] 增加切片s 的长度到其容量，如果两者不同的话 切片并不是数组或指针，他通过内部指针和相关的属性引用数组片段，以实现比如:\narr := [...]int{0,1,2,3,4,5} s := arr[2:6] package main import \u0026#34;fmt\u0026#34; func main() { array := []int{0, 1, 2, 3, 4, 5, 6, 7, 8, 9} s1 := array[:] fmt.Println(\u0026#34;s1=\u0026#34;, s1) fmt.Printf(\u0026#34;len=%d,cap=%d\\n\u0026#34;, len(s1), cap(s1)) } 切片与隐藏数组的关系\npackage main import \u0026#34;fmt\u0026#34; func main(){ s := []string {\u0026#34;A\u0026#34;,\u0026#34;B\u0026#34;,\u0026#34;C\u0026#34;,\u0026#34;D\u0026#34;,\u0026#34;E\u0026#34;,\u0026#34;F\u0026#34;,\u0026#34;G\u0026#34;} t := s[:5] //A,B,C,D,E u := s[3:len(s)-1] //D,E,F fmt.Println(s,t,u) u[1] = \u0026#34;X\u0026#34; fmt.Println(s,t,u) } 结果 [A B C D E F G] [A B C D E] [D E F] [A B C D X F G] [A B C D X] [D X F] 分析 s[0],s[1],s[2],s[3],s[4],s[5],s[6]=\u0026#34;A\u0026#34;,\u0026#34;B\u0026#34;,\u0026#34;C\u0026#34;,\u0026#34;D\u0026#34;,\u0026#34;E\u0026#34;,\u0026#34;F\u0026#34;,\u0026#34;G\u0026#34; 由于切片s、t、u 都是同一底层数组的引用，其中一个改变会影响到其他所有指向该相同数组的任何其他引用 2.2 索引与分割切片 package main import \u0026#34;fmt\u0026#34; func main(){ s := []string {\u0026#34;A\u0026#34;,\u0026#34;B\u0026#34;,\u0026#34;C\u0026#34;,\u0026#34;D\u0026#34;,\u0026#34;E\u0026#34;,\u0026#34;F\u0026#34;,\u0026#34;G\u0026#34;} t := s[2:6] fmt.Println(t,s,\u0026#34;=\u0026#34;,s[:4],\u0026#34;+\u0026#34;,s[4:]) s[3] = \u0026#34;x\u0026#34; t[len(t)-1] = \u0026#34;y\u0026#34; fmt.Println(t,s,\u0026#34;=\u0026#34;,s[:4],\u0026#34;+\u0026#34;,s[4:]) } #对于一个切片s 和一个索引值i(0\u0026lt;=i\u0026lt;=len(s)),s等于s[:1]与s[i:]的连接。 s ==s[:i] + s[i:] //s 是一个字符串，i 是整数，0\u0026lt;=i \u0026lt;= len(s) 我们可以改变数据，无论是通过原始切片s 还是通过切片s 的切片t，它们底层数据都改变了，因此两个切片都受影响。\n2.3 遍历切片 遍历一个切片中的元素。如果我们想要取得切片中的某个元素而不想修改它，可以使用for \u0026hellip;\u0026hellip;range 循环\npackage main import \u0026#34;fmt\u0026#34; func main(){ s := []string {\u0026#34;A\u0026#34;,\u0026#34;B\u0026#34;,\u0026#34;C\u0026#34;,\u0026#34;D\u0026#34;,\u0026#34;E\u0026#34;,\u0026#34;F\u0026#34;,\u0026#34;G\u0026#34;} for _, v := range s { fmt.Println(v) } } # for ....range 循环首先初始化一个从0开始的循环计数器 2.4 修改切片 package main import \u0026#34;fmt\u0026#34; func main(){ s :=[]string{\u0026#34;A\u0026#34;,\u0026#34;B\u0026#34;,\u0026#34;C\u0026#34;,\u0026#34;D\u0026#34;,\u0026#34;E\u0026#34;,\u0026#34;F\u0026#34;,\u0026#34;G\u0026#34;} t :=[]string{\u0026#34;K\u0026#34;,\u0026#34;L\u0026#34;,\u0026#34;M\u0026#34;,\u0026#34;N\u0026#34;} u :=[]string{\u0026#34;m\u0026#34;,\u0026#34;n\u0026#34;,\u0026#34;o\u0026#34;,\u0026#34;p\u0026#34;,\u0026#34;q\u0026#34;,\u0026#34;r\u0026#34;} s =append(s,\u0026#34;h\u0026#34;,\u0026#34;i\u0026#34;,\u0026#34;j\u0026#34;) s =append(s,t...) s =append(s,u[2:5]...) b :=[]byte{\u0026#39;U\u0026#39;,\u0026#39;V\u0026#39;} letters := \u0026#34;WXY\u0026#34; b = append(b,letters...) fmt.Printf(\u0026#34;%v\\n %s\\n\u0026#34;,s,b) } 2.5 删除切片 package main import \u0026#34;fmt\u0026#34; func main(){ s :=[]string{\u0026#34;A\u0026#34;,\u0026#34;B\u0026#34;,\u0026#34;C\u0026#34;,\u0026#34;D\u0026#34;,\u0026#34;E\u0026#34;,\u0026#34;F\u0026#34;,\u0026#34;G\u0026#34;} s =s[2:] // 从开始处删除s[:2]子切片 fmt.Println(s) } ","permalink":"https://xingxing.io/posts/golang/go%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%95%B0%E7%BB%84%E4%B8%8E%E5%88%87%E7%89%87/","tags":["golang"],"title":"Go学习笔记 数组与切片"},{"categories":["golang"],"contents":"Go流程控制 *我们经常需要代码在满足一定条件时进行执行，或者需要重复执行代码多次，此时需要选择条件语句(if-else)或选择语句(switch case)及循环语句(for)\n一. if 的语法 方式一: if condition1 { } 方式二: if condition1 { }else { } 方式三: if condition1{ }else if condition2 { }else if condition3{ }else { } package main import \u0026#34;fmt\u0026#34; func main() { //老婆的想法 fmt.Println(\u0026#34;老婆的想法\u0026#34;) fmt.Println(\u0026#34;买十个包子\u0026#34;) var yes string fmt.Println(\u0026#34;有卖西瓜的吗?\u0026#34;) fmt.Scan(\u0026amp;yes) if yes == \u0026#34;Y\u0026#34; || yes == \u0026#34;y\u0026#34; { fmt.Println(\u0026#34;买一个西瓜\u0026#34;) } fmt.Println(\u0026#34;老公的想法\u0026#34;) if yes == \u0026#34;Y\u0026#34; || yes == \u0026#34;y\u0026#34; { fmt.Println(\u0026#34;买一个包子\u0026#34;) } else { fmt.Println(\u0026#34;买十个包子\u0026#34;) } else-if package main import \u0026#34;fmt\u0026#34; func main() { var score int fmt.Print(\u0026#34;请输入成绩:\u0026#34;) fmt.Scan(\u0026amp;score) if score \u0026gt;= 90 { fmt.Println(\u0026#34;A\u0026#34;) } else { if score \u0026gt;= 80 { fmt.Println(\u0026#34;B\u0026#34;) } else { if score \u0026lt;= 70 { fmt.Println(\u0026#34;C\u0026#34;) } } } 二. switch package main import \u0026#34;fmt\u0026#34; func main(){ var yes string fmt.Print(\u0026#34;老婆的想法:\u0026#34;) fmt.Print(\u0026#34;有卖西瓜的吗?(Y/N):\u0026#34;) fmt.Scan(\u0026amp;yes) fmt.Print(\u0026#34;老婆的想法：\u0026#34;) fmt.Print(\u0026#34;十个包子\u0026#34;) switch yes { case \u0026#34;y\u0026#34;,\u0026#34;Y\u0026#34;: fmt.Println(\u0026#34;一个西瓜\u0026#34;) } fmt.Println(\u0026#34;老公的想法:\u0026#34;) switch yes { case \u0026#34;y\u0026#34;,\u0026#34;Y\u0026#34;: fmt.Println(\u0026#34;一个包子\u0026#34;) default: fmt.Println(\u0026#34;十个包子\u0026#34;) } fmt.Print(\u0026#34;请输入你的成绩:\u0026#34;) var score int fmt.Scan(\u0026amp;score) switch { case score \u0026gt;= 90: fmt.Println(\u0026#34;A\u0026#34;) case score \u0026gt;= 80: fmt.Println(\u0026#34;C\u0026#34;) case score \u0026lt;=60: fmt.Println(\u0026#34;D\u0026#34;) } } 三.for package main import \u0026#34;fmt\u0026#34; func main(){ result :=0 //初始化子语句，条件子语句， 后置子语句 for i :=1;i\u0026lt;=100;i++{ result += i } fmt.Println(result) } 3.1 continue 与break #continue package main import \u0026#34;fmt\u0026#34; func main() { for i := 0; i \u0026lt; 10 ; i++{ if i ==5{ continue // 跳过本次循环，继续下一次 // break // 退出循环 } fmt.Println(i) } } package main import \u0026#34;fmt\u0026#34; func main(){ for i := 0; i \u0026lt;10; i++ { if i == 5 { break // 退出循环，输出结果 } fmt.Println(i) } } 3.2 for \u0026hellip;\u0026hellip;range 完成数据迭代，支持字符串、数组、数组指针、切片、字典、通道类型 ，返回索引、键值数据\npackage main import \u0026#34;fmt\u0026#34; func main() { s := \u0026#34;我爱中国\u0026#34; for i,v := range s { fmt.Printf(\u0026#34;%d, %q\\n\u0026#34;,i,v) } } //# 生成随机数 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;math/rand\u0026#34; \u0026#34;time\u0026#34; ) func main() { rand.Seed(time.Now().Unix()) //以当前系统时间为种子 fmt.Println(rand.Intn(100)) } 四. 冒泡排序 挨着两个元素比较\n","permalink":"https://xingxing.io/posts/golang/go%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6-02/","tags":["golang"],"title":"Go学习笔记-流程控制"},{"categories":["golang"],"contents":"fmt 包介绍 fmt 实现了格式化I/O 函数，类似于C的printf 和scanf 格式\nPrintln: 一次输入多个值的时候，Println 中间有空格 Println 会自动换行，Print 不会 Print 一次输入多个值的时候Print 没有 中间有空格 Print 不会自动换行 Printf\nPrintf 是格式化输出 一般:\n%v 相应值的默认格式。在打印结构体时，“加号” 标记(%+v)会添加字段名 %#v 相应值的Go语法表示 %T 相应值的类型的Go 语法表示 %% 字面上的百分号，并非值的占位符 布尔:\n%t 单词 true 或者 false 整数:\n%b 二进制表达 %c 相应的unicode码点所表示的字符 %d 十进制表示 %o 八进制表示 %q 单引号围绕的字符字面值 %x 十六进制表示，字符形式小写a-f %X 十六进制表示，字母形式的大写A-F %U Unicode格式: U+1234,等于\u0026#34;U+%04X\u0026#34; 浮点数及其复合构成\n%b 无小数部分的，指数为二的幂的科学计数法 %e 科学计数法 %E 科学计数法 %f 有小数点而无指数 字符串与字节切片\n%s 字符串或切片的无解译字节 %q 双引号围绕的字符串，由Go 语法安全地转移 %x 十六进制，小写字母 %X 十六进制，大写字母 指针\n%p 十六进制表示前缀 0x 实例：\npackage main import \u0026#34;fmt\u0026#34; func main() { var a int = 50 var b bool c := \u0026#39;a\u0026#39; fmt.Printf(\u0026#34;%v\u0026#34;, a) fmt.Printf(\u0026#34;%t\u0026#34;, b) fmt.Printf(\u0026#34;%v\u0026#34;, c) } ","permalink":"https://xingxing.io/posts/golang/go%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-fmt%E5%8C%85%E4%BB%8B%E7%BB%8D/","tags":["golang"],"title":"Go学习笔记-fmt包介绍"},{"categories":["golang"],"contents":"函数 1. 函数的定义 函数用于对代码块的逻辑封装，提供代码复用的最基本方式\n语法\nfunc 函数名称(参数) [返回值类型]{ 函数体 } 函数名: 满足标识符规范 形参: 无形参 有形参, 名字、类型，多个形参使用逗号分隔 返回值： 无返回值 返回值省略 有返回值: return关键字 必须指定返回值类型 只有一个返回值，返回值只需要写类型，可以省略小括号 如果有多个返回值，需要用小括号包含所有返回值类型 return 返回值数量必须和函数定义返回值类型数量一致 命名返回值 返回值定义值为每个返回值指定了变量名称及类型,用括号包含 返回是只用指定return 调用: 接收返回值 = 函数名(实参) 非可变参数: 实参数量必须与形参一致 =\u0026gt; 实参 按照顺序传递给形参 可变参数: 可变参数定义之前的变量 必须指定实参传递 可变参数部分可以传递任意多个值(可使用切片解包) 返回值: 无返回值不能接收 有返回值 必须用相同数量的变量接收返回值(按照返回值的顺序赋值给可接收的变量) 2. 函数调用 package main import \u0026#34;fmt\u0026#34; // 定义hello，无参，无返回值 func sysHello(){ fmt.Println(\u0026#34;hello\u0026#34;) } //有参数，无返回值 func sayHi(name string){ fmt.Println(\u0026#34;Hi\u0026#34;,name) } //有返回值 add func add(a int,b int)int{ return a + b //return 关键字用来向函数调用返回结果 } func main(){ // 调用函数,函数名称(参数[实参]) sysHello() sayHi(\u0026#34;xingxing\u0026#34;) c := add(4,3 ) fmt.Println(c) } package main import ( \u0026#34;fmt\u0026#34; ) func sayHello(){ fmt.Println(\u0026#34;hello\u0026#34;) } func add(a,b int) int{ return a + b } //返回多个值 func op(a,b int)(int,int,int,int){ return a + b, a - b , a * b, a/b } // 命名返回值 func opv2(a,b int)(sum int, sub int,mul int, div int){ sum = a + b sub = a - b mul = a * b div = a /b return } func main(){ sayHello() add(1,2) c := add(3,4) fmt.Println(c) fmt.Println(op(4,2)) a,b,c,d :=op(8,2) fmt.Println(\u0026#34;a=\u0026#34;,a,\u0026#34;b=\u0026#34;,b,\u0026#34;c=\u0026#34;,c,\u0026#34;d=\u0026#34;,d) fmt.Println(opv2(3,2)) } 3 汉诺塔算法(递归) package main import \u0026#34;fmt\u0026#34; func tower(a,b,c string,layer int){ if layer ==1 { fmt.Println(a,\u0026#34;-\u0026gt;\u0026#34;,c) return } tower(a,c,b,layer-1) fmt.Println(a,\u0026#34;-\u0026gt;\u0026#34;,c) tower(b,a,c,layer-1) } func main() { tower(\u0026#34;A\u0026#34;,\u0026#34;B\u0026#34;,\u0026#34;C\u0026#34;,3) } 4 冒泡 package main import \u0026#34;fmt\u0026#34; func bubble(nums []int){ for j := 0;j\u0026lt;len(nums)-1;j++{ for i :=0 ; i\u0026lt;len(nums)-1-j;i++{ if nums[i] \u0026gt; nums[i+1] { nums[i],nums[i+1]=nums[i+1],nums[i] } } } fmt.Println(nums) } func main() { nums :=[]int{5,4,3,2} fmt.Println(nums) nums =[]int{100,90,88,70,1000,100001} bubble(nums) } 5.函数类型 package main import \u0026#34;fmt\u0026#34; func add(a, b int) int{ return a + b } func mul(a,b int) int{ return a * b } func main(){ var f func(int, int) int = add var fs []func(int,int)int fs = append(fs,add,mul) c := f(2,3) fmt.Println(c) for _,f :=range fs { fmt.Println(f(2,3)) } } 6. 匿名函数 package main import \u0026#34;fmt\u0026#34; func main(){ c := func(){ fmt.Println(\u0026#34;我是匿名函数\u0026#34;) } fmt.Printf(\u0026#34;%T\\n\u0026#34;,c) c() } 7.引用类型 package main import \u0026#34;fmt\u0026#34; func main() { name :=\u0026#34;kk\u0026#34; nums :=[]int{} func(){ fmt.Println(name,nums) name = \u0026#34;silence\u0026#34; nums = []int{1,2,3} fmt.Println(name,nums) }() fmt.Println(name,nums) } ","permalink":"https://xingxing.io/posts/golang/go%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%87%BD%E6%95%B0/","tags":["golang"],"title":"Go学习笔记-函数"},{"categories":["devops"],"contents":"nexus 2.14 升级到3.20版本 nexus 2.x 升级到2.x 版本的话，只需要将sonatype-work 目录同步下，同步之后密码将是老的nexus 的密码，而不是最新的nexus2.x 的密码\nnexus 2.x 版本升级到3.x 版本\n需要将nexus 2.x 升级到指定的指定升级版本才可以升级到nexus 3.x 版本\nhttps://help.sonatype.com/repomanager3/upgrading?_ga=2.196178220.1873667372.1580866793-823368280.1575427960#Upgrading-Upgrading2.xto3.y 下载版本\nhttps://help.sonatype.com/repomanager3/download/download-archives---repository-manager-3 https://help.sonatype.com/repomanager2/download/download-archives---repository-manager-2 目前将nexus 升级到2.14.16\naccess token 123456 简单点\n创建完成\n在nexus-3.20 版本上登录\n选择设置-升级\n选择需要升级的repo\n选择\n在升级测试发现，2.14.16.01升级到最新版本是没有问题的3.20.1 版本是ok的\n等升级完成即可\n","permalink":"https://xingxing.io/posts/devops/nexus-maven%E4%BB%93%E5%BA%93%E5%8D%87%E7%BA%A7/","tags":["devops"],"title":"Nexus Maven仓库升级"},{"categories":["linux"],"contents":"机器中马 机器的CPU 一直很高，登录机器查看进程\n查看定时任务\n第一步杀掉sshd 的所有进程\nps -aux |grep sshd | kill `awk \u0026#39;{print $2}\u0026#39;` 查看/sbin/sshd 和没有中木马的机器对比一下md5 的值看是否一致，对比之后发现是一致的，说明这个文件没有被修改 ，\n查看网络连接数是否正常了\n第二步 删除文件\ncd /usr/games; rm -rf .sshd/ 第三步 安装clamav 进行杀毒\nyum groupinstall \u0026#34;Development Tools\u0026#34; -y yum install openssl openssl-devel libcurl-devel zlib-devel libpng-devel libxml2-devel json-c-devel bzip2-devel pcre2-devel ncurses-devel -y yum install sendmail sendmail-devel -y yum install epel-release -y yum config-manager --set-enabled PowerTools yum update yum repolist yum install valgrind check-devel ","permalink":"https://xingxing.io/posts/linux/%E6%9C%BA%E5%99%A8%E4%B8%AD%E9%A9%AC/","tags":["linux"],"title":"机器中马"},{"categories":["linux"],"contents":"centos 机器启动加载失败的处理 es 集群今天突然有一台机器，重启之后启动不起来了\n出现了图中的问题，这个问题看起来好像是文件存储有问题，尝试解决一下\ncontinnue 中输入机器的密码\n输入完密码进入到了终端\n在卸载模式下进行xfs_repair /dev/vdc\n出现此类问题，需要检查一下/etc/fstab 文件是否正常，另外就是检查和修复文件系统\n","permalink":"https://xingxing.io/posts/linux/centos%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%99/","tags":["linux"],"title":"Centos启动报错"},{"categories":["linux"],"contents":"netstat 查看服务连接数","permalink":"https://xingxing.io/posts/linux/netstat-%E6%9F%A5%E7%9C%8B%E8%BF%9E%E6%8E%A5/","tags":["linux"],"title":"netstat 查看连接"},{"categories":["linux"],"contents":"Too many open files 用户或系统打开文件限制超过默认设置时，将生成此错误\n查看操作系统级别的最大打开文件设置，使用以下命令:\ncat /proc/sys/fs/file-max 更改系统范围的最大打开文件，需要编辑/etc/sysctl.conf 文件\nfs.nr_open = 2000000 fs.file-max = 6553560 每个用户设置\necho \u0026#34;* soft nofile 655360\u0026#34; \u0026gt;\u0026gt; /etc/security/limits.conf echo \u0026#34;* hard nofile 655360\u0026#34; \u0026gt;\u0026gt; /etc/security/limits.conf 设置所有用户\nvim /etc/security/limits.conf 在最后加入 * soft nofile 655350 * hard nofile 655350 查看用户最大允许打开的文件数量\nulimit -a core file size (blocks, -c) 0 data seg size (kbytes, -d) unlimited scheduling priority (-e) 0 file size (blocks, -f) unlimited pending signals (-i) 31200 max locked memory (kbytes, -l) 64 max memory size (kbytes, -m) unlimited open files (-n) 1024 pipe size (512 bytes, -p) 8 POSIX message queues (bytes, -q) 819200 real-time priority (-r) 0 stack size (kbytes, -s) 8192 cpu time (seconds, -t) unlimited max user processes (-u) 31200 virtual memory (kbytes, -v) unlimited file locks (-x) unlimited # open files 表示每个用户最大允许打开的文件数量是1024 查看当前系统打开的文件数量\nlsof | wc -l watch “lsof | wc -l” 查看某一个进程打开的文件数量\nlsof -p pid | wc -l soft 与hard 的区分\nulimit 对资源的限制区分soft 与hard 两类，即同一个资源存在soft 与hard 两个值 在命令上，ulimit 通过-S 与—H 来区分soft 与hard ，如果没有指定-S 与-H，在显示时指的是soft，而在设置时指的是同时设置了soft 与hard 值 ","permalink":"https://xingxing.io/posts/linux/linux-openfiles%E7%9A%84%E9%97%AE%E9%A2%98/","tags":["linux"],"title":"Linux Openfiles的问题"},{"categories":["linux"],"contents":"iperf 的基本用法 iperf 是一个网络性能测试工具，iperf 可以测试TCP 和UDP 带宽质量\niperf -c ip -u -m -t 60 -i 10 -b 1000M ------------------------------------------------------------ Client connecting to 10.37.57.104, UDP port 5001 Sending 1470 byte datagrams, IPG target: 11.22 us (kalman adjust) UDP buffer size: 208 KByte (default) ------------------------------------------------------------ [ 3] local 10.39.23.47 port 39241 connected with 10.37.57.104 port 5001 [ ID] Interval Transfer Bandwidth [ 3] 0.0-10.0 sec 1.22 GBytes 1.05 Gbits/sec [ 3] 10.0-20.0 sec 1.22 GBytes 1.05 Gbits/sec [ 3] 20.0-30.0 sec 1.22 GBytes 1.05 Gbits/sec [ 3] 30.0-40.0 sec 1.22 GBytes 1.05 Gbits/sec [ 3] 40.0-50.0 sec 1.22 GBytes 1.05 Gbits/sec [ 3] 50.0-60.0 sec 1.22 GBytes 1.05 Gbits/sec [ 3] WARNING: did not receive ack of last datagram after 10 tries. [ 3] 0.0-60.0 sec 7.32 GBytes 1.05 Gbits/sec [ 3] Sent 5349878 datagrams iperf 用法 -c 以客户端模式运行，并制定服务端的地址 -b 指定客户端通过UDP 协议发送信息的带宽 -d 同时进行双向传输测试 -n 指定传输的字节数 -r 单独进行双向传输测试 -t 指定iperf测试的时间，默认10s -i 设置每次报告之间的时间间隔，单位为秒。 TCP 测试 iperf -c host -i 1 -w 1M #hosts为ip 地址 UDP 测试 iperf -u -c 10.2.1.2 -b 100M -i 1 -w 1M -t 60 ","permalink":"https://xingxing.io/posts/linux/iperf%E6%B5%8B%E8%AF%95%E7%BD%91%E7%BB%9C%E5%B8%A6%E5%AE%BD/","tags":["linux"],"title":"Iperf测试网络带宽"},{"categories":["devops"],"contents":"git 代码提交报错 开发人员告知提交不了代码，我测试了一下，报错如下，摘取了部分\nerror: unpack failed: unable to create temporary object directory\n根据报错，查看是否是因为磁盘空间满了或者权限问题导致，最后查看git 目录确认是目前权限导致引起的报错，修改目录权限之后即可提交\n","permalink":"https://xingxing.io/posts/devops/git%E6%8F%90%E4%BA%A4%E4%BB%A3%E7%A0%81%E6%8A%A5%E9%94%99/","tags":["devops"],"title":"Git提交代码报错"},{"categories":["devops"],"contents":"gitlab 容器化部署 docker run -d \\ -p 80:80 \\ -p 443:443 \\ -p 22:22 \\ --name gitlab \\ --restart unless-stopped \\ -v /data/gitlab/config:/etc/gitlab \\ -v /data/gitlab/logs:/var/log/gitlab \\ -v /data/gitlab/data:/var/opt/gitlab \\ gitlab/gitlab-ce:15.2.2-ce.0 #查看密码 docker exec -it gitlab grep \u0026#39;Password:\u0026#39; /etc/gitlab/initial_root_password 修改密码 root@c2a89b6edebf:/# gitlab-rails console -e production -------------------------------------------------------------------------------- Ruby: ruby 2.7.5p203 (2021-11-24 revision f69aeb8314) [x86_64-linux] GitLab: 15.2.2 (4ecb014a935) FOSS GitLab Shell: 14.9.0 PostgreSQL: 13.6 ------------------------------------------------------------[ booted in 37.12s ] Loading production environment (Rails 6.1.4.7) irb(main):001:0\u0026gt; user = User.where(id: 1).first =\u0026gt; #\u0026lt;User id:1 @root\u0026gt; irb(main):002:0\u0026gt; irb(main):003:0\u0026gt; user.password = \u0026#39;admin123456\u0026#39; =\u0026gt; \u0026#34;admin123456\u0026#34; irb(main):004:0\u0026gt; user.password_confirmation = \u0026#39;admin123456\u0026#39; =\u0026gt; \u0026#34;admin123456\u0026#34; irb(main):005:0\u0026gt; user.save =\u0026gt; true irb(main):006:0\u0026gt; 添加用户，开启SMTP # 网易企业邮箱 gitlab_rails[\u0026#39;smtp_enable\u0026#39;] = true gitlab_rails[\u0026#39;smtp_address\u0026#39;] = \u0026#34;smtphz.qiye.163.com\u0026#34; gitlab_rails[\u0026#39;smtp_port\u0026#39;] = 465 gitlab_rails[\u0026#39;smtp_user_name\u0026#39;] = \u0026#34;abc@163.com\u0026#34; gitlab_rails[\u0026#39;smtp_password\u0026#39;] = \u0026#34;Y9zyx2gzTXsCJ8wW\u0026#34; # 网易企业邮箱授权码 gitlab_rails[\u0026#39;smtp_domain\u0026#39;] = \u0026#34;qiye.163.com\u0026#34; gitlab_rails[\u0026#39;smtp_authentication\u0026#39;] = \u0026#34;login\u0026#34; gitlab_rails[\u0026#39;smtp_enable_starttls_auto\u0026#39;] = true gitlab_rails[\u0026#39;smtp_tls\u0026#39;] = true gitlab_rails[\u0026#39;gitlab_email_enabled\u0026#39;] = true gitlab_rails[\u0026#39;gitlab_email_from\u0026#39;] = \u0026#39;abc@163.com\u0026#39; gitlab_rails[\u0026#39;gitlab_email_display_name\u0026#39;] = \u0026#39;Gitlab\u0026#39; #重新加载配置 gitlab-ctl reconfigure gitlab-ctl restart #邮箱测试 gitlab-rails console Notify.test_email(\u0026#39;你的收件邮箱\u0026#39;, \u0026#39;邮件标题\u0026#39;, \u0026#39;邮件正文\u0026#39;).deliver_now ","permalink":"https://xingxing.io/posts/devops/gitlab%E5%AE%B9%E5%99%A8%E5%8C%96%E9%83%A8%E7%BD%B2/","tags":["devops"],"title":"Gitlab容器化部署"},{"categories":["devops"],"contents":"ansible 日常使用 运维管理工具比较 工具 语言 架构 协议 puppet ruby c/s http chef ruby c/s http ansible python 无client ssh saltstack python c/s(可无client) SSH/ZMQ/RAET fabric python ssh ansible 使用 ansible 优缺点 优点: SSH 高安全性； 不需要在远程节点上安装任何代理； 使用YAML 学习简单，Playbook 结构简单； 简便 ，易用； 顺序执行顺序； 缺点： 不需要代理，但是需要 SSH 访问和python 解释器 工作机制 Ansible playbooks: 任务剧本，编排定义ansible 任务集的配置文件，由ansible顺序依次执行，通常是json格式的yaml文件。 INVENTORY: ansible 管理主机的清单 MODULES: ansible 执行命令的功能模块，多数为内置的核心模块，也可自定义 PLUGINS: 模块功能的补充，如连续类型插件、循环插件、变量插件、过滤插件等 API: 供第三方程序调用的应用程序编程接口 Ansible 命令：\nAnsible Ansible-galaxy Ansible-pull Ansible-doc Ansible-playbook Ansible-vault Ansible-console Ansible 执行方式：\nAd-Hoc(命令模式ansible) Ansible-playbook web(收费) Ad-Hoc 命令集由/usr/bin/ansible 实现，其命令用法如下:\nAnsible \u0026lt;host-pattern\u0026gt; [options] ansible -i hosts kafka -m ping # 检查服务器的存活 ansible 返回结果都非常友好，一般会用3种颜色来表示执行结果: 红色、绿色、 橘黄色。其中红色表示执行过程有异常，一般会中止剩余所有的任务，绿色和、 橘黄色表示执行过程没有异常，橘黄色表示执行结束后由状态的变化 命令模式(ansible)\nansible -i hosts k8s -m shell -a “hostname” 解释 -i 指定hosts 文件地址，默认/etc/ansible/ 目录下 -m 需要执行的模块名称 -a 需要执行的模块参数 ansible inventory 配置及详解 inventory 是ansible 管理主机信息的配置文件，默认存放在/etc/ansible/hosts, inventory 是可以定义变量的，只不过这些变量只能在ansible-ploybook 中使用，而ansible 是不支持的 ansible-doc 是ansible 模块文档说明\nAnsible-doc [options] [module…….] Ansible-doc -l 列出支持的模块 Ansible-doc ping 模块功能说明 ansible 命令执行 命令常用模块 Ansible 命令常用模块：\n1. copy 实现文件复制和批量文件下发 ansible -i hosts master -m copy -a “src=etcd.yaml dest=/opt” -k 2. yum 安装和卸载软件包 ansible -i hosts master -m yum -a “name=httpd” 3. ping 模块 ansible –i hosts all -m ping or ansible -I hosts 192.168.1.* -m ping 4. setup 查看主机信息 ansible -i hosts master -m setup 5. Command 执行系统命令 ansible -i hosts master -m command -a “df -h ” 6. Shell ansible -i hosts master -m shell -a “df -h ” 7. Script 执行脚本 ansible -i hosts master -m script –a “bash /opt/script.sh” ","permalink":"https://xingxing.io/posts/devops/ansible-%E4%BD%BF%E7%94%A8/","tags":["devops"],"title":"Ansible 使用"},{"categories":["Kubernetes"],"contents":"Kubernetes 是什么？ Kubernetes 是谷歌严格保密十几年的秘密武器-Borg的一个开源版本，Brog 是谷歌一个久负盛名的内部使用的大规模集群管理系统，他基于容器技术，目的是实现资源管理的自动化，以及跨多个数据中心的资源利用率的最大化，直到2015年才被谷歌首次公开。\nkubernetes 是一个可移植，可扩展的开源平台，用于管理容器化工作负载和服务，有助于声明性配置和自动化。\nKubernetes 能解决什么？\n使用Kubernetes 解决方案可以不必头疼于服务监控、部署实施、故障处理，使开发人员更加专注于业务本身，而且降低了运维难度和运维成本。\nKubernetes 是一个分布式系统支撑平台。Kubernetes 具有完备的集群管理能力，包括多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和服务发现机制、内建的智能负载均衡器、强大的故障发现和自我修复能力、服务滚动升级和在线扩容能力、可扩展的资源自动调度机制，以及多粒度的资源配额管理能力。\nKubernetes 基本概念：\nmaster 指的是集群控制节点，master 是负责整个集群的管理和控制\nmaster 上运行的进程有:\nKubernetes API server(kube-apiserver) : 提供了HTTP Rest 接口的关键服务进程，是kubernetes 里所有资源的增、删、改、查等操作的唯一入口，也是集群控制的入口进程。 kubernetes Controller Manager(kube-controller-manager): Kubernetes 里所有资源对象的控制中心，处理集群中常规任务的后台线程，主要有以下功能： ​ 节点控制器：当节点移除时，负责注意和响应 ​ 副本控制器: 负责维护系统中每个副本控制器对象正确数量的pod ​ 端点控制器: 填充端点(Endpoints)对象(即连接Service\u0026amp;Pods) ​ 服务账户和令牌控制器: 为新的命名空间创建默认账户和API访问令牌 kubernetes Scheduler(kube-scheduler): 负责资源调度(Pod调度)的进程 Etcd: 数据存储 Node 上运行的进程有:\nkubelet: 负责pod 对应容器的创建，启停等任务，同时与master 密切协作，实现集群管理的基本功能 kube-proxy: 实现kubernetes Service 的通信与负载机制的重要组件 Docker: 负责本机的容器创建和管理工作 Pod\npod 是kubernetes 最重要的基本概念，每个pod 都有一个特殊的被称为\u0026#34;根容器\u0026#34;的pause容器。Pause 容器对应的镜像属于kubernetes 平台的一部分，出了pause容器，每个pod 还包含一个或多个紧密相关的用户业务容器 pause 容器作用\n在pod 中担任linux 命名空间共享的基础； 启用pid 命名空间，开启init 进程； Pod 有两种类型：\n普通的pod 和静态pod， 静态pod 并不存储在kubernetes 的etcd 存储，而是存放在某个具体Node上的一个具体文件中，并且只在此Node上启动，运行，用kubelet--pod-mainfest-path=\u0026lt;the directory\u0026gt; 普通的pod 会被存放在etcd 存储中，随后会被kubernetes master 调度到某个node 节点上启动运行 label 标签\nlabel标签是kubernetes 系统中另外一个核心概念，一个label 是一个kye=value 的键值对，其中key 与value 由用户自己指定，label 可以被附加到各种资源对象上，例如Node pod Service RC 等，一个资源对象可以定义任意数量的label，同一个label 也可以被添加到任意数量的资源对象上,label 通常在资源对象定义时确定。 例子\nlabel 被定义在metadata中:\napiVersion: v1 kind: Pod metadata: name: myweb labels: app: myweb 管理RC和Service 则通过Selector 字段设置需要关联Pod的Label\napiVersion: v1 kind: ReplicationController metadata: name: myweb spec: replicas: 1 selector: app: myweb template: ....... apiVersion: v1 kind: Service metadata: name: myweb spec: selector: app: myweb ports: - port: 8080 Replication Controller (RC)\napiVersion: v1 kind: ReplicationController metadata: name: rc-demo labels: app: rc spec: replicas: 3 selector: app: rc template: metadata: labels: app: rc spec: containers: - name: nginx-demo image: nginx ports: - containerPort: 80 Replication Controller 已经升级为Replica setReplication Set与RC 当前的唯一区别是，Replica Sets支持基于集合的Label selector(Set-based selector)\nRC 的特性：\nRC 实现Pod的创建及副本数量 RC 通过Label Selector 机制实现对Pod副本的自动控制 Deployment\nDeployment 相对于RC的最大升级是我们可以随时知道当前pod部署进度\napiVersion: apps/v1beta1 kind: Deployment metadata: name: nginx-deploy labels: app: nginx-demo spec: replicas: 3 minReadySeconds: 5 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 selector: app: rc template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 以下参数为更新应用时使用 minReadySeconds: 5 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 参数为滚动更新应用时使用 ","permalink":"https://xingxing.io/posts/kubernetes/kubernetes-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","tags":["Kubernetes"],"title":"Kubernetes 基础知识"},{"categories":["linux"],"contents":"linux 性能优化笔记 linux 性能优化图\n一、 CPU\n系统平均负载\nuptime 10:49:50 up 149 days, 22:43, 2 users, load average: 0.00, 0.01, 0.05 10:49:50 //当前时间 up 149 days, 22:43 //系统运行时间 2 users // 正在登陆用户数 CPU 统计 grep \u0026#39;model name\u0026#39; /proc/cpuinfo |wc -l 或者 nproc 或者 lscpu 安装stress 和sysstat\nyum install http://download-ib01.fedoraproject.org/pub/epel/7/x86_64/Packages/s/stress-1.0.4-16.el7.x86_64.rpm stress 是一个linux 系统压力测试工具，模拟系统负载较高时场景 -c 产生n个进程，每个进程都反复不停的计算随机数的平方根 -i 产生n个进程，每个进程反复sync()将内存上 CPU 高使用的工具\n负载高的时候可以使用以下工具查看\nmpstat 是一个常用的linux 性能分析工具，用来实时查看每个CPU 的性能指标，以及所有CPU 的平均指标 -P{|ALL} 表示监控哪个CPU，在0，CPU 个数-1 中取值 internal:相邻两个采样的间隔时间 count: 采样的次数，count 只能和delay 一起使用 mpstat -P ALL 1 间隔为1s pidstat 查看进程CPU 使用情况 pidstat -u 5 1 观察 平均负载\n简单来说，平均负载是指单位时间内，系统处于可运行状态和不可中断状态的平均进程数，也就是平均活跃进程数，它和CPU使用率并没有直接关系 可运行状态的进程\n所谓可运行状态的进程，是指正在使用CPU或者正在等待CPU的进程，也就是我们常用ps 命令看到的，处于R 状态(Running或者Runnable)的进程 平均负载与CPU 使用率的关系\n平均负载是指单位时间内处于可运行状态和不可中断状态的进程数。所以它不仅包括了正在使用CPU的进程，好包括等待CPU和等待IO的进程。而CPU使用率，是单位时间内CPU繁忙情况的统计，跟平均负载并不一定完全对应，比如： ​ CPU 密集型进程，使用大量CPU会导致平均负载升高，此时这两者是一致。 ​ IO密集型进程，等待IO 也会导致平均负载升高，但CPU使用率不一定很高 ​ 大量等待CPU的进程调度也会导致平均负载升高，此时的CPU使用率也会比较高 不可中断状态的进程 不可中断状态的进程是处于内核态关键流程中的进程，且这些流程是不可打断的，比如常见的是等待硬件设备的IO响应，也就是我们在ps命令中看到的D 状态的进程。比如当一个进程向磁盘写数据时，为了保证数据的一致性，在得到磁盘回复前，它是不能被其他进程中断打断的，这时进程就处于不可中断状态。所以，不可中断状态实际是系统对进程和硬件设备的一种保护机制。 平均负载其实就是平均活跃进程数，平均活跃进程数，直观上的理解就是单位时间内的活跃进程数，但它实际上是活跃进程数的指数衰减平均值。那么最理想的，就是每个CPU上都刚好运行着一个进程，这样每个CPU 都得到充分利用， 案例\n使用top 查看CPU 消耗\ntop\n%CPU 状态信息\nuser 用户控件占用CPU 百分比 system 内核空间占用CPU 百分比 nice 改变过优先级的进程占用CPU 的百分比 idle 空闲CPU 百分比 iowait IO等待占用CPU 的百分比 hi 硬中断(Harware IRQ) 占用CPU的百分比 si 软中断(Software interrupts) 占用CPU百分比 PID 4872 这个进程可疑\n在这里机器被重启之后进程改变为1258\nyum install psmisc -y 安装的是pstree top -H -p 1258 查看出线程 yum install gdb -y gdb attch 1258\n输出线程\npstack 显示进程的栈跟踪\npstack 1258 进程和线程的区别\n进程是资源分配和执行的基本单位;线程是任务调度和运行的基本单位。线程没有资源，进程给指针提供虚拟内存、栈、变量等共享资源，而线程可以共享进程的资源。 查看系统的上下文切换\nvmstat 每隔3s输出一组数据 cs(context switch) 表示每秒上下文切换次数 in(interrupt) 表示每秒中断的次数 r(running of runnable)表示就绪队列的长度，也就是正在运行和等待CPU的进程数 b(Blocked) 表示处于不可中断睡眠状态的进程数 故障解决工具使用:\n如果top 查看某个进程使用的CPU 高，可以使用pidstat 查看进程使用\npidstat -p\n如果进程不断重启的话，创建新进程的话使用\npstree 查看父进程\nperf 用来分析CPU 性能分析\nperf record -g 等待15s 后按Ctrl+C 退出，然后运行perf report 查看报告\n进程的状态\n使用top 会看到进程显示的状态\nR 是Running 或者Runnable 的缩写，表示进程在CPU 的就绪队列中，正在运行或者正在等待运行\nD 是Disk Sleep的缩写，也就是不可中断状态睡眠(Uninterruptible Sleep),一般表示进程正在跟硬件交互，并且交互过程不允许被其他进程或中断打断\nZ 是Zombie 的缩写，表示僵尸进程，也就是已经结束了，父进程没有回收它的资源\nS 是Interruptible Sleep 的缩写，也就是中断状态睡眠，表示进程因为等待某个事件而被系统挂起，当进程等待事件发生时，它会被唤醒并进入R 状态\nI 是ldle的缩写，也就是空闲状态，用在不可中断睡眠的内核线上，\niowait 升高\niowait 升高需要先使用dstat、pidstat 等工具查看CPU和I/O 这两种资源的使用情况\nlinux中的软中断\n查看软中断和内核线程\ncat /proc/softirqs 查看软中断运行情况\ncat /proc/interrupts 查看硬中断的运行情况\ncat /proc/softirqs CPU0 CPU1 CPU2 CPU3 HI: 0 0 2 0 TIMER: 246520371 287657513 276025689 471741036 NET_TX: 5813 7255 10618 11953 NET_RX: 8457973 11947129 12194105 2090436989 BLOCK: 0 0 0 0 BLOCK_IOPOLL: 0 0 0 0 TASKLET: 98 154 163 156 SCHED: 95782931 95760089 90090419 123126076 HRTIMER: 0 0 0 0 RCU: 179145718 200933026 196728001 293878650 ","permalink":"https://xingxing.io/posts/linux/linux%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0/","tags":["linux"],"title":"Linux性能优化笔记"},{"categories":["kubernetes"],"contents":"helm 使用详解 仓库\n查看仓库 helm repo list NAME URL stable\thttps://kubernetes-charts.storage.googleapis.com local http://127.0.0.1:8879/charts 添加仓库 ","permalink":"https://xingxing.io/posts/kubernetes/helm-%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3/","tags":["kubernetes"],"title":"Helm 使用详解"},{"categories":["kubernetes"],"contents":"helm 安装使用 helm 可帮助管理kubernetes 应用程序，helm charts 可帮助定义，安装和升级最复杂的kubernetes 应用\n下载helm wget https://github.com/helm/helm/archive/v2.13.1.tar.gz tar -zxvf helm-v2.13.1-linux-amd64.tar.gz mv linux-amd64/helm /usr/local/bin/helm helm version Client: \u0026amp;version.Version{SemVer:\u0026#34;v2.13.1\u0026#34;, GitCommit:\u0026#34;618447cbf203d147601b4b9bd7f8c37a5d39fbb4\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;} Error: could not find tiller 服务器端安装\nhelm init 或者helm init --upgrade --tiller-image ***** habror.enncloud.cn/enncloud/tiller:v2.13.1 $HELM_HOME has been configured at /root/.helm. Tiller (the Helm server-side component) has been upgraded to the current version. Happy Helming\t查看\nkubectl get pod -n kube-system tiller-deploy-54cc7bf8d7-gtvsn 1/1 Running 0 30m helm version Client: \u0026amp;version.Version{SemVer:\u0026#34;v2.13.1\u0026#34;, GitCommit:\u0026#34;618447cbf203d147601b4b9bd7f8c37a5d39fbb4\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;} Server: \u0026amp;version.Version{SemVer:\u0026#34;v2.13.1\u0026#34;, GitCommit:\u0026#34;618447cbf203d147601b4b9bd7f8c37a5d39fbb4\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;} helm 绑定k8s 集群admin 权限\nvim helm-rabc.yaml apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - name: tiller kind: ServiceAccount namespace: kube-system 创建权限\nkubectl create -f helm-rabc.yaml serviceaccount/tiller created clusterrolebinding.rbac.authorization.k8s.io/tiller created kubectl patch deploy --namespace kube-system tiller-deploy -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;spec\u0026#34;:{\u0026#34;serviceAccount\u0026#34;:\u0026#34;tiller\u0026#34;}}}}\u0026#39; 使用helm helm 库\nhelm repo list NAME URL stable\thttps://kubernetes-charts.storage.googleapis.com local http://127.0.0.1:8879/charts! 创建一个nginx 访问 helm create hello-helm 查看一下目录结构 ​ 创建nginx 来访问\n修改values.yaml 中的ClusterIP 为NodePort helm install ./hello-helm ​ ​ 访问测试\n​ nodeIP:Port\n​\n","permalink":"https://xingxing.io/posts/kubernetes/helm-%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8/","tags":["kubernetes"],"title":"Helm 安装使用"},{"categories":["golang"],"contents":"复合数据类型 1.数组 数组是具有相同数据类型的数据项组成的一组长度固定的序列，数据项叫做数组的元素，数组的长度必须是非负整数的常量，长度也是类型的一部分\n1.1声明 数组声明需要指定组成元素的类型以及存储元素的数量(长度)。在数组声明后，其长度不可修改，数组的每个元素会根据对应类型的零值对进行初始化\npackage main import \u0026#34;fmt\u0026#34; func main() { var names [10]string var scores [10]int fmt.Printf(\u0026#34;%T, %T\\n\u0026#34;,names,scores) fmt.Printf(\u0026#34;%q\\n\u0026#34;,names) fmt.Println(scores) } #索引 package main import \u0026#34;fmt\u0026#34; func main() { var abc [10]int = [10]int{2:1000} fmt.Println(abc) } #索引 0 \u0026gt; len-1 #0 1 2 索引打印的值为 [0 0 1000 0 0 0 0 0 0 0] 索引赋值 顺序赋值 [lenght]type{v1,v2,v3.......} 索引 [lenght]type{i1:v1,i2,v2....} 1.2字面量 *指定数据的长度\n*使用初始化元素数量推到数组长度\n*对指定位置元素进行初始化\npackage main import \u0026#34;fmt\u0026#34; func main(){ var names[10] string var scores [5]int = [5]int{1,2,3,4,5} //数组赋值，字面量 fmt.Printf(\u0026#34;%T\\n\u0026#34;,names) fmt.Printf(\u0026#34;%q\\n\u0026#34;,names) fmt.Printf(\u0026#34;%T\\n\u0026#34;,scores) fmt.Println(names,scores) } 1.2 数组操作 *关系运算== 、!=\npackage main import \u0026#34;fmt\u0026#34; func main(){ var names[10] string var scores = [...]int{100,88} //数组赋值，字面量 fmt.Println(names,scores) //运算 var nums [2]int = [...]int{88,100} fmt.Println(nums == scores) } 1.2.1 字面量访问 *访问值通过索引\npackage main import \u0026#34;fmt\u0026#34; func main(){ var scores = [...]int{100,88} //数组赋值，字面量 var nums [2]int = [...]int{88,100} //运算 fmt.Println(nums == scores) //访问 fmt.Println(nums[0]) nums[0] = 101 nums[1] = 102 fmt.Println(nums) //赋值 nums[0] = 101 nums[1] = 102 fmt.Println(nums[0]) fmt.Println(nums[1]) } 1.2.2 数组长度 *使用len函数可以获取数组的长度\nfmt.Println(len(nums)) 1.2.3 元素访问(遍历) 方法一: for i :=0; i \u0026lt; len(nums); i++ { fmt.Println(i,nums[i]) } 方法二: for v := range nums { fmt.Println(v,nums[v]) } 方法三: for i, v := range nums{ fmt.Println(i,v) } 2. 切片 *切片是长度可变的数组(具有相同数据类型的数据项组成的一组长度可变的序列)，切片由三部分组成:\n指针: 指向切片第一个元素指向元素的地址 长度: 切片元素的数量 容量: 切片开始到结束位置元素的数量 2.1 声明 *切片声明需要指定组成元素的类型，但不需要指定存储元素的数量(长度),在切片声明后，会被初始化为nil，表示暂不存在的切片\npackage main import \u0026#34;fmt\u0026#34; func main(){ var names []string fmt.Printf(\u0026#34;%T\\n\u0026#34;,names) fmt.Printf(\u0026#34;%v\\n\u0026#34;,names) } 2.2 初始化 //初始化值 names = []string{\u0026#34;xingxing\u0026#34;,\u0026#34;123\u0026#34;} fmt.Printf(\u0026#34;%T\\n\u0026#34;,names) fmt.Printf(\u0026#34;%q\\n\u0026#34;,names) 2.3 字面量 //字面量 //[]type{}=\u0026gt; 空切片 //[]type{v1,v2,v3......} //[]type{i1:v1,i2:v2....} names =[]string{1:\u0026#34;golang\u0026#34;, 10:\u0026#34;123\u0026#34;} fmt.Printf(\u0026#34;%T\\n\u0026#34;,names) fmt.Printf(\u0026#34;%q\\n\u0026#34;,names) 2.4 元素访问 //访问 修改元素 //索引 fmt.Println(names[1]) fmt.Println(names[10]) fmt.Println(names[8]) names[8]=\u0026#34;学习go\u0026#34; //修改 fmt.Println(names) 2.5 长度与遍历 fmt.Println(len(names)) //长度，切片中已经存在元素的数量 //遍历 for i := 0;i\u0026lt;len(names);i++{ fmt.Println(i,names[i]) } for v := range names{ fmt.Println(v,names[v]) } for i, v := range names{ fmt.Println(i,v) } 2.6 添加元素 //添加元素，是给末位添加 names = append(names,\u0026#34;测试添加\u0026#34;) fmt.Println(names) 2.7 删除元素 //删除元素 //切片操作 //names[start:end] names 中从start开始到end-1 所有元素组成的切片 fmt.Printf(\u0026#34;%q\\n\u0026#34;,names[1:10]) //删除索引为0，如果索引为len-1元素 names = names[1:len(names)] fmt.Printf(\u0026#34;%q\\n\u0026#34;,names) names = names[0:len(names)-1] fmt.Printf(\u0026#34;%q\\n\u0026#34;,names) //删除中间的元素 nums :=[]int{0,1,2,3,4,5} //删除3 nums2 :=[]int{10,11,12,13,14,15,16} copy(nums,nums2) fmt.Println(nums,nums2) //nums[0:3],nums[4:5] //nums2 多 copy(nums,nums2) fmt.Println(nums,nums2) //nums[0:3],nums[4:5] copy(nums[3:len(nums)],nums[4:len(nums)]) fmt.Println(nums) #切片底层共享数组 2.8 make 函数 package main import \u0026#34;fmt\u0026#34; func main(){ //make // 2个参数: make(type,len) // 3个参数: make(type,len,cap) // nums := make([]int,3) fmt.Println(len(nums),cap(nums)) fmt.Println(nums) //指定len 与cap nums2 := make([]int,2,5) fmt.Println(len(nums2),cap(nums2)) fmt.Println(nums2) nums3 := nums2 nums3 = append(nums3,4) nums2 = append(nums2,4) fmt.Println(nums3) fmt.Println(nums2) //变量赋值的时候是复制的方式 } package main import \u0026#34;fmt\u0026#34; func main() { //复制nums 中的所有数据到nums2(两个不会有相互影响) //第一种方法 //nums := []int{1,2,3,4,5} //nums2 :=[]int{} //copy(nums2,nums) //第二种方法 //nums := []int{1,2,3,4,5} //nums2 := make([]int,len(nums)) //for i, v := range nums { // nums2[i] = v //} //fmt.Println(nums2) //第三种方法 nums := []int{1, 2, 3, 4, 5} nums2 := make([]int, 0, len(nums)) // 空白标识符 for _, v := range nums { nums2 = append(nums2, v) } fmt.Println(nums2) } 2.9 容量 package main import \u0026#34;fmt\u0026#34; func main() { nums := []int{1,2,3,4,5} fmt.Println(len(nums),cap(nums)) nums = append(nums,6) fmt.Println(len(nums),cap(nums)) nums= append(nums,7) fmt.Println(len(nums),cap(nums)) nums = append(nums,8) fmt.Println(len(nums),cap(nums)) } package main import \u0026#34;fmt\u0026#34; func main() { // 复制nums 中所有数据到nums2 //第一种方法 //nums := []int{1,2,3,4,5} //nums2 := []{} //copy(nums2,nums) /* 第二种方法 nums := []int{1,2,3,4,5} nums2 := make([]int,len(nums)) for i, v := range nums { nums2[i] =v } fmt.Println(nums2)*/ nums := []int{1,2,3,4,5} nums2 := make ([]int,0,len(nums)) for _,v := range nums { nums2 = append(nums2,v) } fmt.Println(nums2) } //容量的计算 package main import \u0026#34;fmt\u0026#34; func main() { //nums := []int{1,2,3,4,5} nums := make([]int,5,100) // slice[start:end] // 0 \u0026lt;= start \u0026lt;= end \u0026lt;= cap // len := end-start // cap := cap - start nums2 := nums[10:100] fmt.Println(nums2) fmt.Println(nums2,len(nums2),cap(nums2)) } 2.10 切片的解包操作 package main import \u0026#34;fmt\u0026#34; func main() { nums :=[]int{1,2,3} nums2 :=[]int{3,4,5} nums = append(nums,100,102,105) fmt.Println(nums,nums2) /*for _, v:=range nums2 { nums = append(nums,v) } fmt.Println(nums,nums2) */ // 切片解包 nums = append(nums,nums2...) fmt.Println(nums,nums2) } 2.11 队列与堆栈 队列\npackage main import \u0026#34;fmt\u0026#34; func main(){ //队列 //先进先出 queue := []int{} queue = append(queue,1) queue = append(queue,2) queue = append(queue,3) queue = append(queue,4) queue = append(queue,5) queue = append(queue,6) //append 右边进入 // 1 - \u0026gt; [1] // 2 -\u0026gt; [2] // 3 -\u0026gt; [1,2,3] //从左边出 //\u0026lt;- 1 [2,3] //\u0026lt;- 2 [3] //\u0026lt; -3 [] for len(queue) != 0{ fmt.Println(queue[0]) queue = queue[1:] } fmt.Println(\u0026#34;over\u0026#34;) } 堆栈\npackage main import \u0026#34;fmt\u0026#34; func main(){ stack := []int{} //先进后出 //append 从右边进入 // 1 -\u0026gt; [1] // 2 -\u0026gt; [2] // 3 -\u0026gt; [3] // 从左边出 // 3,2,1 stack = append(stack ,1) stack = append(stack,2) stack = append(stack,3) for len(stack) !=0 { fmt.Println(stack[len(stack)-1]) stack = stack[:len(stack)-1] } } 3. 浮点数 带小数的，非精确的\n字面量\n十进制表示法: 3.1415926 科学记数法: Le-5 package main import \u0026#34;fmt\u0026#34; func main() { height := 1.169 fmt.Printf(\u0026#34;%T,%f,%g,%e\\n\u0026#34;,height,height,height,height) //运算 //算数运算 //关系运算 \u0026gt;,\u0026gt;=,\u0026lt;,\u0026lt;= //赋值运算 fmt.Println(1.2+1.1 ) fmt.Println(1.2-1.1) fmt.Println(1.2*1.1) fmt.Println(1.2/1.1) //类型转换 float64() } 4.字符串 go语言内置了字符串类型，使用string 表示\n字面量 ****可解析字符串: 通过双引号(\u0026#34;)来创建，不能包含多行，支持特殊字符转义序列 ****原生字符串:通过反引号(`)来创建，可包含多行，不支持特殊字符穿衣序列 package main import \u0026#34;fmt\u0026#34; func main() { var name string = `xingxing` fmt.Printf(\u0026#34;%T,%s\\n\u0026#34;,name,name) name += \u0026#34;hi\u0026#34; fmt.Println(name) name = \u0026#34;我爱中国\u0026#34; //索引 fmt.Printf(\u0026#34;%T\\n\u0026#34;,name[0]) //长度 fmt.Println(len(name)) //切片 fmt.Println(name[0:4]) } package main import ( \u0026#34;fmt\u0026#34; \u0026#34;unicode/utf8\u0026#34; ) func main() { var name string = \u0026#34;xingxing\u0026#34; var desc string =\u0026#34;i love china\u0026#34; // 字面量 // 零值 // 操作 // 连接 // 关系运算 // 索引，字节 // 切片 =\u0026gt; 字符串，字节 fmt.Println(name,desc) fmt.Println(desc[1:5]) for i, v := range desc { fmt.Println(i,v) } var txt = \u0026#34;我爱中国\u0026#34; for i , v := range txt { fmt.Printf(\u0026#34;%d,%q\\n\u0026#34;, i,v ) } fmt.Println(utf8.RuneCountInString(txt)) // 打印字节 } 语法 描述/结果 s += t 将字符串t 追加到字符串s 末尾 s[n] 字符串s中索引位置为n 处的原始字节 s[n:m] 从位置n到位置m-1处取得的字符串 s[n:] 从位置n到位置len(s)-1处取得的字符串 s[:m] 从索引位置0到位置m-1处取得的字符串 len(s) 字串符s中的字节数 5 流程控制 条件语句(if-elase if-else) 或选择语句(switch case)及循环语句(for)\npackage main import \u0026#34;fmt\u0026#34; func main() { var score int fmt.Print(\u0026#34;请输入成绩:\u0026#34;) fmt.Scan(\u0026amp;score) if score \u0026gt;= 90 { fmt.Println(\u0026#34;A\u0026#34;) } else if score \u0026gt;= 80 { fmt.Println(\u0026#34;B\u0026#34;) } else if score \u0026gt;= 60 { fmt.Println(\u0026#34;C\u0026#34;) } } package main import \u0026#34;fmt\u0026#34; func main() { var score int fmt.Print(\u0026#34;请输入成绩:\u0026#34;) fmt.Scan(\u0026amp;score) switch { case score \u0026gt;= 90: fmt.Println(\u0026#34;A\u0026#34;) case score \u0026gt;= 80: fmt.Println(\u0026#34;B\u0026#34;) case score \u0026gt;=70: fmt.Println(\u0026#34;C\u0026#34;) default: fmt.Println(\u0026#34;E\u0026#34;) } } 6 类型转换 类型转换格式 [:] =()\npackage main import \u0026#34;fmt\u0026#34; func main() { var c float32 = 100.1 fmt.Println(c) d := int(c) fmt.Println(d) } 结果:100 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;unicode/utf8\u0026#34; ) func main() { var name string = \u0026#34;xingxing\u0026#34; var desc string = \u0026#34;i love china \u0026#34; // 字面量 \u0026#34;\u0026#34; `` // 零值 // 操作 // 连接 // 关系运算 // 赋值操作 // 长度len 字节长度 // 索引 字节 // 切片= 字符串 字节 fmt.Println(name,desc) fmt.Println(desc[1:5]) var txt = \u0026#34;我爱中国\u0026#34; for i,v := range txt { fmt.Printf(\u0026#34;%d，%q\\n\u0026#34;,i,v) } fmt.Println(utf8.RuneCountInString(txt)) //转换 字符串=\u0026gt; byte fmt.Println([]byte(desc)) fmt.Println(string([]byte(desc))) // 字符串\u0026lt;=\u0026gt; int i, err := strconv.Atoi(\u0026#34;-15\u0026#34;) fmt.Println(i,err) // 字符串\u0026lt;=\u0026gt; bool } 7 指针类型 每个变量在内存中都有对应存储位置(内存地址)，可以通过\u0026amp; 运算符获取。指针是用来存储变量地址的变量\npackage main import \u0026#34;fmt\u0026#34; func main() { //指针 //值类型 //赋值 原有的数据复制一份给新的变量 //两个变量之间没有任何关系 //对nums 进行修改都不会影响nums2 //对nums2 进行任何修改也不会影响nums nums := [5]int{1,2,3,4,5} nums2 := nums fmt.Println(nums,nums2) nums2[0] = 100 fmt.Println(nums,nums2) var age = 1 var agePointer *int fmt.Println(age,agePointer) agePointer = \u0026amp;age fmt.Println(agePointer,age) fmt.Println(*agePointer) *agePointer = 33 fmt.Println(age,*agePointer) var numsPoint = \u0026amp;nums fmt.Printf(\u0026#34;%T\\n\u0026#34;,numsPoint) numsPoint[0] = 100 fmt.Println(nums,numsPoint) } ","permalink":"https://xingxing.io/posts/golang/go%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%A4%8D%E5%90%88%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B-03/","tags":["golang"],"title":"Go-复合数据类型"},{"categories":["golang"],"contents":"Go 学习笔记-基础知识 Go(又称Golang) 是Google 开发的一种静态强类型、编译型、并发型，并具有垃圾回收功能的编程语言\nGo 特性\n静态类型并具有丰富的内置类型\n函数多返回值\n错误处理机制\n语言层并发\n面向对象: 使用类型、组合、接口来实现面向对象\n自动垃圾回收机制\n交差编译\n1. 在mac 上安装 tar -C /usr/local -xzf go$VERSION.$OS-$ARCH.tar.gz 设置GOROOT GOPATH GOPROXY #GOROOT export PATH=$PATH:/usr/local/go/bin #GOPATH export GOPATH=$HOME/GoWork export PATH=$PATH:$GOPATH/bin #GOROOT 是go安装目录路径 #GOPATH 工作路径， bin 是可执行文件存放路径 pkg 编译包时，生成文件存放路径 src 源码路径 #GOPROXY 设置 go env -w GOPROXY=https://goproxy.cn,direct 2. GO Module设置 go env -w GO111MODULE=on godoc -http=:8000 官方文档查看 3. Go hello word package main //定义包main import ( \u0026#34;fmt\u0026#34; //导入标准包fmt ) // 程序入口，manin 函数 func main() { /* 注释 这里调用fmt包下的Println 函数打印内容到控制台 */ fmt.Println(\u0026#34;Hello world\u0026#34;) // 打印hello world 到控制台 } 4. 基本组成元素 标识符\n#go 语言提供一些预先定义的标识符用来表示内置的常量、函数、类型\n#在自定义标识符时避免使用\n标识符:程序中定义的名字 变量名，常量名字，函数名字，自定义类型，接口，包名等进行命名，以建立名称和使用之间的关系 Go语言标识符的命名规则: 1. 只能由非空字母(Unicode)、数字、下划线(_)组成 2. 只能以字母或下划线开 3. 不能go语言关键字 4. 避免使用go语言预定义标识符 5. 标识符区分大小写 规范: 1.必须满足: 组成只能由非空的Unicode 编码字符串、数字、下划线组成 2.必须满足: 必须以Unicode 编码的字符串或下划线开头(不能以数字开头) 3.必须满足: 不能与go 的关键字冲突(package，func,var .....25 ) 建议: 1. ASCILL 编码 2. 变量使用驼峰式，多个英文字母首字母大写 多个英文字母my_name myName(驼峰) 3. 与go内置的标识符不要冲突(string ...) 说明: 标识符区分大小写 my = \u0026#34;\u0026#34; My = \u0026#34;\u0026#34; 内置常量: true false nil iota\n内置类型:\nbool、byte、rune、int、int8、int16、int32、int64、uint、uint8、uint16、uint32、uint64、uintptr、float32、float64、complex64、complex128、string、error 内置函数:\nmake、len、cap、new、append、copy、close、delete、complex、real、imag、panic、recove 空白表示符: _\n5. 基本组成元素 关键字\n#关键字用于特定的语法结构\n#Go语言定义25个关键字\n*声明：import\n*实体声明和定义:\nchar、const、func、interface、map、struct、type、var *流程控制:\nbreak、case、continue、default、defer、else、fallthrough、for、go、goto、if、range、return、select、switch *字面量:\n字面量是值的表示方法，常用与对变量/常量进行初始化 主要分为: *标识基础数据类型值的字面量，例如: 0,1.1,true,3+4i,\u0026#39;a\u0026#39;,\u0026#34;我爱中国\u0026#34; *构造自定义的复合数据类型的类型字面量，例如: type Interval int *用于表示复合数据类型值的复合字面量，用来构造array、slice、map、struct、的值,例如:{1,2,3} 操作符:\n算术运算符 关系运算符 逻辑运算符 位运算符 赋值运算符 其他运算符 分隔符\n小括号() 中括号[] 大括号() 分号;, 逗号， 6. 变量 变量的含义: 程序允许过程中的数据都是保存在内存中，我们想要在代码中操作某个数据时就需要去内存找到这个变量，但是如果我们直接在代码中通过内存地址去操作变量的话，代码的可读性会非常差还容易出错，所以我们就利用变量将这个数据的内存地址保存起来，以后直接通过这个变量就能找到内存上对应的数据了。\n变量的类型: 变量的功能是存储数据。不同变量保存的数据类型会不一样。常见变量的数据类型有: 整型、浮点型、布尔型等。Go语言中每一个变量都有自己的类型，并且变量必须经过声明才开始使用。\n变量声明: Go 语言中的变量需要声明后才能使用，同一作用域内不支持重复声明。并且Go语言的变量声明后必须使用，否则会报。\n单个变量的声明与赋值\n全局变量是在函数外\n局部变量是在函数内，局部变量必须引用\n变量的声明格式: var \u0026lt;变量名称\u0026gt; \u0026lt;变量类型\u0026gt; 函数外的变量声明必须都是以var 开头进行声明 变量的赋值格式: var \u0026lt;变量名称\u0026gt; \u0026lt;表达式\u0026gt; 声明的同时赋值: var \u0026lt;变量名称\u0026gt; [变量类型] =\u0026lt;表达式\u0026gt; #单个变量的声明与赋值\npackage main import \u0026#34;fmt\u0026#34; func variableZeroValue() { var a int var b string //变量赋值 a = 10 fmt.Printf(\u0026#34;%d %q\u0026#34;, a, b) } func main() { variableZeroValue() fmt.Println(\u0026#34;hello, world\u0026#34;) } 6.1 变量的定义 在Go中，如果一个名字以大写字母开头，那么它就是已导出的，例如：\npi和pizza 并未以大写字母开头，所以它们是未导出的\n如果将Pi 修改为pi 就是未导出的\npackage main import \u0026#34;fmt\u0026#34; func main() { //局部变量给数据起一个名字 var msg string = \u0026#34;hello world\u0026#34; fmt.Println(msg) //其他代码 fmt.Println(msg) //其他代码 fmt.Println(msg) //其他代码 fmt.Println(msg) //其他代码 } # 多重赋值 package main import \u0026#34;fmt\u0026#34; func main(){ a, b := 10,20 // 交换2个变量的值 var tmp int tmp = a a = b b = tmp fmt.Printf(\u0026#34;a=%d,b=%d\\n\u0026#34;,a,b) i ,j := 10,20 i ,j = j,i fmt.Printf(\u0026#34;i=%d,j=%d\u0026#34;,i,j) } 6.2 变量作用域 package main import \u0026#34;fmt\u0026#34; //包级别 var packageVar string = \u0026#34;package Var \u0026#34; func main() { // 函数级别的 var funcVar string = \u0026#34;func Var\u0026#34; { // 块级别的 var blockVar string = \u0026#34;block Var \u0026#34; { //限定变量的使用范围 //子块级别 var innerBlockVar string = \u0026#34;inner block var\u0026#34; fmt.Println(packageVar, funcVar, innerBlockVar,blockVar) } fmt.Println(packageVar,funcVar,blockVar) } fmt.Println(packageVar,funcVar) } # 一个大括号就是一个作用域 6.3 变量的定义方式 package main import ( \u0026#34;fmt\u0026#34; ) func main(){ var name string = \u0026#34;xingxing\u0026#34; //定义类型并且初始化了值 var zeroString string // 定义变量类型,但不初始化值 var typeString = \u0026#34;xing\u0026#34; // 定义变量省略类型，不能省略初始化值 //通过对应的值类型推到变量的类型 // 短声明(必须在函数内包含函数内子块使用，不能再包级别使用) shortString := \u0026#34;xingxing\u0026#34; //通过对应的值类型推到变量的类型 fmt.Println(name,zeroString,typeString,shortString) } package main import ( \u0026#34;fmt\u0026#34; ) func main(){ // 函数内(块)定义的变量必须使用 /*\tvar name string = \u0026#34;xingxing\u0026#34; var msg = \u0026#34;hello world \u0026#34; var desc string */ var ( name string = \u0026#34;xingxing\u0026#34; msg = \u0026#34;hello world \u0026#34; desc string ) x,y := \u0026#34;x\u0026#34;,\u0026#34;y\u0026#34; fmt.Println(name,msg,desc,x,y) } 6.4 更新变量的值 package main import ( \u0026#34;fmt\u0026#34; ) func main(){ var name string = \u0026#34;xingxing\u0026#34; fmt.Println(name) name = \u0026#34;xing\u0026#34; // 更新变量的值 fmt.Println(name) } 7 常量声明 常量是一个简单值的标识符，在程序运行时，不会被修改的量。\n常量中的数据类型只有布尔型、数字型(整数型、浮点型和复数)和字符串型\n使用const 进行常量声明\npackage main import \u0026#34;fmt\u0026#34; const ( name string = \u0026#34;xingxing\u0026#34; msg = \u0026#34;msg\u0026#34; ) func main(){ fmt.Println(name) fmt.Println(msg) } 7.1 枚举 iota 标识符在一个小括号内，初始化为0，每调用一次+1\npackage main import \u0026#34;fmt\u0026#34; func main() { const ( Mon = iota //在常量组中使用iota,初始化0，每次调用+1 Tuesd Web Thur Fir Sat Sun ) fmt.Println(Mon,Tuesd,Web,Thur,Fir,Sat,Sun) } 8.数据类型 布尔类型: 用来表示真假，只有两个值,真 true 假 false 数字类型 浮点类型 字符串 数组 切片 映射 自定义类型 8.1 布尔类型 *布尔类型用于真假，类型名为bool，只有两个值true 和false，占用一个字节宽度，零值为false\npackage main import \u0026#34;fmt\u0026#34; func main(){ isGirl := true fmt.Println(\u0026#34;%T,%#v\u0026#34;, isGirl,isGirl) //操作 //逻辑运算 a,b,c,d := true,true,false,false //与: 左操作数与右操作数都为true，结果为true \u0026amp;\u0026amp; fmt.Println(\u0026#34;a,b:\u0026#34;,a\u0026amp;\u0026amp;b) // true \u0026amp;\u0026amp; true : true fmt.Println(\u0026#34;a,c:\u0026#34;,a\u0026amp;\u0026amp;c) // true \u0026amp;\u0026amp; false : false fmt.Println(\u0026#34;c,b:\u0026#34;,c\u0026amp;\u0026amp;b)// flase \u0026amp;\u0026amp; true : false fmt.Println(\u0026#34;c,d:\u0026#34;,c\u0026amp;\u0026amp;d) // false \u0026amp;\u0026amp; false : false //或: 左操作数与右操作数只要有一个为true，结果为true || fmt.Println(\u0026#34;a,b:\u0026#34;,a\u0026amp;\u0026amp;b) // true || true : true fmt.Println(\u0026#34;a,c:\u0026#34;,a\u0026amp;\u0026amp;c) // true || false : true fmt.Println(\u0026#34;c,b:\u0026#34;,c\u0026amp;\u0026amp;b)// flase || true : true fmt.Println(\u0026#34;c,d:\u0026#34;,c\u0026amp;\u0026amp;d) // false || false : false //非: 取反 true=\u0026gt; false, false=\u0026gt; true ! fmt.Println(\u0026#34;c,b:\u0026#34;,c\u0026amp;\u0026amp;b)// !true : false fmt.Println(\u0026#34;c,d:\u0026#34;,c\u0026amp;\u0026amp;d) // !false: true } 8.2 整数类型 package main import \u0026#34;fmt\u0026#34; func main(){ var age8 int8 = 31 var age int = 31 fmt.Println(\u0026#34;%T, %#v,%d\\n\u0026#34;,age8,age8,age8) fmt.Printf(\u0026#34;%T,%#v,%d\\n\u0026#34;,age,age,age) } package main import \u0026#34;fmt\u0026#34; func main(){ var age8 int8 = 31 var age int = 31 fmt.Println(\u0026#34;%T, %#v,%d\\n\u0026#34;,age8,age8,age8) fmt.Printf(\u0026#34;%T,%#v,%d\\n\u0026#34;,age,age,age) a,b := 2,4 fmt.Println(a + b ) fmt.Println(a - b ) fmt.Println(a * b ) fmt.Println(a/b) fmt.Println(a%b) a++ b-- fmt.Println(a,b) //关系运算 \u0026gt; \u0026lt; \u0026gt;= \u0026lt;= != == fmt.Println(a\u0026gt;b) fmt.Println(a\u0026lt;b) fmt.Println(a\u0026gt;=b) fmt.Println(a\u0026lt;=b) fmt.Println(a==b) fmt.Println(a !=b ) fmt.Println(a,b) 8.3 浮点数类型 浮点数用于表示带小数的数字，go 提供float32和float64两种浮点类型\npackage main import ( \u0026#34;fmt\u0026#34; ) func main(){ name := \u0026#34;\u0026#34; fmt.Print(\u0026#34;请输入你的名字:\u0026#34;) fmt.Scan(\u0026amp;name) fmt.Println(\u0026#34;你输入的名字是:\u0026#34;,name) age := 0 fmt.Println(\u0026#34;你输入的年龄是:\u0026#34;,age) msg := \u0026#34;\u0026#34; fmt.Print(\u0026#34;请输入你的msg:\u0026#34;) fmt.Scan(\u0026amp;msg) fmt.Println(\u0026#34;你输入的msg是:\u0026#34;,msg) } 8.4 格式化输出 #使用fmt.Printf 进行格式化参数输出，占位符\n%b 二进制 %c 以字符的方式打印 %d 以整数方式打印，十进制 %T 打印变量所属的类型 8.5 枚举类型 常使用iota 生成器用于初始化一系列相同规则的常量，批量声明常量的第一个常量使用iota进行赋值，此时iota被重置为0，其他常量省略类型和赋值，在每初始化一个常量则加1\npackage main import \u0026#34;fmt\u0026#34; func main() { const ( c1 int = iota c2 c3 c4 ) fmt.Println(c1,c2,c3,c4) } 8.6 指针类型 #指针声明需要指定存储地址中对应数据的类型，并使用*作为类型前缀，指针变量声明后会被初始化为nil，表示空指针\npackage main import \u0026#34;fmt\u0026#34; func main() { A := 2 B := A fmt.Println(A,B) C := \u0026amp;A fmt.Println(C) fmt.Println(*C) *C = 5 fmt.Println(A) } 8.7 字符串类型 Go内置了字符串类型，使用string 表示\n9.注释 #go 支持两种注释方式,行注释和块注释\n行注释: 以// 开头\n块注释: 以/* */\n10 打印 可以通过打印来调试代码\npackage main import \u0026#34;fmt\u0026#34; func main() { var a = 1 fmt.Println(a) a = a + 3 fmt.Println(a) } 10.1 输出 package main import \u0026#34;fmt\u0026#34; func main(){ fmt.Println(\u0026#34;1. 我叫星星\u0026#34;) // 打印并换行 fmt.Println(\u0026#34;2.我叫星星\u0026#34;) fmt.Print(\u0026#34;3. 我叫星星\u0026#34;) //打印 fmt.Print(\u0026#34;4.我叫星星\u0026#34;) name := \u0026#34;星星\u0026#34; fmt.Printf(\u0026#34;5.我叫%s\u0026#34;, name) //打印并占位 fmt.Printf(\u0026#34;6.我叫%s\u0026#34;,name) } #printf 格式化输出， 10.2 输入 package main import \u0026#34;fmt\u0026#34; func main(){ var name string fmt.Print(\u0026#34;请输入你的名字：\u0026#34;) fmt.Scan(\u0026amp;name) // 接收控制台输入内容赋值给变量name // \u0026amp;name =\u0026gt; 取name 指针(地址) fmt.Println(\u0026#34;你输入的内容是:\u0026#34;, name) } ","permalink":"https://xingxing.io/posts/golang/go%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","tags":["golang"],"title":"Go学习笔记-基础知识"},{"categories":null,"contents":"1. 目录规划 目录 存放地址说明 备注 /application 应用安装目录 /mnt/tool 软件存放目录 ##关闭防火墙 systemctl stop firewalld systemctl disable firewalld systemctl status firewalld #关闭selinux ##修改文件打开数 cat \u0026lt;\u0026lt;EOF\u0026gt;\u0026gt;/etc/systemd/system.conf DefaultLimitNOFILE=65535 DefaultLimitNPROC=65535 EOF cat \u0026lt;\u0026lt;EOF\u0026gt;\u0026gt;/etc/security/limits.conf * soft nofile 65535 * hard nofile 65535 * soft nproc 4096 * hard nproc 4096 EOF 1.1 zookeeper 集群部署 JDK 的安装\nexport JAVA_HOME=/home/application/jdk1.8.0_121/ export JRE_HOME=/home/application/jdk1.8.0_121/jre export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH vim /application/apache-zookeeper-3.8.0-bin/conf/zoo.cfg\ntickTime=2000 initLimit=10 syncLimit=5 dataDir=/home/application/data/zookeeper dataLogDir=/home/application/data/zookeeper/logs clientPort=2181 server.1=192.168.101.65:2887:3887 server.2=192.168.101.66:2887:3887 server.3=192.168.101.67:2887:3887 autopurge.purgeInterval=1 autopurge.snapRetainCount=5 ####以下单独执行 ##192.168.101.65执行 echo \u0026#34;1\u0026#34; \u0026gt; /home/application/data/zookeeper/myid ##192.168.101.66执行 echo \u0026#34;2\u0026#34; \u0026gt; /home/application/data/zookeeper/myid ##192.168.101.67执行 echo \u0026#34;3\u0026#34; \u0026gt; /home/application/data/zookeeper/myid vim /home/application/apache-zookeeper-3.8.0-bin/bin/zkEnv.sh\n##添加JDK 的目录 JAVA_HOME=\u0026#34;/home/application/jdk1.8.0_121/\u0026#34; 配置启动文件\nvim /usr/lib/systemd/system/zookeeper.service\n[Unit] Description=Zookeeper Server Service After=network.target [Service] Type=forking UMask=0027 User=root Group=root ExecStart=/home/application/apache-zookeeper-3.8.0-bin/bin/zkServer.sh start ExecStop=/home/application/apache-zookeeper-3.8.0-bin/bin/zkServer.sh stop Restart=on-failure RestartSec=10 [Install] WantedBy=multi-user.target #启动 systemctl start zookeeper #netstat -lntp 1.2 kafka 集群的部署 tar xvf kafka_2.12-2.4.1.tgz -C /home/application/ cd /home/application/kafka_2.12-2.4.1/ vim server.properties\nbroker.id=1 # 其他节点不一样，修改配置然后启动 listeners=PLAINTEXT://192.168.101.67:9092 num.network.threads=9 num.io.threads=16 socket.send.buffer.bytes=1024000 socket.receive.buffer.bytes=1024000 socket.request.max.bytes=104857600 log.dirs=/tmp/kafka-logs num.partitions=30 num.recovery.threads.per.data.dir=1 log.retention.hours=24 log.segment.bytes=1073741824 log.retention.check.interval.ms=300000 zookeeper.connect=192.168.101.65:2181,192.168.101.66:2181,192.168.101.67:2181 zookeeper.connection.timeout.ms=6000 delete.topic.enable = true auto.create.topics.enable=true message.max.bytes=20000000 replica.fetch.max.bytes=20485760 acks=1 log.cleanup.policy=delete 启动\n/home/application/kafka_2.12-2.4.1/bin/kafka-server-start.sh -daemon /home/application/kafka_2.12-2.4.1/config/server.properties \u0026amp; 1.3 hbase 集群部署 配置免密 关闭swap 时间同步 #192.168.101.65 上配置免密 ssh-keygen -t rsa ssh-copy-id root@192.168.101.66/67 swapoff -a sed -i \u0026#34;s/\\/dev\\/mapper\\/centos-swap/# \\/dev\\/mapper\\/centos-swap/\u0026#34; /etc/fstab cat /etc/fstab ##修改文件打开数 cat \u0026lt;\u0026lt;EOF\u0026gt;\u0026gt;/etc/systemd/system.conf DefaultLimitNOFILE=65535 DefaultLimitNPROC=65535 EOF cat \u0026lt;\u0026lt;EOF\u0026gt;\u0026gt;/etc/security/limits.conf * soft nofile 65535 * hard nofile 65535 * soft nproc 4096 * hard nproc 4096 EOF ##退出终端重连查看文件打开数 ulimit -n ##时间同步 yum install ntp -y systemctl start ntpd.service systemctl enable ntpd.service 1.3.1 Hadoop tar xvf hadoop-3.2.2.tar.gz -C /home/application/ mkdir -p /home/application/hadoop-3.2.2/conf touch /home/application/hadoop-3.2.2/conf/excludes mkdir -p /home/application/hadoop-3.2.2/{log,pids,mapred} mkdir -p /home/application/hadoop-3.2.2/mapred/log mkdir -p /home/data/hadoop/tmp/ mkdir -p /home/data/hadoop/hdfs/name/coredata/ mkdir -p /home/data/hadoop/hdfs/data-1/coredata/ mkdir -p /home/data/hadoop/hdfs/journal/ mkdir -p /home/data/hadoop/yarn/local-1/coredata/ cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/profile #HADOOP_HOME export HADOOP_HOME=/home/application/hadoop-3.2.2/ export PATH=\\$PATH:\\$HADOOP_HOME/bin:\\$HADOOP_HOME/sbin export HADOOP_CONF_DIR=/home/application/hadoop-3.2.2/conf EOF cp -rp /home/application/hadoop-3.2.2/etc/hadoop/* /home/application/hadoop-3.2.2/conf/ cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /home/application/hadoop-3.2.2/conf/hadoop-env.sh export JAVA_HOME=/home/application/jdk1.8.0_121/ export HADOOP_LOG_DIR=/home/application/hadoop-3.2.2/log export HADOOP_PID_DIR=/home/application/hadoop-3.2.2/pids export HADOOP_HEAPSIZE=4096 export HADOOP_OPTS=\u0026#34;-Djava.net.preferIPv4Stack=true -XX:+UseCompressedOops -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSClassUnloadingEnabled -XX:+CMSParallelRemarkEnabled -XX:+DisableExplicitGC -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=75 -XX:SoftRefLRUPolicyMSPerMB=0\u0026#34; EOF hadoop 配置文件 cat \u0026lt;\u0026lt; EOF \u0026gt; /home/application/hadoop-3.2.2/conf/core-site.xml \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;?xml-stylesheet type=\u0026#34;text/xsl\u0026#34; href=\u0026#34;configuration.xsl\u0026#34;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.defaultFS\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://masters\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;ha.zookeeper.quorum\u0026lt;/name\u0026gt;\t--指定 zookeeper 集群主机地址 \u0026lt;value\u0026gt;172.19.0.65:2181,172.19.0.66:2181,172.19.0.67:2181\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.tmp.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;file:/mnt/data/hadoop/tmp/\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.logfile.size\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;10000000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.logfile.count\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;10\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; EOF cat \u0026lt;\u0026lt; EOF \u0026gt; /home/application/hadoop-3.2.2/conf/hdfs-site.xml \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;?xml-stylesheet type=\u0026#34;text/xsl\u0026#34; href=\u0026#34;configuration.xsl\u0026#34;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt;\t--副本数为3 \u0026lt;name\u0026gt;dfs.replication\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;1\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.name.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;file:/mnt/data/hadoop/hdfs/name/coredata\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.datanode.data.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;file:/mnt/data/hadoop/hdfs/data-1/coredata\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt;\t--指定hdfs的nameservices为masters,和core-site.xml 文件中的设置保持一致 \u0026lt;name\u0026gt;dfs.nameservices\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;masters\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt;\t--masters 下面有两个 namenode 节点,分别是 h1 和 h2 (名称可自定义) \u0026lt;name\u0026gt;dfs.ha.namenodes.masters\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;h1,h2\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt;\t--指定 h1 节点的 rpc 通信地址 \u0026lt;name\u0026gt;dfs.namenode.rpc-address.masters.h1\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;192.168.101.66:9000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt;\t--指定 h1 节点的 http 通信地址 \u0026lt;name\u0026gt;dfs.namenode.http-address.masters.h1\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;192.168.101.66:9870\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt;\t--指定 h2 节点的 rpc 通信地址 \u0026lt;name\u0026gt;dfs.namenode.rpc-address.masters.h2\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;192.168.101.67:9000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt;\t--指定 h2 节点的 http 通信地址 \u0026lt;name\u0026gt;dfs.namenode.http-address.masters.h2\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;192.168.101.67:9870\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt;\t--指定 NameNode 元数据在 JournalNode 上的存放位置 \u0026lt;name\u0026gt;dfs.namenode.shared.edits.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;qjournal://192.168.101.65:8485;192.168.101.66:8485;192.168.101.67:8485/masters\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt;\t--指定 JournalNode 在本地磁盘存放数据的位置 \u0026lt;name\u0026gt;dfs.journalnode.edits.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/home/data/hadoop/hdfs/journal/\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt;\t--开启 NameNode 失败自动切换 \u0026lt;name\u0026gt;dfs.ha.automatic-failover.enabled\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt;\t--配置失败自动切换实现方式 \u0026lt;name\u0026gt;dfs.client.failover.proxy.provider.masters\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt;\t--配置隔离机制方法,每个机制占用一行 \u0026lt;name\u0026gt;dfs.ha.fencing.methods\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt; sshfence shell(/bin/true) \u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt;\t--配置 sshfence 隔离机制超时时间 \u0026lt;name\u0026gt;dfs.ha.fencing.ssh.connect-timeout\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;30000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.hosts.exclude\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/home/application/hadoop-3.2.2/conf/excludes\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.hosts\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/home/application/hadoop-3.2.2/conf/workers\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.fs-limits.max-directory-items\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;6400000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.datanode.registration.ip-hostname-check\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;false\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; EOF ","permalink":"https://xingxing.io/posts/data/%E7%9B%B8%E5%85%B3%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/","tags":null,"title":""},{"categories":null,"contents":"Canal-server 的配置 ################################################# ## mysql serverId , v1.0.26+ will autoGen #canal.instance.mysql.slaveId=101 # enable gtid use true/false canal.instance.gtidon=false # position info canal.instance.master.address=172.19.0.31:32060 canal.instance.master.journal.name= canal.instance.master.position= canal.instance.master.timestamp= canal.instance.master.gtid= # rds oss binlog #canal.instance.rds.accesskey= #canal.instance.rds.secretkey= #canal.instance.rds.instanceId= # table meta tsdb info #canal.instance.tsdb.enable=true #canal.instance.tsdb.url=jdbc:mysql://127.0.0.1:3306/canal_tsdb #canal.instance.tsdb.dbUsername=canal #canal.instance.tsdb.dbPassword=canal #canal.instance.standby.address = #canal.instance.standby.journal.name = #canal.instance.standby.position = #canal.instance.standby.timestamp = #canal.instance.standby.gtid= # username/password canal.instance.dbUsername=canal canal.instance.dbPassword=canal canal.instance.connectionCharset = UTF-8 #canal.instance.defaultDatabaseName = dc_canal # enable druid Decrypt database password canal.instance.enableDruid=false #canal.instance.pwdPublicKey=MFwwDQYJKoZIhvcNAQEBBQADSwAwSAJBALK4BUxdDltRRE5/zXpVEVPUgunvscYFtEip3pmLlhrWpacX7y7GCMo2/JM6LeHmiiNdH1FWgGCpUfircSwlWKUCAwEAAQ== # table regex canal.instance.filter.regex=dc_canal_dic_dt_data # table black regex canal.instance.filter.black.regex= # table field filter(format: schema1.tableName1:field1/field2,schema2.tableName2:field1/field2) #canal.instance.filter.field=test1.t_product:id/subject/keywords,test2.t_company:id/name/contact/ch # table field black filter(format: schema1.tableName1:field1/field2,schema2.tableName2:field1/field2) #canal.instance.filter.black.field=test1.t_product:subject/product_image,test2.t_company:id/name/contact/ch # mq config canal.mq.topic=dc_canal_dic_dt_data # dynamic topic route by schema or table regex #canal.mq.dynamicTopic=mytest1.user,mytest2\\\\..*,.*\\\\..* canal.mq.partition=0 # hash partition config #canal.mq.partitionsNum=3 #canal.mq.partitionHash=test.table:id^name,.*\\\\..* ################################################# canal-adapter 配置 application.yml server: port: 8081 spring: jackson: date-format: yyyy-MM-dd HH:mm:ss time-zone: GMT+8 default-property-inclusion: non_null canal.conf: mode: tcp #tcp kafka rocketMQ rabbitMQ flatMessage: true zookeeperHosts: syncBatchSize: 1000 retries: -1 timeout: accessKey: secretKey: consumerProperties: # canal tcp consumer canal.tcp.server.host: 172.19.0.31:30740 #canal.tcp.zookeeper.hosts: canal.tcp.batch.size: 500 #canal.tcp.username: #canal.tcp.password: srcDataSources: ## 源库 defaultDS: url: jdbc:mysql://172.19.0.31:32060/dc-canal?useUnicode=true username: canal password: canal canalAdapters: - instance: dc_canal_dic_dt_data # canal instance Name or mq topic name groups: - groupId: g1 outerAdapters: - name: logger - name: rdb key: mysql1 properties: jdbc.driverClassName: com.mysql.jdbc.Driver jdbc.url: jdbc:mysql://172.19.0.31:32060/sync_canal?useUnicode=true # 目标数据库，将新数据同步到这里 jdbc.username: canal jdbc.password: canal druid.stat.enable: false druid.stat.slowSqlMillis: 1000 mytest_user.yml dataSourceKey: defaultDS destination: dc_canal_dic_dt_data groupId: g1 outerAdapterKey: mysql1 concurrent: true dbMapping: database: dc-canal # 源库的名字 table: dic_dt_data # 源库的表名 targetTable: sync_canal.dic_dt_data ### 目标库名表名 targetPk: id: id mapAll: true # targetColumns: # id: # name: # role_id: # c_time: # test1: etlCondition: \u0026#34;where c_time\u0026gt;={}\u0026#34; commitBatch: 3000 # 批量提交的大小 在dc_canal 库的dc_dic_data 表中插入数据\n但是没有同步到sync_canal 库的dc_dic_data表中\nkind: Service apiVersion: v1 metadata: name: canal-server namespace: iot spec: ports: - name: http port: 11110 targetPort: 11110 - name: tcp11111 port: 11111 protocol: TCP targetPort: 11111 - name: tcp11112 protocol: TCP port: 11112 targetPort: 11112\ntype: NodePort selector: app.kubernetes.io/name: canal-server app: canal-server\n","permalink":"https://xingxing.io/posts/linux/canal-%E6%8A%A5%E9%94%99%E9%97%AE%E9%A2%98/","tags":null,"title":""},{"categories":null,"contents":"\n","permalink":"https://xingxing.io/posts/linux/%E6%9C%AA%E5%91%BD%E5%90%8D/","tags":null,"title":""},{"categories":["linux"],"contents":"​\nFastdfs 是一款开源的轻量级分布式文件系统纯C 实现，支持linux、FreeBSD 等UNIX 系统类google FS， 不是通用的文件系统，只能通过专有API 访问 1. 部署fastfs 下载https://github.com/happyfish100/fastdfs 安装依赖包 yum install gcc* perl-devel -y cd /opt/fastdfs-5.11/ ./make.sh 报错，需要安装libfastcommon wget https://github.com/happyfish100/libfastcommon/archive/V1.0.39.tar.gz tar xvf V1.0.39.tar.gz cd libfastcommon-1.0.39 ./make.sh ./make.sh install 继续安装fastdfs cd /opt/fastdfs-5.11 ./make.sh ./make.sh install 安装完成 2. 配置fastdfs 将fastdfs 安装目录下的conf 文件拷贝到/etc/fdfs/ [root@fastdfs fastdfs-5.11]# cp -r conf/* /etc/fdfs/ 配置trackerd vim /etc/fdfs/tracker.conf base_path=/data/fastdfs/tracker http.server_port=80 3. 启动tracker mkdir -p /data/fastdfs/tracker (/data/ 单独挂载的磁盘) /usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf restart 日志在 /data/fastdfs/tracker/logs/trackerd.log 4. 配置storaged vim /etc/fdfs/storage.conf base_path=/data/fastdfs/storage store_path0=/data/fastdfs/storage tracker_server=0.0.0.0:22122 http.server_port=80 mkdir -p /data/fastdfs/storage /usr/bin/fdfs_storaged /etc/fdfs/storage.conf restart 日志目录/data/fastdfs/logs/storaged.log 5. nginx 安装 yum -y install zlib zlib-devel openssl openssl--devel pcre pcre-devel wget https://nginx.org/download/nginx-1.16.0.tar.gz 下载fastdfs-nginx-module 模块 tar xvf nginx-1.16.0.tar.gz tar xvf V1.20.tar.gz cd nginx-1.16.0 ./configure --prefix=/usr/local/nginx --add-module=/opt/fastdfs-nginx-module-1.20/src make 报错 需要去找修改config 文件 vim /usr/local/fastdfs-nginx-module/src/config ngx_module_incs=\u0026#34;/usr/include/fastdfs /usr/include/fastcommon/\u0026#34; CORE_INCS=\u0026#34;$CORE_INCS /usr/include/fastdfs /usr/include/fastcommon/\u0026#34; 再次执行make make install 6. 修改nginx 的配置文件 vim /usr/local/nginx/conf/nginx.conf worker_processes 8; events { worker_connections 10240; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; ​ gzip on; ​ server { ​ listen 80; ​ server_name 10.39.33.37; ​ location /group1/M00 { ​ root /data/fastdfs/storage/data; ​ ngx_fastdfs_module; ​ } ​ error_page 500 502 503 504 /50x.html; ​ location = /50x.html { ​ root html; ​ } ​ } } 7.修改mod_fastdfs.conf cp /usr/local/fastdfs-nginx-module/src/mod_fastdfs.conf /etc/fdfs/ vim /etc/fdfs/mod_fastdfs.conf base_path=/data/fastdfs/storage tracker_server=10.39.33.37:22122 # the group name of the local storage server group_name=group1 # if the url / uri including the group name # set to false when uri like /M00/00/00/xxx # set to true when uri like ${group_name}/M00/00/00/xxx, such as group1/M00/xxx # default value is false url_have_group_name = true 8. 拷贝文件 cp /opt/fastdfs-5.11/conf/http.conf /etc/fdfs/ cp /opt/fastdfs-5.11/conf/mime.types /etc/fdfs/ ln -s /data/fastdfs/storage/data /data/fastdfs/storage/data/M00 9. 启动nginx /usr/local/nginx/sbin/nginx -s reload ngx_http_fastdfs_set pid=4022 10. 测试 vim /etc/fdfs/client.conf base_path=/data/fastdfs/storage tracker_server=10.39.33.37:22122 上传图片和文件测试 /usr/bin/fdfs_test /etc/fdfs/client.conf upload anti-steal.jpg 文件位置 ll /data/fastdfs/storage/data/00/00/CichJVz-bTeASuxcAAAFtNHOfzM63_big.conf 在浏览器中测试\nhttp://10.39.33.37/group1/M00/00/00/CichJVz-bM-AcO1MAABdrSqbHGQ105_big.jpg ","permalink":"https://xingxing.io/posts/linux/fastdfs%E9%83%A8%E7%BD%B2/","tags":["linux"],"title":"fastdfs部署"}]